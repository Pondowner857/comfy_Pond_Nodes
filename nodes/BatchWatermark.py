import os
import sys
import torch
import numpy as np
from PIL import Image
import folder_paths
import comfy.model_management
from typing import Tuple, List, Dict, Any

# å°†æ’ä»¶ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„
plugin_dir = os.path.dirname(os.path.abspath(__file__))
if plugin_dir not in sys.path:
    sys.path.append(plugin_dir)

# å°è¯•å¯¼å…¥æ‰€éœ€åº“
try:
    from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
    from huggingface_hub import snapshot_download
except ImportError:
    print("æœªæ‰¾åˆ°æ‰€éœ€åº“ã€‚è¯·å®‰è£…: pip install transformers huggingface_hub")
    raise

class WatermarkObjectDetector:
    """
    ComfyUI èŠ‚ç‚¹ï¼Œä½¿ç”¨ HuggingFace çš„ GroundingDINO æ¨¡å‹
    æ£€æµ‹å›¾åƒä¸­çš„æ°´å°æˆ–ä»»æ„æŒ‡å®šå¯¹è±¡
    """
    
    def __init__(self):
        self.model = None
        self.processor = None
        self.device = comfy.model_management.get_torch_device()
        self.model_name = "IDEA-Research/grounding-dino-tiny"
        
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
                "prompt": ("STRING", {
                    "default": "watermark",
                    "multiline": False,
                    "placeholder": "è¾“å…¥è¦æ£€æµ‹çš„å†…å®¹ï¼ˆå¦‚ï¼šæ°´å°ã€logoã€æ–‡å­—ï¼‰"
                }),
                "threshold": ("FLOAT", {
                    "default": 0.3,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.01,
                    "display": "slider"
                }),
                "nms_threshold": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.01,
                    "display": "slider"
                }),
            },
            "optional": {
                "model_name": ("STRING", {
                    "default": "IDEA-Research/grounding-dino-tiny",
                    "multiline": False,
                    "placeholder": "HuggingFace æ¨¡å‹åç§°"
                }),
            }
        }
    
    RETURN_TYPES = ("MASK", "BBOXES")
    RETURN_NAMES = ("é®ç½©", "è¾¹ç•Œæ¡†")
    FUNCTION = "detect"
    CATEGORY = "ğŸ³Pond/image"
    
    def get_model_path(self):
        """è·å–æ¨¡å‹å­˜å‚¨è·¯å¾„"""
        models_dir = folder_paths.models_dir
        detector_models_dir = os.path.join(models_dir, "object_detectors")
        os.makedirs(detector_models_dir, exist_ok=True)
        return detector_models_dir
    
    def load_model(self, model_name=None):
        """ä» HuggingFace åŠ è½½æ£€æµ‹æ¨¡å‹"""
        if model_name is None:
            model_name = self.model_name
            
        if self.model is None or model_name != self.model_name:
            print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
            
            # è®¾ç½®ç¼“å­˜ç›®å½•
            cache_dir = self.get_model_path()
            os.environ['HF_HOME'] = cache_dir
            os.environ['TRANSFORMERS_CACHE'] = cache_dir
            
            try:
                # åŠ è½½å¤„ç†å™¨å’Œæ¨¡å‹
                self.processor = AutoProcessor.from_pretrained(
                    model_name,
                    cache_dir=cache_dir
                )
                self.model = AutoModelForZeroShotObjectDetection.from_pretrained(
                    model_name,
                    cache_dir=cache_dir
                ).to(self.device)
                self.model_name = model_name
                print(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
            except Exception as e:
                print(f"æ¨¡å‹åŠ è½½é”™è¯¯: {e}")
                raise
    
    def detect(self, image, prompt, threshold, nms_threshold, model_name=None):
        """
        æ ¹æ®æ–‡æœ¬æç¤ºæ£€æµ‹å›¾åƒä¸­çš„å¯¹è±¡
        
        å‚æ•°:
            image: è¾“å…¥å›¾åƒå¼ é‡ (B, H, W, C)
            prompt: æè¿°è¦æ£€æµ‹å†…å®¹çš„æ–‡æœ¬æç¤º
            threshold: æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
            nms_threshold: éæå¤§å€¼æŠ‘åˆ¶é˜ˆå€¼
            model_name: å¯é€‰çš„è‡ªå®šä¹‰æ¨¡å‹åç§°
            
        è¿”å›:
            mask: æ£€æµ‹åŒºåŸŸçš„äºŒå€¼é®ç½©
            bboxes: è¾¹ç•Œæ¡†ï¼Œæ ¼å¼ä¸º [[x1, y1, x2, y2, ç½®ä¿¡åº¦, ç±»åˆ«ID], ...]
        """
        # æ ¹æ®éœ€è¦åŠ è½½æ¨¡å‹
        if model_name and model_name != self.model_name:
            self.load_model(model_name)
        elif self.model is None:
            self.load_model()
        
        # å°† ComfyUI å›¾åƒæ ¼å¼è½¬æ¢ä¸º PIL
        image_np = (image[0].cpu().numpy() * 255).astype(np.uint8)
        pil_image = Image.fromarray(image_np, mode='RGB')
        
        # ä½¿ç”¨æ¨¡å‹å¤„ç†å›¾åƒ
        inputs = self.processor(images=pil_image, text=prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
        
        # åå¤„ç†ç»“æœ
        results = self.processor.post_process_grounded_object_detection(
            outputs,
            inputs.input_ids,
            box_threshold=threshold,
            text_threshold=threshold,
            target_sizes=[(pil_image.height, pil_image.width)]
        )[0]
        
        # æå–è¾¹ç•Œæ¡†å’Œåˆ†æ•°
        boxes = results["boxes"].cpu().numpy()
        scores = results["scores"].cpu().numpy()
        labels = results["labels"]
        
        # å¦‚æœéœ€è¦ï¼Œåº”ç”¨ NMS
        if len(boxes) > 0 and nms_threshold < 1.0:
            from torchvision.ops import nms
            keep = nms(
                torch.tensor(boxes),
                torch.tensor(scores),
                nms_threshold
            ).numpy()
            boxes = boxes[keep]
            scores = scores[keep]
            labels = [labels[i] for i in keep]
        
        # åˆ›å»ºé®ç½©
        mask = np.zeros((image_np.shape[0], image_np.shape[1]), dtype=np.float32)
        
        # åˆ›å»ºè¾¹ç•Œæ¡†åˆ—è¡¨
        bboxes_list = []
        
        for i, (box, score) in enumerate(zip(boxes, scores)):
            x1, y1, x2, y2 = box.astype(int)
            
            # æ·»åŠ åˆ°é®ç½©
            mask[y1:y2, x1:x2] = 1.0
            
            # æ·»åŠ åˆ°è¾¹ç•Œæ¡†åˆ—è¡¨ï¼ˆæ ¼å¼ï¼š[x1, y1, x2, y2, ç½®ä¿¡åº¦, ç±»åˆ«ID]ï¼‰
            bboxes_list.append([float(x1), float(y1), float(x2), float(y2), float(score), 0])
        
        # å°†é®ç½©è½¬æ¢ä¸º ComfyUI æ ¼å¼ (B, H, W)
        mask_tensor = torch.from_numpy(mask).unsqueeze(0)
        
        # å°†è¾¹ç•Œæ¡†è½¬æ¢ä¸ºè‡ªå®šä¹‰æ ¼å¼
        bboxes_output = BoundingBoxes(bboxes_list, image_np.shape[1], image_np.shape[0])
        
        return (mask_tensor, bboxes_output)

class BoundingBoxes:
    """
    åœ¨ ComfyUI ä¸­å­˜å‚¨è¾¹ç•Œæ¡†çš„è‡ªå®šä¹‰ç±»
    """
    def __init__(self, boxes: List[List[float]], width: int, height: int):
        self.boxes = boxes  # [[x1, y1, x2, y2, ç½®ä¿¡åº¦, ç±»åˆ«ID], ...]
        self.width = width
        self.height = height
    
    def to_list(self) -> List[List[float]]:
        return self.boxes
    
    def to_normalized(self) -> List[List[float]]:
        """è½¬æ¢ä¸ºå½’ä¸€åŒ–åæ ‡ (0-1)"""
        normalized = []
        for box in self.boxes:
            x1, y1, x2, y2, conf, cls = box
            normalized.append([
                x1 / self.width,
                y1 / self.height,
                x2 / self.width,
                y2 / self.height,
                conf,
                cls
            ])
        return normalized
    
    def __len__(self):
        return len(self.boxes)
    
    def __repr__(self):
        return f"è¾¹ç•Œæ¡†({len(self.boxes)} ä¸ªæ¡†, {self.width}x{self.height})"

class DrawBoundingBoxes:
    """
    åœ¨å›¾åƒä¸Šå¯è§†åŒ–è¾¹ç•Œæ¡†çš„è¾…åŠ©èŠ‚ç‚¹
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
                "bboxes": ("BBOXES",),
                "color": ("STRING", {"default": "red"}),
                "thickness": ("INT", {"default": 2, "min": 1, "max": 10}),
                "show_confidence": ("BOOLEAN", {"default": True}),
            }
        }
    
    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "draw"
    CATEGORY = "ğŸ³Pond/image"
    
    def draw(self, image, bboxes, color, thickness, show_confidence):
        """åœ¨å›¾åƒä¸Šç»˜åˆ¶è¾¹ç•Œæ¡†"""
        import cv2
        
        # é¢œè‰²æ˜ å°„
        color_map = {
            "red": (255, 0, 0),
            "green": (0, 255, 0),
            "blue": (0, 0, 255),
            "yellow": (255, 255, 0),
            "white": (255, 255, 255),
            "black": (0, 0, 0)
        }
        
        draw_color = color_map.get(color.lower(), (255, 0, 0))
        
        # å°†å›¾åƒè½¬æ¢ä¸º numpy
        image_np = (image[0].cpu().numpy() * 255).astype(np.uint8)
        result = image_np.copy()
        
        # ç»˜åˆ¶æ¯ä¸ªè¾¹ç•Œæ¡†
        for box in bboxes.to_list():
            x1, y1, x2, y2, conf, cls = box
            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
            
            # ç»˜åˆ¶çŸ©å½¢
            cv2.rectangle(result, (x1, y1), (x2, y2), draw_color, thickness)
            
            # å¦‚æœéœ€è¦ï¼Œç»˜åˆ¶ç½®ä¿¡åº¦åˆ†æ•°
            if show_confidence:
                text = f"{conf:.2f}"
                cv2.putText(result, text, (x1, y1 - 5),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, draw_color, 1)
        
        # è½¬æ¢å› ComfyUI æ ¼å¼
        result_tensor = torch.from_numpy(result.astype(np.float32) / 255.0).unsqueeze(0)
        
        return (result_tensor,)

# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "WatermarkObjectDetector": WatermarkObjectDetector,
    "DrawBoundingBoxes": DrawBoundingBoxes,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "WatermarkObjectDetector": "æ°´å°/å¯¹è±¡æ£€æµ‹å™¨",
    "DrawBoundingBoxes": "ç»˜åˆ¶è¾¹ç•Œæ¡†",
}

# è‡ªå®šä¹‰ç±»å‹æ³¨å†Œ
def register_custom_types():
    """ä¸º ComfyUI æ³¨å†Œè‡ªå®šä¹‰ç±»å‹"""
    # è¿™å…è®¸ BBOXES åœ¨èŠ‚ç‚¹ä¹‹é—´ä¼ é€’
    if hasattr(comfy, 'supported_types') and "BBOXES" not in comfy.supported_types:
        comfy.supported_types.add("BBOXES")

# æ¨¡å—åŠ è½½æ—¶å°è¯•æ³¨å†Œç±»å‹
try:
    register_custom_types()
except:
    pass

__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS']