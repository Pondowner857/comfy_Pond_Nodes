import torch
import numpy as np
from PIL import Image, ImageFilter, ImageEnhance
import comfy.utils

class PixelizeNode:
    """
    å°†æ™®é€šå›¾åƒè½¬æ¢ä¸ºåƒç´ é£æ ¼
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "å›¾åƒ": ("IMAGE",),
                "åƒç´ å¤§å°": ("INT", {
                    "default": 8,
                    "min": 1,
                    "max": 64,
                    "step": 1,
                    "display": "number"
                }),
                "ç¼©æ”¾æ¨¡å¼": (["ä¿æŒåŸå§‹å°ºå¯¸", "ç¼©æ”¾åˆ°åƒç´ ç½‘æ ¼"],),
                "æŠ—é”¯é½¿": ("BOOLEAN", {"default": False}),
            },
        }
    
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("å›¾åƒ",)
    FUNCTION = "pixelize"
    CATEGORY = "ğŸ³Pond/å›¾åƒ"
    
    def pixelize(self, å›¾åƒ, åƒç´ å¤§å°, ç¼©æ”¾æ¨¡å¼, æŠ—é”¯é½¿):
        batch_size, height, width, channels = å›¾åƒ.shape
        processed_images = []
        
        for i in range(batch_size):
            # è½¬æ¢ä¸ºPILå›¾åƒ
            img_tensor = å›¾åƒ[i]
            img_np = (img_tensor.cpu().numpy() * 255).astype(np.uint8)
            img_pil = Image.fromarray(img_np, mode='RGB' if channels == 3 else 'RGBA')
            
            if ç¼©æ”¾æ¨¡å¼ == "ä¿æŒåŸå§‹å°ºå¯¸":
                # ä¿æŒåŸå§‹å°ºå¯¸ï¼Œåªæ˜¯åƒç´ åŒ–
                # è®¡ç®—åƒç´ åŒ–åçš„å°ºå¯¸
                pixel_width = width // åƒç´ å¤§å°
                pixel_height = height // åƒç´ å¤§å°
                
                # ç¼©å°åˆ°åƒç´ å°ºå¯¸
                downsample_method = Image.LANCZOS if æŠ—é”¯é½¿ else Image.NEAREST
                img_small = img_pil.resize((pixel_width, pixel_height), downsample_method)
                
                # æ”¾å¤§å›åŸå§‹å°ºå¯¸
                img_pixelated = img_small.resize((width, height), Image.NEAREST)
            else:
                # ç¼©æ”¾åˆ°åƒç´ ç½‘æ ¼ï¼ˆç¡®ä¿æ¯ä¸ªåƒç´ å—éƒ½æ˜¯å®Œæ•´çš„ï¼‰
                pixel_width = width // åƒç´ å¤§å°
                pixel_height = height // åƒç´ å¤§å°
                new_width = pixel_width * åƒç´ å¤§å°
                new_height = pixel_height * åƒç´ å¤§å°
                
                # å…ˆè°ƒæ•´åˆ°ç½‘æ ¼å°ºå¯¸
                img_resized = img_pil.resize((new_width, new_height), Image.LANCZOS)
                
                # åƒç´ åŒ–
                downsample_method = Image.LANCZOS if æŠ—é”¯é½¿ else Image.NEAREST
                img_small = img_resized.resize((pixel_width, pixel_height), downsample_method)
                img_pixelated = img_small.resize((new_width, new_height), Image.NEAREST)
            
            # è½¬æ¢å›tensor
            img_np = np.array(img_pixelated).astype(np.float32) / 255.0
            img_tensor = torch.from_numpy(img_np)
            processed_images.append(img_tensor)
        
        output = torch.stack(processed_images)
        return (output,)

class SquarePixelCorrectionNode:
    """
    å°†åƒç´ å›¾åƒä¸­çš„éæ­£æ–¹å½¢åƒç´ æ ¡æ­£ä¸º1:1çš„æ­£æ–¹å½¢
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "å›¾åƒ": ("IMAGE",),
                "æ£€æµ‹æ¨¡å¼": (["è‡ªåŠ¨æ£€æµ‹", "æ‰‹åŠ¨è®¾ç½®"],),
                "åƒç´ å®½åº¦": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 64,
                    "step": 1,
                    "display": "number"
                }),
                "åƒç´ é«˜åº¦": ("INT", {
                    "default": 2,
                    "min": 1,
                    "max": 64,
                    "step": 1,
                    "display": "number"
                }),
                "è¾“å‡ºæ¨¡å¼": (["æ‹‰ä¼¸å›¾åƒ", "æ·»åŠ è¾¹è·", "è£å‰ªå›¾åƒ"],),
                "å¯¹é½æ–¹å¼": (["å±…ä¸­", "å·¦ä¸Š", "å³ä¸Š", "å·¦ä¸‹", "å³ä¸‹"],),
            },
        }
    
    RETURN_TYPES = ("IMAGE", "INT", "INT")
    RETURN_NAMES = ("å›¾åƒ", "åƒç´ å®½åº¦", "åƒç´ é«˜åº¦")
    FUNCTION = "correct_pixels"
    CATEGORY = "ğŸ³Pond/å›¾åƒ"
    
    def correct_pixels(self, å›¾åƒ, æ£€æµ‹æ¨¡å¼, åƒç´ å®½åº¦, åƒç´ é«˜åº¦, è¾“å‡ºæ¨¡å¼, å¯¹é½æ–¹å¼):
        batch_size, height, width, channels = å›¾åƒ.shape
        processed_images = []
        
        for i in range(batch_size):
            # è½¬æ¢ä¸ºPILå›¾åƒ
            img_tensor = å›¾åƒ[i]
            img_np = (img_tensor.cpu().numpy() * 255).astype(np.uint8)
            img_pil = Image.fromarray(img_np, mode='RGB' if channels == 3 else 'RGBA')
            
            # è‡ªåŠ¨æ£€æµ‹åƒç´ å¤§å°
            if æ£€æµ‹æ¨¡å¼ == "è‡ªåŠ¨æ£€æµ‹":
                detected_width, detected_height = self._detect_pixel_size(img_pil)
                if detected_width > 0 and detected_height > 0:
                    åƒç´ å®½åº¦ = detected_width
                    åƒç´ é«˜åº¦ = detected_height
            
            # è®¡ç®—æ ¡æ­£åçš„å°ºå¯¸
            if è¾“å‡ºæ¨¡å¼ == "æ‹‰ä¼¸å›¾åƒ":
                # è®¡ç®—éœ€è¦çš„æ‹‰ä¼¸æ¯”ä¾‹
                if åƒç´ å®½åº¦ > åƒç´ é«˜åº¦:
                    # éœ€è¦å‚ç›´æ‹‰ä¼¸
                    scale_factor = åƒç´ å®½åº¦ / åƒç´ é«˜åº¦
                    new_width = width
                    new_height = int(height * scale_factor)
                else:
                    # éœ€è¦æ°´å¹³æ‹‰ä¼¸
                    scale_factor = åƒç´ é«˜åº¦ / åƒç´ å®½åº¦
                    new_width = int(width * scale_factor)
                    new_height = height
                
                img_corrected = img_pil.resize((new_width, new_height), Image.NEAREST)
                
            elif è¾“å‡ºæ¨¡å¼ == "æ·»åŠ è¾¹è·":
                # è®¡ç®—éœ€è¦æ·»åŠ çš„è¾¹è·
                target_ratio = 1.0  # ç›®æ ‡æ˜¯1:1
                current_ratio = åƒç´ å®½åº¦ / åƒç´ é«˜åº¦
                
                if current_ratio > target_ratio:
                    # åƒç´ å¤ªå®½ï¼Œéœ€è¦æ·»åŠ ä¸Šä¸‹è¾¹è·
                    new_height = int(height * current_ratio)
                    new_width = width
                    
                    # åˆ›å»ºæ–°å›¾åƒ
                    img_corrected = Image.new(img_pil.mode, (new_width, new_height), (0, 0, 0))
                    
                    # æ ¹æ®å¯¹é½æ–¹å¼æ”¾ç½®åŸå›¾
                    y_offset = self._calculate_offset(new_height - height, å¯¹é½æ–¹å¼, 'vertical')
                    img_corrected.paste(img_pil, (0, y_offset))
                else:
                    # åƒç´ å¤ªé«˜ï¼Œéœ€è¦æ·»åŠ å·¦å³è¾¹è·
                    new_width = int(width / current_ratio)
                    new_height = height
                    
                    # åˆ›å»ºæ–°å›¾åƒ
                    img_corrected = Image.new(img_pil.mode, (new_width, new_height), (0, 0, 0))
                    
                    # æ ¹æ®å¯¹é½æ–¹å¼æ”¾ç½®åŸå›¾
                    x_offset = self._calculate_offset(new_width - width, å¯¹é½æ–¹å¼, 'horizontal')
                    img_corrected.paste(img_pil, (x_offset, 0))
                    
            else:  # è£å‰ªå›¾åƒ
                # è®¡ç®—è£å‰ªå°ºå¯¸
                if åƒç´ å®½åº¦ > åƒç´ é«˜åº¦:
                    # éœ€è¦è£å‰ªå®½åº¦
                    crop_ratio = åƒç´ é«˜åº¦ / åƒç´ å®½åº¦
                    new_width = int(width * crop_ratio)
                    new_height = height
                    
                    # æ ¹æ®å¯¹é½æ–¹å¼è®¡ç®—è£å‰ªä½ç½®
                    x_offset = self._calculate_offset(width - new_width, å¯¹é½æ–¹å¼, 'horizontal')
                    img_corrected = img_pil.crop((x_offset, 0, x_offset + new_width, height))
                else:
                    # éœ€è¦è£å‰ªé«˜åº¦
                    crop_ratio = åƒç´ å®½åº¦ / åƒç´ é«˜åº¦
                    new_width = width
                    new_height = int(height * crop_ratio)
                    
                    # æ ¹æ®å¯¹é½æ–¹å¼è®¡ç®—è£å‰ªä½ç½®
                    y_offset = self._calculate_offset(height - new_height, å¯¹é½æ–¹å¼, 'vertical')
                    img_corrected = img_pil.crop((0, y_offset, width, y_offset + new_height))
            
            # è½¬æ¢å›tensor
            img_np = np.array(img_corrected).astype(np.float32) / 255.0
            img_tensor = torch.from_numpy(img_np)
            processed_images.append(img_tensor)
        
        output = torch.stack(processed_images)
        return (output, åƒç´ å®½åº¦, åƒç´ é«˜åº¦)
    
    def _detect_pixel_size(self, img):
        """è‡ªåŠ¨æ£€æµ‹åƒç´ å¤§å°"""
        # å°†å›¾åƒè½¬æ¢ä¸ºnumpyæ•°ç»„
        img_array = np.array(img)
        height, width = img_array.shape[:2]
        
        # æ£€æµ‹æ°´å¹³æ–¹å‘çš„åƒç´ å¤§å°
        pixel_width = 1
        for w in range(1, min(width // 2, 64)):
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åƒç´ éƒ½æ˜¯wçš„å€æ•°å®½åº¦
            is_valid = True
            for x in range(0, width - w, w):
                # æ£€æŸ¥åƒç´ å—æ˜¯å¦ä¸€è‡´
                block = img_array[:, x:x+w]
                if not self._is_uniform_block(block):
                    is_valid = False
                    break
            if is_valid:
                pixel_width = w
                break
        
        # æ£€æµ‹å‚ç›´æ–¹å‘çš„åƒç´ å¤§å°
        pixel_height = 1
        for h in range(1, min(height // 2, 64)):
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åƒç´ éƒ½æ˜¯hçš„å€æ•°é«˜åº¦
            is_valid = True
            for y in range(0, height - h, h):
                # æ£€æŸ¥åƒç´ å—æ˜¯å¦ä¸€è‡´
                block = img_array[y:y+h, :]
                if not self._is_uniform_block(block):
                    is_valid = False
                    break
            if is_valid:
                pixel_height = h
                break
        
        return pixel_width, pixel_height
    
    def _is_uniform_block(self, block):
        """æ£€æŸ¥åƒç´ å—æ˜¯å¦å‡åŒ€"""
        if block.size == 0:
            return False
        
        # è·å–ç¬¬ä¸€ä¸ªåƒç´ çš„é¢œè‰²
        first_pixel = block.flat[0:block.shape[-1]]
        
        # æ£€æŸ¥æ‰€æœ‰åƒç´ æ˜¯å¦ç›¸åŒ
        return np.all(block == first_pixel)
    
    def _calculate_offset(self, total_offset, alignment, direction):
        """æ ¹æ®å¯¹é½æ–¹å¼è®¡ç®—åç§»é‡"""
        if alignment == "å±…ä¸­":
            return total_offset // 2
        elif alignment == "å·¦ä¸Š":
            return 0
        elif alignment == "å³ä¸Š":
            return total_offset if direction == 'horizontal' else 0
        elif alignment == "å·¦ä¸‹":
            return 0 if direction == 'horizontal' else total_offset
        elif alignment == "å³ä¸‹":
            return total_offset
        return 0

class PartialPixelizeNode:
    """
    å±€éƒ¨åƒç´ åŒ–èŠ‚ç‚¹ï¼Œé€šè¿‡é®ç½©æ§åˆ¶åƒç´ åŒ–åŒºåŸŸ
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "å›¾åƒ": ("IMAGE",),
                "é®ç½©": ("MASK",),
                "åƒç´ å¤§å°": ("INT", {
                    "default": 8,
                    "min": 1,
                    "max": 64,
                    "step": 1,
                    "display": "number"
                }),
                "æ··åˆå¼ºåº¦": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.01,
                    "display": "slider"
                }),
                "ç¾½åŒ–åŠå¾„": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                    "display": "number"
                }),
                "æ··åˆæ¨¡å¼": (["æ­£å¸¸", "å åŠ ", "æŸ”å…‰", "å¼ºå…‰"],),
                "åè½¬é®ç½©": ("BOOLEAN", {"default": False}),
                "ä¿æŒé¢œè‰²": ("BOOLEAN", {"default": False}),
            },
        }
    
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("å›¾åƒ",)
    FUNCTION = "partial_pixelize"
    CATEGORY = "ğŸ³Pond/å›¾åƒ"
    
    def partial_pixelize(self, å›¾åƒ, é®ç½©, åƒç´ å¤§å°, æ··åˆå¼ºåº¦, ç¾½åŒ–åŠå¾„, æ··åˆæ¨¡å¼, åè½¬é®ç½©, ä¿æŒé¢œè‰²):
        batch_size, height, width, channels = å›¾åƒ.shape
        processed_images = []
        
        for i in range(batch_size):
            # è½¬æ¢ä¸ºPILå›¾åƒ
            img_tensor = å›¾åƒ[i]
            img_np = (img_tensor.cpu().numpy() * 255).astype(np.uint8)
            img_pil = Image.fromarray(img_np, mode='RGB' if channels == 3 else 'RGBA')
            
            # å¤„ç†é®ç½©
            if i < é®ç½©.shape[0]:
                mask_tensor = é®ç½©[i]
            else:
                mask_tensor = é®ç½©[0]  # å¦‚æœé®ç½©æ•°é‡ä¸è¶³ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª
            
            mask_np = (mask_tensor.cpu().numpy() * 255).astype(np.uint8)
            mask_pil = Image.fromarray(mask_np, mode='L')
            
            # ç¡®ä¿é®ç½©å°ºå¯¸åŒ¹é…
            if mask_pil.size != (width, height):
                mask_pil = mask_pil.resize((width, height), Image.LANCZOS)
            
            # åè½¬é®ç½©
            if åè½¬é®ç½©:
                mask_np = 255 - np.array(mask_pil)
                mask_pil = Image.fromarray(mask_np, mode='L')
            
            # åº”ç”¨ç¾½åŒ–
            if ç¾½åŒ–åŠå¾„ > 0:
                mask_pil = mask_pil.filter(ImageFilter.GaussianBlur(radius=ç¾½åŒ–åŠå¾„))
            
            # åˆ›å»ºåƒç´ åŒ–ç‰ˆæœ¬
            pixel_width = max(1, width // åƒç´ å¤§å°)
            pixel_height = max(1, height // åƒç´ å¤§å°)
            
            # ç¼©å°å›¾åƒ
            img_small = img_pil.resize((pixel_width, pixel_height), Image.NEAREST)
            
            # å¦‚æœä¿æŒé¢œè‰²ï¼Œåªåƒç´ åŒ–å½¢çŠ¶
            if ä¿æŒé¢œè‰²:
                # åˆ›å»ºäº®åº¦å›¾
                img_gray = img_pil.convert('L')
                gray_small = img_gray.resize((pixel_width, pixel_height), Image.NEAREST)
                gray_pixelated = gray_small.resize((width, height), Image.NEAREST)
                
                # å°†åƒç´ åŒ–çš„äº®åº¦åº”ç”¨åˆ°åŸå§‹é¢œè‰²
                img_hsv = img_pil.convert('HSV')
                h, s, v = img_hsv.split()
                img_hsv = Image.merge('HSV', (h, s, gray_pixelated))
                img_pixelated = img_hsv.convert('RGB')
            else:
                # æ ‡å‡†åƒç´ åŒ–
                img_pixelated = img_small.resize((width, height), Image.NEAREST)
            
            # åº”ç”¨æ··åˆæ¨¡å¼
            if æ··åˆæ¨¡å¼ == "æ­£å¸¸":
                img_blended = img_pixelated
            elif æ··åˆæ¨¡å¼ == "å åŠ ":
                img_blended = self._overlay_blend(img_pil, img_pixelated)
            elif æ··åˆæ¨¡å¼ == "æŸ”å…‰":
                img_blended = self._soft_light_blend(img_pil, img_pixelated)
            elif æ··åˆæ¨¡å¼ == "å¼ºå…‰":
                img_blended = self._hard_light_blend(img_pil, img_pixelated)
            
            # æ ¹æ®é®ç½©å’Œå¼ºåº¦æ··åˆåŸå›¾å’Œåƒç´ åŒ–å›¾åƒ
            if æ··åˆå¼ºåº¦ < 1.0:
                # è°ƒæ•´é®ç½©å¼ºåº¦
                mask_np = np.array(mask_pil).astype(np.float32) / 255.0
                mask_np = mask_np * æ··åˆå¼ºåº¦
                mask_pil = Image.fromarray((mask_np * 255).astype(np.uint8), mode='L')
            
            # ä½¿ç”¨é®ç½©åˆæˆ
            img_result = Image.composite(img_blended, img_pil, mask_pil)
            
            # è½¬æ¢å›tensor
            img_np = np.array(img_result).astype(np.float32) / 255.0
            img_tensor = torch.from_numpy(img_np)
            processed_images.append(img_tensor)
        
        output = torch.stack(processed_images)
        return (output,)
    
    def _overlay_blend(self, base, overlay):
        """å åŠ æ··åˆæ¨¡å¼"""
        base_np = np.array(base).astype(np.float32) / 255.0
        overlay_np = np.array(overlay).astype(np.float32) / 255.0
        
        # å åŠ å…¬å¼
        result = np.where(base_np < 0.5,
                         2 * base_np * overlay_np,
                         1 - 2 * (1 - base_np) * (1 - overlay_np))
        
        result = (result * 255).astype(np.uint8)
        return Image.fromarray(result, mode='RGB')
    
    def _soft_light_blend(self, base, overlay):
        """æŸ”å…‰æ··åˆæ¨¡å¼"""
        base_np = np.array(base).astype(np.float32) / 255.0
        overlay_np = np.array(overlay).astype(np.float32) / 255.0
        
        # æŸ”å…‰å…¬å¼
        result = np.where(overlay_np < 0.5,
                         base_np - (1 - 2 * overlay_np) * base_np * (1 - base_np),
                         base_np + (2 * overlay_np - 1) * (np.sqrt(base_np) - base_np))
        
        result = np.clip(result, 0, 1)
        result = (result * 255).astype(np.uint8)
        return Image.fromarray(result, mode='RGB')
    
    def _hard_light_blend(self, base, overlay):
        """å¼ºå…‰æ··åˆæ¨¡å¼"""
        base_np = np.array(base).astype(np.float32) / 255.0
        overlay_np = np.array(overlay).astype(np.float32) / 255.0
        
        # å¼ºå…‰å…¬å¼ï¼ˆä¸å åŠ ç›¸åï¼‰
        result = np.where(overlay_np < 0.5,
                         2 * base_np * overlay_np,
                         1 - 2 * (1 - base_np) * (1 - overlay_np))
        
        result = (result * 255).astype(np.uint8)
        return Image.fromarray(result, mode='RGB')

class PixelArtEnhanceNode:
    """
    åƒç´ è‰ºæœ¯å¢å¼ºèŠ‚ç‚¹ï¼Œæä¾›æ›´å¤šåƒç´ å¤„ç†é€‰é¡¹
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "å›¾åƒ": ("IMAGE",),
                "å¤„ç†æ¨¡å¼": (["åƒç´ åŒ–", "åƒç´ æ ¡æ­£", "åƒç´ ä¼˜åŒ–"],),
                "åƒç´ å¤§å°": ("INT", {
                    "default": 8,
                    "min": 1,
                    "max": 64,
                    "step": 1,
                    "display": "number"
                }),
                "é¢œè‰²é‡åŒ–": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 256,
                    "step": 1,
                    "display": "number"
                }),
                "æŠ–åŠ¨": ("BOOLEAN", {"default": False}),
                "ä¿æŒé”åˆ©": ("BOOLEAN", {"default": True}),
            },
        }
    
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("å›¾åƒ",)
    FUNCTION = "enhance"
    CATEGORY = "ğŸ³Pond/å›¾åƒ"
    
    def enhance(self, å›¾åƒ, å¤„ç†æ¨¡å¼, åƒç´ å¤§å°, é¢œè‰²é‡åŒ–, æŠ–åŠ¨, ä¿æŒé”åˆ©):
        batch_size, height, width, channels = å›¾åƒ.shape
        processed_images = []
        
        for i in range(batch_size):
            # è½¬æ¢ä¸ºPILå›¾åƒ
            img_tensor = å›¾åƒ[i]
            img_np = (img_tensor.cpu().numpy() * 255).astype(np.uint8)
            img_pil = Image.fromarray(img_np, mode='RGB' if channels == 3 else 'RGBA')
            
            if å¤„ç†æ¨¡å¼ == "åƒç´ åŒ–":
                # æ ‡å‡†åƒç´ åŒ–å¤„ç†
                pixel_width = width // åƒç´ å¤§å°
                pixel_height = height // åƒç´ å¤§å°
                
                # åº”ç”¨é¢œè‰²é‡åŒ–
                if é¢œè‰²é‡åŒ– > 0:
                    img_pil = img_pil.quantize(colors=é¢œè‰²é‡åŒ–, dither=Image.FLOYDSTEINBERG if æŠ–åŠ¨ else Image.NONE)
                    img_pil = img_pil.convert('RGB')
                
                # ç¼©å°
                img_small = img_pil.resize((pixel_width, pixel_height), Image.NEAREST if ä¿æŒé”åˆ© else Image.LANCZOS)
                
                # æ”¾å¤§
                img_processed = img_small.resize((width, height), Image.NEAREST)
                
            elif å¤„ç†æ¨¡å¼ == "åƒç´ æ ¡æ­£":
                # æ£€æµ‹å¹¶æ ¡æ­£éæ­£æ–¹å½¢åƒç´ 
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥æŒ‰ç…§é«˜åº¦è¿›è¡Œç¼©æ”¾
                img_processed = img_pil.resize((width, width), Image.NEAREST)
                
            else:  # åƒç´ ä¼˜åŒ–
                # ä¼˜åŒ–åƒç´ è‰ºæœ¯ï¼ˆå»é™¤æ¨¡ç³Šï¼Œå¢å¼ºè¾¹ç¼˜ï¼‰
                # å¢å¼ºé”åº¦
                if ä¿æŒé”åˆ©:
                    enhancer = ImageEnhance.Sharpness(img_pil)
                    img_pil = enhancer.enhance(2.0)
                
                # åº”ç”¨æœ€è¿‘é‚»é‡‡æ ·ç¡®ä¿åƒç´ æ¸…æ™°
                img_processed = img_pil
                
                # é¢œè‰²é‡åŒ–
                if é¢œè‰²é‡åŒ– > 0:
                    img_processed = img_processed.quantize(colors=é¢œè‰²é‡åŒ–, dither=Image.FLOYDSTEINBERG if æŠ–åŠ¨ else Image.NONE)
                    img_processed = img_processed.convert('RGB')
            
            # è½¬æ¢å›tensor
            img_np = np.array(img_processed).astype(np.float32) / 255.0
            img_tensor = torch.from_numpy(img_np)
            processed_images.append(img_tensor)
        
        output = torch.stack(processed_images)
        return (output,)

# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "Pixelize": PixelizeNode,
    "SquarePixelCorrection": SquarePixelCorrectionNode,
    "PartialPixelize": PartialPixelizeNode,
    "PixelArtEnhance": PixelArtEnhanceNode
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Pixelize": "ğŸ³åƒç´ åŒ–",
    "SquarePixelCorrection": "ğŸ³åƒç´ æ ¡æ­£",
    "PartialPixelize": "ğŸ³å±€éƒ¨åƒç´ åŒ–",
    "PixelArtEnhance": "ğŸ³åƒç´ å¢å¼º"
}