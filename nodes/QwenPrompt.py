import torch
import os
import folder_paths
from PIL import Image
import numpy as np
from transformers.generation import GenerationConfig
import gc
import json

# å°è¯•å¯¼å…¥ä¸åŒç‰ˆæœ¬çš„ç±»
try:
    from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor
    QWEN2VL_AVAILABLE = True
except ImportError:
    QWEN2VL_AVAILABLE = False
    print("[Qwen Image Captioner] Qwen2VL not available, trying Qwen2_5VL...")

try:
    from transformers import Qwen2_5VLForConditionalGeneration, AutoProcessor as Qwen2_5VLProcessor
    QWEN2_5VL_AVAILABLE = True
except ImportError:
    QWEN2_5VL_AVAILABLE = False
    print("[Qwen Image Captioner] Qwen2_5VL not available...")

# åŸºç¡€ç±»å¯¼å…¥
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor


class QwenImageCaptioner:
    """
    A ComfyUI node for generating image captions/prompts using Qwen vision-language models
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        # è·å–Qwenæ¨¡å‹ç›®å½•
        qwen_models_dir = os.path.join(folder_paths.models_dir, "Qwen")
        available_models = cls._scan_available_models(qwen_models_dir)
        
        return {
            "required": {
                "image": ("IMAGE",),
                "model_name": (available_models,),
                "prompt_type": (["detailed", "brief", "technical", "artistic", "custom"],),
                "language": (["English", "ä¸­æ–‡"],),
                "device": (["auto", "cuda", "cpu"],),
                "max_length": ("INT", {
                    "default": 256,
                    "min": 32,
                    "max": 2048,
                    "step": 32
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1
                }),
                "auto_unload": ("BOOLEAN", {
                    "default": True,
                    "label_on": "Auto Unload",
                    "label_off": "Keep Loaded"
                }),
            },
            "optional": {
                "custom_instruction": ("STRING", {
                    "multiline": True,
                    "default": "Describe this image in detail for use as a prompt in image generation."
                }),
                "max_image_size": ("INT", {
                    "default": 1024,
                    "min": 512,
                    "max": 2048,
                    "step": 128,
                    "display": "number"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("caption",)
    FUNCTION = "generate_caption"
    CATEGORY = "ğŸ³Pond/Qwen"
    
    def __init__(self):
        self.model = None
        self.processor = None
        self.tokenizer = None
        self.current_model = None
        self.current_device = None
    
    @staticmethod
    def _scan_available_models(qwen_models_dir):
        """æ‰«æå¯ç”¨çš„Qwen VLæ¨¡å‹"""
        available_models = []
        
        if not os.path.exists(qwen_models_dir):
            return ["No models found in models/Qwen/"]
        
        for item in os.listdir(qwen_models_dir):
            model_path = os.path.join(qwen_models_dir, item)
            
            if not os.path.isdir(model_path):
                continue
                
            config_path = os.path.join(model_path, "config.json")
            if not os.path.exists(config_path):
                continue
            
            try:
                with open(config_path, 'r') as f:
                    config = json.load(f)
                model_type = config.get("model_type", "").lower()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯VLæ¨¡å‹
                if "vl" in model_type or "VL" in item:
                    available_models.append(item)
                else:
                    available_models.append(f"[éVLæ¨¡å‹] {item}")
            except:
                available_models.append(item)
        
        return available_models if available_models else ["No models found in models/Qwen/"]
    
    def _determine_device(self, device):
        """ç¡®å®šä½¿ç”¨çš„è®¾å¤‡"""
        if device == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        return device
    
    def _cleanup_previous_model(self):
        """æ¸…ç†ä¹‹å‰åŠ è½½çš„æ¨¡å‹"""
        if self.model is not None:
            del self.model
            self.model = None
        if self.processor is not None:
            del self.processor
            self.processor = None
        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None
        
        self.current_model = None
        self.current_device = None
        
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("[Qwen Image Captioner] Model unloaded and memory cleared")
    
    def _get_model_config(self, model_path):
        """è¯»å–æ¨¡å‹é…ç½®"""
        config_path = os.path.join(model_path, "config.json")
        with open(config_path, 'r') as f:
            return json.load(f)
    
    def _prepare_model_kwargs(self, device):
        """å‡†å¤‡æ¨¡å‹åŠ è½½å‚æ•°"""
        model_kwargs = {
            "trust_remote_code": True,
        }
        
        # è®¾ç½®è®¾å¤‡æ˜ å°„
        model_kwargs["device_map"] = device
        
        # è®¾ç½®ç²¾åº¦ - ç®€åŒ–ç‰ˆæœ¬ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³ç²¾åº¦
        if device == "cpu":
            model_kwargs["torch_dtype"] = torch.float32
            print("[Qwen Image Captioner] Using FP32 for CPU")
        else:
            # GPUä¸Šä¼˜å…ˆä½¿ç”¨bf16ï¼Œå¦åˆ™ä½¿ç”¨fp16
            if torch.cuda.is_available() and torch.cuda.is_bf16_supported():
                model_kwargs["torch_dtype"] = torch.bfloat16
                print("[Qwen Image Captioner] Using BF16 for GPU")
            else:
                model_kwargs["torch_dtype"] = torch.float16
                print("[Qwen Image Captioner] Using FP16 for GPU")
        
        return model_kwargs
    
    def _load_qwen2_5_vl(self, model_path, model_kwargs):
        """åŠ è½½Qwen2.5-VLæ¨¡å‹"""
        if not QWEN2_5VL_AVAILABLE:
            return False
            
        try:
            print("[Qwen Image Captioner] Loading as Qwen2.5-VL model...")
            self.processor = Qwen2_5VLProcessor.from_pretrained(
                model_path,
                trust_remote_code=True
            )
            self.model = Qwen2_5VLForConditionalGeneration.from_pretrained(
                model_path,
                **model_kwargs
            ).eval()
            return True
        except Exception as e:
            print(f"[Qwen Image Captioner] Failed to load as Qwen2.5-VL: {e}")
            return False
    
    def _load_qwen2_vl(self, model_path, model_kwargs):
        """åŠ è½½Qwen2-VLæ¨¡å‹"""
        if not QWEN2VL_AVAILABLE:
            return False
            
        try:
            print("[Qwen Image Captioner] Loading as Qwen2-VL model...")
            self.processor = Qwen2VLProcessor.from_pretrained(
                model_path,
                trust_remote_code=True
            )
            self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                model_path,
                **model_kwargs
            ).eval()
            return True
        except Exception as e:
            print(f"[Qwen Image Captioner] Failed to load as Qwen2-VL: {e}")
            return False
    
    def _load_auto_model(self, model_path, model_kwargs, model_type):
        """ä½¿ç”¨AutoModelåŠ è½½æ¨¡å‹"""
        try:
            print("[Qwen Image Captioner] Loading with AutoModel...")
            
            if "qwen2_5_vl" in model_type:
                # ä½¿ç”¨AutoModelçš„from_pretrained
                from transformers import AutoModelForVision2Seq
                try:
                    self.model = AutoModelForVision2Seq.from_pretrained(
                        model_path,
                        **model_kwargs
                    ).eval()
                except:
                    from transformers import AutoModel
                    self.model = AutoModel.from_pretrained(
                        model_path,
                        **model_kwargs
                    ).eval()
                
                self.processor = AutoProcessor.from_pretrained(
                    model_path,
                    trust_remote_code=True
                )
            else:
                # æ—§ç‰ˆQwen-VL
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_path,
                    trust_remote_code=True
                )
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    **model_kwargs
                ).eval()
                
                try:
                    self.processor = AutoProcessor.from_pretrained(
                        model_path,
                        trust_remote_code=True
                    )
                except:
                    self.processor = None
            
            return True
        except Exception as e:
            print(f"[Qwen Image Captioner] Failed to load with AutoModel: {e}")
            return False
    
    def load_model(self, model_name, device="auto"):
        """åŠ è½½Qwenè§†è§‰è¯­è¨€æ¨¡å‹"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸æ”¯æŒçš„æ ¼å¼
        if "[éVLæ¨¡å‹]" in model_name:
            raise ValueError(
                "è¿™ä¸æ˜¯ä¸€ä¸ªè§†è§‰è¯­è¨€(VL)æ¨¡å‹ï¼\n"
                "å›¾åƒæè¿°éœ€è¦ä½¿ç”¨åŒ…å«è§†è§‰ç¼–ç å™¨çš„VLæ¨¡å‹ã€‚\n"
                "è¯·ä¸‹è½½Qwen-VLæˆ–Qwen2-VLç³»åˆ—æ¨¡å‹ã€‚"
            )
        
        # ç¡®å®šè®¾å¤‡
        self.device = self._determine_device(device)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½
        if self.current_model == model_name and self.current_device == device:
            print(f"[Qwen Image Captioner] Model {model_name} already loaded on {device}")
            return
        
        # æ¸…ç†ä¹‹å‰çš„æ¨¡å‹
        self._cleanup_previous_model()
        
        # æ¨¡å‹è·¯å¾„
        model_path = os.path.join(folder_paths.models_dir, "Qwen", model_name)
        
        try:
            # è¯»å–é…ç½®
            config = self._get_model_config(model_path)
            model_type = config.get("model_type", "").lower()
            architectures = config.get("architectures", [])
            
            print(f"[Qwen Image Captioner] Model type: {model_type}")
            print(f"[Qwen Image Captioner] Architectures: {architectures}")
            
            # å‡†å¤‡æ¨¡å‹åŠ è½½å‚æ•°
            model_kwargs = self._prepare_model_kwargs(self.device)
            
            # æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„åŠ è½½æ–¹å¼
            loaded = False
            
            # 1. å°è¯•Qwen2.5-VL
            if "qwen2_5_vl" in model_type:
                loaded = self._load_qwen2_5_vl(model_path, model_kwargs)
            
            # 2. å°è¯•Qwen2-VL
            if not loaded and ("qwen2_vl" in model_type or "Qwen2VL" in str(architectures)):
                loaded = self._load_qwen2_vl(model_path, model_kwargs)
            
            # 3. å°è¯•ä½¿ç”¨AutoModel
            if not loaded:
                loaded = self._load_auto_model(model_path, model_kwargs, model_type)
            
            if not loaded:
                raise ValueError(f"æ— æ³•åŠ è½½æ¨¡å‹ {model_name}ã€‚è¯·ç¡®ä¿æ¨¡å‹æ ¼å¼æ­£ç¡®ã€‚")
            
            self.current_model = model_name
            self.current_device = device
            print(f"[Qwen Image Captioner] Successfully loaded model: {model_name}")
            
        except Exception as e:
            print(f"[Qwen Image Captioner] Error loading model {model_name}: {str(e)}")
            raise
    
    def _resize_image(self, pil_image, max_size=1024):
        """ç­‰æ¯”ä¾‹ç¼©æ”¾å›¾åƒåˆ°æŒ‡å®šæœ€å¤§å°ºå¯¸"""
        width, height = pil_image.size
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç¼©æ”¾
        if width <= max_size and height <= max_size:
            print(f"[Qwen Image Captioner] Image size ({width}x{height}) is within limit, no resizing needed")
            return pil_image
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        if width > height:
            scale = max_size / width
        else:
            scale = max_size / height
        
        new_width = int(width * scale)
        new_height = int(height * scale)
        
        # ä½¿ç”¨é«˜è´¨é‡çš„é‡é‡‡æ ·æ–¹æ³•
        resized_image = pil_image.resize((new_width, new_height), Image.Resampling.LANCZOS)
        print(f"[Qwen Image Captioner] Resized image from {width}x{height} to {new_width}x{new_height}")
        
        return resized_image
    
    def _prepare_image(self, image, max_size=1024):
        """å‡†å¤‡å›¾åƒè¾“å…¥ï¼ŒåŒ…æ‹¬æ ¼å¼è½¬æ¢å’Œå°ºå¯¸è°ƒæ•´"""
        if isinstance(image, torch.Tensor):
            # ComfyUIå›¾åƒæ ¼å¼: [batch, height, width, channels]
            if len(image.shape) == 4:
                image = image[0]  # å–ç¬¬ä¸€å¼ å›¾
            # è½¬æ¢ä¸ºPIL Image
            image_np = (image.cpu().numpy() * 255).astype(np.uint8)
            pil_image = Image.fromarray(image_np)
        else:
            pil_image = image
        
        # ç­‰æ¯”ä¾‹ç¼©æ”¾å›¾åƒ
        pil_image = self._resize_image(pil_image, max_size)
        
        return pil_image
    
    def _get_instruction(self, prompt_type, language, custom_instruction):
        """æ ¹æ®ç±»å‹å’Œè¯­è¨€ç”ŸæˆæŒ‡ä»¤"""
        instructions = {
            "ä¸­æ–‡": {
                "detailed": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ï¼ŒåŒ…æ‹¬æ‰€æœ‰çš„ç‰©ä½“ã€äººç‰©ã€é¢œè‰²ã€çº¹ç†ã€æ„å›¾ã€å…‰çº¿å’Œæ°›å›´ã€‚è¯·å…·ä½“è¯´æ˜ç©ºé—´å…³ç³»å’Œè§†è§‰å…ƒç´ ã€‚",
                "brief": "è¯·ç®€æ´åœ°æè¿°å›¾ç‰‡çš„ä¸»è¦ä¸»é¢˜å’Œå…³é”®å…ƒç´ ã€‚",
                "technical": "è¯·ä»æŠ€æœ¯è§’åº¦æè¿°è¿™å¼ å›¾ç‰‡ï¼šæ„å›¾ã€å…‰çº¿ã€è‰²å½©æ­é…ã€æ‹æ‘„è§’åº¦ã€æ™¯æ·±ä»¥åŠä»»ä½•åæœŸå¤„ç†æ•ˆæœã€‚",
                "artistic": "è¯·ä»è‰ºæœ¯è§’åº¦æè¿°è¿™å¼ å›¾ç‰‡ï¼Œé‡ç‚¹å…³æ³¨è‰ºæœ¯é£æ ¼ã€æƒ…ç»ªã€ç¾å­¦ç‰¹è´¨å’Œæƒ…æ„Ÿå½±å“ã€‚åŒ…æ‹¬è‰ºæœ¯æŠ€å·§å’Œè§†è§‰å™äº‹çš„ç»†èŠ‚ã€‚",
                "custom": custom_instruction if custom_instruction else "è¯·æè¿°è¿™å¼ å›¾ç‰‡ã€‚"
            },
            "English": {
                "detailed": "Describe this image in great detail, including all objects, people, colors, textures, composition, lighting, and atmosphere. Be specific about spatial relationships and visual elements.",
                "brief": "Provide a concise description of the main subject and key elements in this image.",
                "technical": "Describe this image focusing on technical aspects: composition, lighting, color palette, camera angle, depth of field, and any post-processing effects.",
                "artistic": "Describe this image with focus on artistic style, mood, aesthetic qualities, and emotional impact. Include details about artistic techniques and visual storytelling.",
                "custom": custom_instruction if custom_instruction else "Describe this image."
            }
        }
        
        return instructions[language][prompt_type]
    
    def _generate_with_qwen2_vl(self, pil_image, instruction, max_length, temperature):
        """ä½¿ç”¨Qwen2-VLæ ¼å¼ç”Ÿæˆæè¿°"""
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": instruction}
                ]
            }
        ]
        
        text = self.processor.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.processor(
            text=[text],
            images=[pil_image],
            return_tensors="pt"
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        generation_config = GenerationConfig(
            max_new_tokens=max_length,
            temperature=temperature,
            do_sample=temperature > 0.1,
            pad_token_id=self.processor.tokenizer.pad_token_id,
            eos_token_id=self.processor.tokenizer.eos_token_id,
        )
        
        with torch.no_grad():
            outputs = self.model.generate(**inputs, generation_config=generation_config)
        
        return outputs
    
    def _generate_with_qwen_vl(self, pil_image, instruction, max_length, temperature):
        """ä½¿ç”¨æ—§ç‰ˆQwen-VLæ ¼å¼ç”Ÿæˆæè¿°"""
        query = self.tokenizer.from_list_format([
            {'image': pil_image},
            {'text': instruction}
        ])
        inputs = self.tokenizer(query, return_tensors='pt')
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        generation_config = GenerationConfig(
            max_new_tokens=max_length,
            temperature=temperature,
            do_sample=temperature > 0.1,
            pad_token_id=self.tokenizer.pad_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
        )
        
        with torch.no_grad():
            outputs = self.model.generate(**inputs, generation_config=generation_config)
        
        return outputs
    
    def _decode_output(self, outputs, instruction):
        """è§£ç æ¨¡å‹è¾“å‡º"""
        if hasattr(outputs, 'cpu'):
            outputs = outputs.cpu()
        
        # ä½¿ç”¨æ­£ç¡®çš„è§£ç å™¨
        if self.processor and hasattr(self.processor, 'decode'):
            response = self.processor.decode(outputs[0], skip_special_tokens=True)
        elif self.tokenizer:
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        else:
            response = self.processor.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆå»é™¤è¾“å…¥éƒ¨åˆ†ï¼‰
        if instruction in response:
            caption = response.split(instruction)[-1].strip()
        else:
            caption = response.split('\n')[-1].strip() if '\n' in response else response
        
        return caption
    
    def generate_caption(self, image, model_name, prompt_type, language, device, 
                        max_length, temperature, auto_unload=True, 
                        custom_instruction="", max_image_size=1024):
        """ç”Ÿæˆå›¾åƒæè¿°"""
        
        try:
            # åŠ è½½æ¨¡å‹
            self.load_model(model_name, device)
            
            # å‡†å¤‡å›¾åƒï¼ˆåŒ…æ‹¬ç¼©æ”¾ï¼‰
            pil_image = self._prepare_image(image, max_image_size)
            
            # è·å–æŒ‡ä»¤
            instruction = self._get_instruction(prompt_type, language, custom_instruction)
            
            # æ ¹æ®ä¸åŒçš„æ¨¡å‹ç±»å‹ä½¿ç”¨ä¸åŒçš„ç”Ÿæˆæ–¹æ³•
            if self.processor and hasattr(self.processor, 'apply_chat_template'):
                # Qwen2-VLæ ¼å¼
                outputs = self._generate_with_qwen2_vl(pil_image, instruction, max_length, temperature)
            elif self.tokenizer and hasattr(self.tokenizer, 'from_list_format'):
                # æ—§ç‰ˆQwen-VLæ ¼å¼
                outputs = self._generate_with_qwen_vl(pil_image, instruction, max_length, temperature)
            else:
                # é€šç”¨æ ¼å¼ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                raise NotImplementedError("Unsupported model format")
            
            # è§£ç è¾“å‡º
            caption = self._decode_output(outputs, instruction)
            
            print(f"[Qwen Image Captioner] Caption generated successfully")
            
            return (caption,)
            
        except Exception as e:
            error_msg = f"Error generating caption: {str(e)}"
            print(f"[Qwen Image Captioner] {error_msg}")
            
            # æä¾›è°ƒè¯•å»ºè®®
            if "CUDA" in str(e):
                print("[Qwen Image Captioner] CUDA error detected. Suggestions:")
                print("1. Try using CPU instead of GPU")
                print("2. Try reducing max_length")
                print("3. Check GPU memory usage")
            
            return (error_msg,)
        
        finally:
            # å¦‚æœå¯ç”¨äº†è‡ªåŠ¨å¸è½½ï¼Œåœ¨ç”Ÿæˆå®Œæˆåå¸è½½æ¨¡å‹
            if auto_unload:
                print("[Qwen Image Captioner] Auto-unloading model after generation...")
                self._cleanup_previous_model()


# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "QwenImageCaptioner": QwenImageCaptioner,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "QwenImageCaptioner": "ğŸ³Qwen Image Captioner",
}