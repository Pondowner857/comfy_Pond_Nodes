import torch
import os
import folder_paths
from PIL import Image
import numpy as np
from transformers.generation import GenerationConfig
import gc
import json
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import warnings
from typing import Optional, Dict, Any, Tuple
from functools import partial
import asyncio
from pathlib import Path

# å°è¯•å¯¼å…¥ä¸åŒç‰ˆæœ¬çš„ç±»
try:
    from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor
    QWEN2VL_AVAILABLE = True
except ImportError:
    QWEN2VL_AVAILABLE = False
    print("[Qwen Image Captioner] Qwen2VL not available, trying Qwen2_5VL...")

try:
    from transformers import Qwen2_5VLForConditionalGeneration, AutoProcessor as Qwen2_5VLProcessor
    QWEN2_5VL_AVAILABLE = True
except ImportError:
    QWEN2_5VL_AVAILABLE = False
    print("[Qwen Image Captioner] Qwen2_5VL not available...")

# åŸºç¡€ç±»å¯¼å…¥
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor

# æ£€æµ‹FlashAttention2æ˜¯å¦å¯ç”¨
FLASH_ATTENTION_AVAILABLE = False
try:
    from flash_attn import flash_attn_func
    FLASH_ATTENTION_AVAILABLE = True
    print("[Qwen Image Captioner] FlashAttention2 is available âœ“")
except ImportError:
    print("[Qwen Image Captioner] FlashAttention2 not available - Install with: pip install flash-attn")

# å°è¯•å¯¼å…¥åŠ é€Ÿåº“
try:
    from accelerate import init_empty_weights, load_checkpoint_and_dispatch
    ACCELERATE_AVAILABLE = True
    print("[Qwen Image Captioner] Accelerate is available âœ“")
except ImportError:
    ACCELERATE_AVAILABLE = False
    print("[Qwen Image Captioner] Accelerate not available - Install for faster loading: pip install accelerate")


class FastModelLoader:
    """ä¼˜åŒ–çš„æ¨¡å‹åŠ è½½å™¨ï¼Œæ”¯æŒå¤šçº¿ç¨‹å’Œå¹¶è¡ŒåŠ è½½"""
    
    @staticmethod
    def parallel_load_safetensors(model_path, device, dtype):
        """å¹¶è¡ŒåŠ è½½safetensorsæ ¼å¼çš„æ¨¡å‹æƒé‡"""
        try:
            import safetensors.torch
            from safetensors import safe_open
            
            # æŸ¥æ‰¾æ‰€æœ‰çš„safetensorsæ–‡ä»¶
            safetensor_files = list(Path(model_path).glob("*.safetensors"))
            if not safetensor_files:
                return None
                
            print(f"[Qwen Image Captioner] Found {len(safetensor_files)} safetensor files")
            
            # ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡ŒåŠ è½½
            state_dict = {}
            
            def load_single_file(file_path):
                with safe_open(file_path, framework="pt", device=str(device)) as f:
                    return {key: f.get_tensor(key) for key in f.keys()}
            
            with ThreadPoolExecutor(max_workers=4) as executor:
                futures = [executor.submit(load_single_file, f) for f in safetensor_files]
                for future in as_completed(futures):
                    state_dict.update(future.result())
            
            return state_dict
        except Exception as e:
            print(f"[Qwen Image Captioner] Safetensors loading failed: {e}")
            return None
    
    @staticmethod
    def optimize_model_loading_params(device):
        """è·å–ä¼˜åŒ–çš„æ¨¡å‹åŠ è½½å‚æ•°"""
        params = {
            "trust_remote_code": True,
            "low_cpu_mem_usage": True,  # å‡å°‘CPUå†…å­˜ä½¿ç”¨
        }
        
        # è®¾å¤‡é…ç½®
        if device == "cuda" and torch.cuda.is_available():
            params["device_map"] = "auto"  # è‡ªåŠ¨è®¾å¤‡æ˜ å°„
            
            # é€‰æ‹©æœ€ä½³ç²¾åº¦
            if torch.cuda.is_bf16_supported():
                params["torch_dtype"] = torch.bfloat16
                print("[Qwen Image Captioner] Using BF16 precision")
            else:
                params["torch_dtype"] = torch.float16
                print("[Qwen Image Captioner] Using FP16 precision")
            
            # FlashAttention2é…ç½®
            if FLASH_ATTENTION_AVAILABLE:
                params["attn_implementation"] = "flash_attention_2"
                print("[Qwen Image Captioner] FlashAttention2 enabled âœ“")
            else:
                params["attn_implementation"] = "sdpa"
                print("[Qwen Image Captioner] Using SDPA (FlashAttention2 not available)")
        else:
            params["device_map"] = "cpu"
            params["torch_dtype"] = torch.float32
            print("[Qwen Image Captioner] Using CPU with FP32")
        
        return params
    
    @staticmethod
    def load_with_accelerate(model_class, model_path, device, dtype):
        """ä½¿ç”¨accelerateåº“åŠ é€ŸåŠ è½½"""
        if not ACCELERATE_AVAILABLE:
            return None
            
        try:
            from accelerate import init_empty_weights, load_checkpoint_and_dispatch
            
            print("[Qwen Image Captioner] Loading with Accelerate...")
            
            # åˆå§‹åŒ–ç©ºæƒé‡æ¨¡å‹
            with init_empty_weights():
                model = model_class.from_pretrained(
                    model_path,
                    trust_remote_code=True,
                    torch_dtype=dtype
                )
            
            # å¹¶è¡ŒåŠ è½½æƒé‡
            model = load_checkpoint_and_dispatch(
                model,
                model_path,
                device_map="auto" if device == "cuda" else "cpu",
                no_split_module_classes=["Qwen2VLDecoderLayer"],
                dtype=dtype
            )
            
            return model
        except Exception as e:
            print(f"[Qwen Image Captioner] Accelerate loading failed: {e}")
            return None


class QwenImageCaptioner:
    """
    ä¼˜åŒ–ç‰ˆçš„ComfyUIèŠ‚ç‚¹ï¼Œä¸“æ³¨äºFlashAttention2åŠ é€Ÿå’Œå¿«é€Ÿæ¨¡å‹åŠ è½½
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        # è·å–Qwenæ¨¡å‹ç›®å½•
        qwen_models_dir = os.path.join(folder_paths.models_dir, "Qwen")
        available_models = cls._scan_available_models(qwen_models_dir)
        
        return {
            "required": {
                "image": ("IMAGE",),
                "model_name": (available_models,),
                "prompt_type": (["detailed", "brief", "technical", "artistic", "custom"],),
                "language": (["English", "ä¸­æ–‡"],),
                "device": (["auto", "cuda", "cpu"],),
                "max_length": ("INT", {
                    "default": 256,
                    "min": 32,
                    "max": 2048,
                    "step": 32
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1
                }),
                "auto_unload": ("BOOLEAN", {
                    "default": True,
                    "label_on": "Auto Unload",
                    "label_off": "Keep Loaded"
                }),
                "use_flash_attention": ("BOOLEAN", {
                    "default": True,
                    "label_on": "Use FlashAttention2",
                    "label_off": "Use Default"
                }),
                "fast_load": ("BOOLEAN", {
                    "default": True,
                    "label_on": "Fast Load",
                    "label_off": "Normal Load"
                }),
            },
            "optional": {
                "custom_instruction": ("STRING", {
                    "multiline": True,
                    "default": "Describe this image in detail for use as a prompt in image generation."
                }),
                "max_image_size": ("INT", {
                    "default": 1024,
                    "min": 512,
                    "max": 2048,
                    "step": 128,
                    "display": "number"
                }),
                "num_beams": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 5,
                    "step": 1,
                    "display": "number"
                }),
                "use_cache": ("BOOLEAN", {
                    "default": True,
                    "label_on": "Use KV Cache",
                    "label_off": "No Cache"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("caption",)
    FUNCTION = "generate_caption"
    CATEGORY = "ğŸ³Pond/Qwen"
    
    def __init__(self):
        self.model = None
        self.processor = None
        self.tokenizer = None
        self.current_model = None
        self.current_device = None
        self.load_time_stats = {}
    
    @staticmethod
    def _scan_available_models(qwen_models_dir):
        """æ‰«æå¯ç”¨çš„Qwen VLæ¨¡å‹"""
        available_models = []
        
        if not os.path.exists(qwen_models_dir):
            return ["No models found in models/Qwen/"]
        
        for item in os.listdir(qwen_models_dir):
            model_path = os.path.join(qwen_models_dir, item)
            
            if not os.path.isdir(model_path):
                continue
                
            config_path = os.path.join(model_path, "config.json")
            if not os.path.exists(config_path):
                continue
            
            try:
                with open(config_path, 'r') as f:
                    config = json.load(f)
                model_type = config.get("model_type", "").lower()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯VLæ¨¡å‹
                if "vl" in model_type or "VL" in item:
                    available_models.append(item)
                else:
                    available_models.append(f"[éVLæ¨¡å‹] {item}")
            except:
                available_models.append(item)
        
        return available_models if available_models else ["No models found in models/Qwen/"]
    
    def _determine_device(self, device):
        """ç¡®å®šä½¿ç”¨çš„è®¾å¤‡"""
        if device == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        return device
    
    def _cleanup_previous_model(self):
        """æ¸…ç†ä¹‹å‰åŠ è½½çš„æ¨¡å‹"""
        if self.model is not None:
            del self.model
            self.model = None
        if self.processor is not None:
            del self.processor
            self.processor = None
        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None
        
        self.current_model = None
        self.current_device = None
        
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("[Qwen Image Captioner] Model unloaded and memory cleared")
    
    def _get_model_config(self, model_path):
        """è¯»å–æ¨¡å‹é…ç½®"""
        config_path = os.path.join(model_path, "config.json")
        with open(config_path, 'r') as f:
            return json.load(f)
    
    def _fast_load_processor(self, model_path):
        """å¿«é€ŸåŠ è½½å¤„ç†å™¨"""
        def load_processor():
            return AutoProcessor.from_pretrained(
                model_path,
                trust_remote_code=True
            )
        
        # ä½¿ç”¨çº¿ç¨‹å¹¶è¡ŒåŠ è½½
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(load_processor)
            return future.result()
    
    def _load_model_fast(self, model_path, model_type, use_flash, fast_load):
        """ä¼˜åŒ–çš„æ¨¡å‹åŠ è½½æ–¹æ³•"""
        start_time = time.time()
        
        # ç¡®å®šè®¾å¤‡
        device = self.device
        
        # è·å–ä¼˜åŒ–çš„åŠ è½½å‚æ•°
        model_kwargs = FastModelLoader.optimize_model_loading_params(device)
        
        # å¦‚æœä¸ä½¿ç”¨FlashAttentionï¼Œå›é€€åˆ°SDPA
        if not use_flash or not FLASH_ATTENTION_AVAILABLE:
            model_kwargs["attn_implementation"] = "sdpa"
            print("[Qwen Image Captioner] Using SDPA attention")
        
        try:
            # å¹¶è¡ŒåŠ è½½å¤„ç†å™¨
            processor_thread = threading.Thread(
                target=lambda: setattr(self, 'processor', self._fast_load_processor(model_path))
            )
            processor_thread.start()
            
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åŠ è½½æ–¹å¼
            if "qwen2_5_vl" in model_type and QWEN2_5VL_AVAILABLE:
                print("[Qwen Image Captioner] Loading Qwen2.5-VL model...")
                
                if fast_load and ACCELERATE_AVAILABLE:
                    # å°è¯•ä½¿ç”¨accelerateåŠ é€ŸåŠ è½½
                    self.model = FastModelLoader.load_with_accelerate(
                        Qwen2_5VLForConditionalGeneration,
                        model_path,
                        device,
                        model_kwargs.get("torch_dtype", torch.float32)
                    )
                
                if self.model is None:
                    # æ ‡å‡†åŠ è½½æ–¹å¼
                    self.model = Qwen2_5VLForConditionalGeneration.from_pretrained(
                        model_path,
                        **model_kwargs
                    )
                    
            elif "qwen2_vl" in model_type and QWEN2VL_AVAILABLE:
                print("[Qwen Image Captioner] Loading Qwen2-VL model...")
                
                if fast_load and ACCELERATE_AVAILABLE:
                    self.model = FastModelLoader.load_with_accelerate(
                        Qwen2VLForConditionalGeneration,
                        model_path,
                        device,
                        model_kwargs.get("torch_dtype", torch.float32)
                    )
                
                if self.model is None:
                    self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                        model_path,
                        **model_kwargs
                    )
            else:
                # ä½¿ç”¨AutoModel
                print("[Qwen Image Captioner] Loading with AutoModel...")
                from transformers import AutoModelForVision2Seq
                
                try:
                    self.model = AutoModelForVision2Seq.from_pretrained(
                        model_path,
                        **model_kwargs
                    )
                except:
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        **model_kwargs
                    )
            
            # ç­‰å¾…å¤„ç†å™¨åŠ è½½å®Œæˆ
            processor_thread.join()
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            # å¯ç”¨ä¼˜åŒ–
            if device == "cuda":
                # å¯ç”¨cudnnä¼˜åŒ–
                torch.backends.cudnn.benchmark = True
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.allow_tf32 = True
                
                # ç¼–è¯‘æ¨¡å‹ï¼ˆå¦‚æœæ”¯æŒï¼‰
                if hasattr(torch, 'compile') and fast_load:
                    try:
                        print("[Qwen Image Captioner] Compiling model with torch.compile...")
                        self.model = torch.compile(
                            self.model, 
                            mode="reduce-overhead",
                            fullgraph=False
                        )
                    except Exception as e:
                        print(f"[Qwen Image Captioner] Compilation failed: {e}")
            
            load_time = time.time() - start_time
            self.load_time_stats[model_path] = load_time
            print(f"[Qwen Image Captioner] Model loaded in {load_time:.2f} seconds")
            
            return True
            
        except Exception as e:
            print(f"[Qwen Image Captioner] Error loading model: {e}")
            return False
    
    def load_model(self, model_name, device="auto", use_flash=True, fast_load=True):
        """åŠ è½½Qwenè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸æ”¯æŒçš„æ ¼å¼
        if "[éVLæ¨¡å‹]" in model_name:
            raise ValueError(
                "è¿™ä¸æ˜¯ä¸€ä¸ªè§†è§‰è¯­è¨€(VL)æ¨¡å‹ï¼\n"
                "å›¾åƒæè¿°éœ€è¦ä½¿ç”¨åŒ…å«è§†è§‰ç¼–ç å™¨çš„VLæ¨¡å‹ã€‚\n"
                "è¯·ä¸‹è½½Qwen-VLæˆ–Qwen2-VLç³»åˆ—æ¨¡å‹ã€‚"
            )
        
        # ç¡®å®šè®¾å¤‡
        self.device = self._determine_device(device)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½
        if self.current_model == model_name and self.current_device == device:
            print(f"[Qwen Image Captioner] Model {model_name} already loaded")
            return
        
        # æ¸…ç†ä¹‹å‰çš„æ¨¡å‹
        self._cleanup_previous_model()
        
        # æ¨¡å‹è·¯å¾„
        model_path = os.path.join(folder_paths.models_dir, "Qwen", model_name)
        
        try:
            # è¯»å–é…ç½®
            config = self._get_model_config(model_path)
            model_type = config.get("model_type", "").lower()
            
            print(f"[Qwen Image Captioner] Loading model: {model_name}")
            print(f"[Qwen Image Captioner] Model type: {model_type}")
            
            # åŠ è½½æ¨¡å‹
            success = self._load_model_fast(model_path, model_type, use_flash, fast_load)
            
            if not success:
                raise ValueError(f"Failed to load model {model_name}")
            
            self.current_model = model_name
            self.current_device = device
            
            # æ˜¾ç¤ºåŠ è½½ç»Ÿè®¡
            if model_path in self.load_time_stats:
                avg_time = sum(self.load_time_stats.values()) / len(self.load_time_stats)
                print(f"[Qwen Image Captioner] Average load time: {avg_time:.2f}s")
            
        except Exception as e:
            print(f"[Qwen Image Captioner] Error: {str(e)}")
            raise
    
    def _resize_image(self, pil_image, max_size=1024):
        """ç­‰æ¯”ä¾‹ç¼©æ”¾å›¾åƒåˆ°æŒ‡å®šæœ€å¤§å°ºå¯¸"""
        width, height = pil_image.size
        
        if width <= max_size and height <= max_size:
            return pil_image
        
        scale = max_size / max(width, height)
        new_width = int(width * scale)
        new_height = int(height * scale)
        
        resized_image = pil_image.resize((new_width, new_height), Image.Resampling.LANCZOS)
        return resized_image
    
    def _prepare_image(self, image, max_size=1024):
        """å‡†å¤‡å›¾åƒè¾“å…¥"""
        if isinstance(image, torch.Tensor):
            if len(image.shape) == 4:
                image = image[0]
            image_np = (image.cpu().numpy() * 255).astype(np.uint8)
            pil_image = Image.fromarray(image_np)
        else:
            pil_image = image
        
        pil_image = self._resize_image(pil_image, max_size)
        return pil_image
    
    def _get_instruction(self, prompt_type, language, custom_instruction):
        """æ ¹æ®ç±»å‹å’Œè¯­è¨€ç”ŸæˆæŒ‡ä»¤"""
        instructions = {
            "ä¸­æ–‡": {
                "detailed": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ï¼ŒåŒ…æ‹¬æ‰€æœ‰çš„ç‰©ä½“ã€äººç‰©ã€é¢œè‰²ã€çº¹ç†ã€æ„å›¾ã€å…‰çº¿å’Œæ°›å›´ã€‚è¯·å…·ä½“è¯´æ˜ç©ºé—´å…³ç³»å’Œè§†è§‰å…ƒç´ ã€‚",
                "brief": "è¯·ç®€æ´åœ°æè¿°å›¾ç‰‡çš„ä¸»è¦ä¸»é¢˜å’Œå…³é”®å…ƒç´ ã€‚",
                "technical": "è¯·ä»æŠ€æœ¯è§’åº¦æè¿°è¿™å¼ å›¾ç‰‡ï¼šæ„å›¾ã€å…‰çº¿ã€è‰²å½©æ­é…ã€æ‹æ‘„è§’åº¦ã€æ™¯æ·±ä»¥åŠä»»ä½•åæœŸå¤„ç†æ•ˆæœã€‚",
                "artistic": "è¯·ä»è‰ºæœ¯è§’åº¦æè¿°è¿™å¼ å›¾ç‰‡ï¼Œé‡ç‚¹å…³æ³¨è‰ºæœ¯é£æ ¼ã€æƒ…ç»ªã€ç¾å­¦ç‰¹è´¨å’Œæƒ…æ„Ÿå½±å“ã€‚åŒ…æ‹¬è‰ºæœ¯æŠ€å·§å’Œè§†è§‰å™äº‹çš„ç»†èŠ‚ã€‚",
                "custom": custom_instruction if custom_instruction else "è¯·æè¿°è¿™å¼ å›¾ç‰‡ã€‚"
            },
            "English": {
                "detailed": "Describe this image in great detail, including all objects, people, colors, textures, composition, lighting, and atmosphere. Be specific about spatial relationships and visual elements.",
                "brief": "Provide a concise description of the main subject and key elements in this image.",
                "technical": "Describe this image focusing on technical aspects: composition, lighting, color palette, camera angle, depth of field, and any post-processing effects.",
                "artistic": "Describe this image with focus on artistic style, mood, aesthetic qualities, and emotional impact. Include details about artistic techniques and visual storytelling.",
                "custom": custom_instruction if custom_instruction else "Describe this image."
            }
        }
        
        return instructions[language][prompt_type]
    
    def _generate_optimized(self, pil_image, instruction, max_length, temperature, 
                           num_beams=1, use_cache=True):
        """ä¼˜åŒ–çš„ç”Ÿæˆæ–¹æ³•"""
        if self.processor and hasattr(self.processor, 'apply_chat_template'):
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": instruction}
                    ]
                }
            ]
            
            text = self.processor.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.processor(
                text=[text],
                images=[pil_image],
                return_tensors="pt"
            )
        elif self.tokenizer and hasattr(self.tokenizer, 'from_list_format'):
            query = self.tokenizer.from_list_format([
                {'image': pil_image},
                {'text': instruction}
            ])
            inputs = self.tokenizer(query, return_tensors='pt')
        else:
            raise NotImplementedError("Unsupported model format")
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # ç”Ÿæˆé…ç½®
        generation_config = GenerationConfig(
            max_new_tokens=max_length,
            temperature=temperature,
            do_sample=temperature > 0.1,
            num_beams=num_beams,
            use_cache=use_cache,
            pad_token_id=self.processor.tokenizer.pad_token_id if self.processor else self.tokenizer.pad_token_id,
            eos_token_id=self.processor.tokenizer.eos_token_id if self.processor else self.tokenizer.eos_token_id,
        )
        
        # ä½¿ç”¨æ··åˆç²¾åº¦æ¨ç†
        with torch.cuda.amp.autocast(enabled=(self.device == "cuda")):
            with torch.no_grad():
                outputs = self.model.generate(**inputs, generation_config=generation_config)
        
        return outputs
    
    def _decode_output(self, outputs, instruction):
        """è§£ç æ¨¡å‹è¾“å‡º"""
        if hasattr(outputs, 'cpu'):
            outputs = outputs.cpu()
        
        if self.processor and hasattr(self.processor, 'decode'):
            response = self.processor.decode(outputs[0], skip_special_tokens=True)
        elif self.tokenizer:
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        else:
            response = self.processor.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        if instruction in response:
            caption = response.split(instruction)[-1].strip()
        else:
            caption = response.split('\n')[-1].strip() if '\n' in response else response
        
        return caption
    
    def generate_caption(self, image, model_name, prompt_type, language, device, 
                        max_length, temperature, auto_unload=True, 
                        custom_instruction="", max_image_size=1024,
                        use_flash_attention=True, fast_load=True,
                        num_beams=1, use_cache=True):
        """ç”Ÿæˆå›¾åƒæè¿°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        
        try:
            total_start = time.time()
            
            # åŠ è½½æ¨¡å‹
            load_start = time.time()
            self.load_model(model_name, device, use_flash_attention, fast_load)
            load_time = time.time() - load_start
            
            # å‡†å¤‡å›¾åƒ
            prep_start = time.time()
            pil_image = self._prepare_image(image, max_image_size)
            prep_time = time.time() - prep_start
            
            # è·å–æŒ‡ä»¤
            instruction = self._get_instruction(prompt_type, language, custom_instruction)
            
            # ç”Ÿæˆæè¿°
            gen_start = time.time()
            outputs = self._generate_optimized(
                pil_image, instruction, max_length, temperature,
                num_beams, use_cache
            )
            gen_time = time.time() - gen_start
            
            # è§£ç è¾“å‡º
            decode_start = time.time()
            caption = self._decode_output(outputs, instruction)
            decode_time = time.time() - decode_start
            
            total_time = time.time() - total_start
            
            # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
            print(f"[Qwen Image Captioner] Performance breakdown:")
            print(f"  - Model loading: {load_time:.2f}s")
            print(f"  - Image prep: {prep_time:.3f}s")
            print(f"  - Generation: {gen_time:.2f}s")
            print(f"  - Decoding: {decode_time:.3f}s")
            print(f"  - Total time: {total_time:.2f}s")
            
            if FLASH_ATTENTION_AVAILABLE and use_flash_attention:
                print(f"[Qwen Image Captioner] FlashAttention2 enabled âœ“")
            
            return (caption,)
            
        except Exception as e:
            error_msg = f"Error: {str(e)}"
            print(f"[Qwen Image Captioner] {error_msg}")
            
            if "CUDA" in str(e):
                print("[Qwen Image Captioner] Tips:")
                print("1. Try using CPU instead")
                print("2. Reduce max_length")
                print("3. Check GPU memory")
            
            return (error_msg,)
        
        finally:
            if auto_unload:
                print("[Qwen Image Captioner] Auto-unloading model...")
                self._cleanup_previous_model()


# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "QwenImageCaptioner": QwenImageCaptioner,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "QwenImageCaptioner": "ğŸ³Qwen Image Captioner",
}