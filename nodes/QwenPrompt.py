import torch
import os
import folder_paths
from PIL import Image
import numpy as np
from transformers.generation import GenerationConfig
import gc
import json
import time
import threading
from collections import OrderedDict
from typing import Optional, Dict, Any, Tuple, List, Union
from dataclasses import dataclass

# ============================================================================
# ä¾èµ–æ£€æµ‹
# ============================================================================

class DependencyStatus:
    """ä¾èµ–çŠ¶æ€ç®¡ç†"""
    qwen2vl_available: bool = False
    qwen2_5vl_available: bool = False
    flash_attention_available: bool = False
    accelerate_available: bool = False
    bitsandbytes_available: bool = False

_deps = DependencyStatus()

# Qwen2VL
try:
    from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor
    _deps.qwen2vl_available = True
except ImportError:
    pass

# Qwen2.5VL
try:
    from transformers import Qwen2_5VLForConditionalGeneration, AutoProcessor as Qwen2_5VLProcessor
    _deps.qwen2_5vl_available = True
except ImportError:
    pass

# åŸºç¡€ç±»
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor, BitsAndBytesConfig

# FlashAttention2
try:
    from flash_attn import flash_attn_func
    _deps.flash_attention_available = True
    print("[Qwen Captioner] âœ“ FlashAttention2 available")
except ImportError:
    print("[Qwen Captioner] âœ— FlashAttention2 not available (pip install flash-attn)")

# Accelerate
try:
    from accelerate import init_empty_weights, load_checkpoint_and_dispatch
    _deps.accelerate_available = True
    print("[Qwen Captioner] âœ“ Accelerate available")
except ImportError:
    print("[Qwen Captioner] âœ— Accelerate not available (pip install accelerate)")

# bitsandbytes (é‡åŒ–æ”¯æŒ)
try:
    import bitsandbytes as bnb
    _deps.bitsandbytes_available = True
    print("[Qwen Captioner] âœ“ BitsAndBytes available (INT8/INT4 quantization)")
except ImportError:
    print("[Qwen Captioner] âœ— BitsAndBytes not available (pip install bitsandbytes)")


# ============================================================================
# é”™è¯¯ç±»å‹å®šä¹‰
# ============================================================================

class QwenCaptionerError(Exception):
    """åŸºç¡€å¼‚å¸¸ç±»"""
    pass

class ModelNotFoundError(QwenCaptionerError):
    """æ¨¡å‹æœªæ‰¾åˆ°"""
    pass

class ModelLoadError(QwenCaptionerError):
    """æ¨¡å‹åŠ è½½å¤±è´¥"""
    pass

class InvalidModelTypeError(QwenCaptionerError):
    """æ— æ•ˆçš„æ¨¡å‹ç±»å‹"""
    pass

class ImageProcessingError(QwenCaptionerError):
    """å›¾åƒå¤„ç†é”™è¯¯"""
    pass

class GenerationError(QwenCaptionerError):
    """ç”Ÿæˆé”™è¯¯"""
    pass

class CUDAOutOfMemoryError(QwenCaptionerError):
    """æ˜¾å­˜ä¸è¶³"""
    pass

class QuantizationError(QwenCaptionerError):
    """é‡åŒ–é”™è¯¯"""
    pass


# ============================================================================
# æ•°æ®ç±»
# ============================================================================

@dataclass
class ModelInfo:
    """æ¨¡å‹ä¿¡æ¯"""
    name: str
    path: str
    model_type: str
    is_vl_model: bool
    config: Dict[str, Any]


@dataclass
class LoadedModel:
    """å·²åŠ è½½çš„æ¨¡å‹"""
    model: Any
    processor: Any
    tokenizer: Any
    device: str
    dtype: torch.dtype
    quantization: str = "none"
    load_time: float = 0.0


@dataclass
class PerformanceStats:
    """æ€§èƒ½ç»Ÿè®¡"""
    model_load_time: float = 0.0
    image_prep_time: float = 0.0
    generation_time: float = 0.0
    decode_time: float = 0.0
    total_time: float = 0.0
    peak_memory_mb: float = 0.0
    
    def __str__(self) -> str:
        mem_str = f"  â€¢ Peak memory:   {self.peak_memory_mb:.1f}MB\n" if self.peak_memory_mb > 0 else ""
        return (
            f"Performance Stats:\n"
            f"  â€¢ Model loading: {self.model_load_time:.2f}s\n"
            f"  â€¢ Image prep:    {self.image_prep_time:.3f}s\n"
            f"  â€¢ Generation:    {self.generation_time:.2f}s\n"
            f"  â€¢ Decoding:      {self.decode_time:.3f}s\n"
            f"{mem_str}"
            f"  â€¢ Total:         {self.total_time:.2f}s"
        )


# ============================================================================
# LRUæ¨¡å‹ç¼“å­˜
# ============================================================================

class ModelCache:
    """LRUæ¨¡å‹ç¼“å­˜ï¼Œæ”¯æŒå¤šæ¨¡å‹çƒ­åˆ‡æ¢"""
    
    def __init__(self, max_size: int = 2):
        self.max_size = max_size
        self._cache: OrderedDict[str, LoadedModel] = OrderedDict()
        self._lock = threading.Lock()
    
    def get(self, key: str) -> Optional[LoadedModel]:
        """è·å–ç¼“å­˜çš„æ¨¡å‹"""
        with self._lock:
            if key in self._cache:
                self._cache.move_to_end(key)
                return self._cache[key]
            return None
    
    def put(self, key: str, model: LoadedModel) -> None:
        """ç¼“å­˜æ¨¡å‹"""
        with self._lock:
            if key in self._cache:
                self._cache.move_to_end(key)
                self._cache[key] = model
            else:
                if len(self._cache) >= self.max_size:
                    oldest_key, oldest_model = self._cache.popitem(last=False)
                    self._unload_model(oldest_model, oldest_key)
                self._cache[key] = model
    
    def remove(self, key: str) -> None:
        """ç§»é™¤æŒ‡å®šæ¨¡å‹"""
        with self._lock:
            if key in self._cache:
                model = self._cache.pop(key)
                self._unload_model(model, key)
    
    def clear(self) -> None:
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        with self._lock:
            for key, model in list(self._cache.items()):
                self._unload_model(model, key)
            self._cache.clear()
    
    def _unload_model(self, loaded_model: LoadedModel, key: str) -> None:
        """å¸è½½æ¨¡å‹å¹¶é‡Šæ”¾å†…å­˜"""
        print(f"[Qwen Captioner] Unloading cached model: {key}")
        
        if loaded_model.model is not None:
            del loaded_model.model
        if loaded_model.processor is not None:
            del loaded_model.processor
        if loaded_model.tokenizer is not None:
            del loaded_model.tokenizer
        
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
    
    @property
    def cached_models(self) -> List[str]:
        """è¿”å›ç¼“å­˜çš„æ¨¡å‹åˆ—è¡¨"""
        with self._lock:
            return list(self._cache.keys())
    
    def __len__(self) -> int:
        return len(self._cache)


# å…¨å±€æ¨¡å‹ç¼“å­˜
_model_cache = ModelCache(max_size=2)


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def get_dtype_from_precision(precision: str, device: str) -> torch.dtype:
    """æ ¹æ®ç²¾åº¦è®¾ç½®è·å–æ•°æ®ç±»å‹"""
    if device == "cpu":
        return torch.float32
    
    if precision == "bf16":
        if torch.cuda.is_bf16_supported():
            return torch.bfloat16
        print("[Qwen Captioner] BF16 not supported, using FP16")
        return torch.float16
    elif precision in ("int8", "int4"):
        # é‡åŒ–æ¨¡å‹ä½¿ç”¨bf16ä½œä¸ºè®¡ç®—ç±»å‹
        if torch.cuda.is_bf16_supported():
            return torch.bfloat16
        return torch.float16
    else:
        return torch.float32


def get_attention_implementation(attention_mode: str) -> str:
    """è·å–æ³¨æ„åŠ›å®ç°æ–¹å¼"""
    if attention_mode == "auto":
        # è‡ªåŠ¨é€‰æ‹©ï¼šä¼˜å…ˆFlashAttention2ï¼Œå…¶æ¬¡SDPA
        if _deps.flash_attention_available:
            return "flash_attention_2"
        return "sdpa"
    elif attention_mode == "flash_attention_2":
        if not _deps.flash_attention_available:
            print("[Qwen Captioner] âš  FlashAttention2 not available, falling back to SDPA")
            return "sdpa"
        return "flash_attention_2"
    elif attention_mode == "sdpa":
        return "sdpa"
    elif attention_mode == "eager":
        return "eager"
    else:
        return "sdpa"


def clear_cuda_memory():
    """æ¸…ç†CUDAå†…å­˜"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()


def get_gpu_memory_usage() -> float:
    """è·å–GPUå†…å­˜ä½¿ç”¨é‡(MB)"""
    if torch.cuda.is_available():
        return torch.cuda.max_memory_allocated() / 1024 / 1024
    return 0.0


def reset_gpu_memory_stats():
    """é‡ç½®GPUå†…å­˜ç»Ÿè®¡"""
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()


# ============================================================================
# å›¾åƒå¤„ç†å™¨
# ============================================================================

class ImageProcessor:
    """ä¼˜åŒ–çš„å›¾åƒå¤„ç†å™¨"""
    
    @staticmethod
    def tensor_to_pil(image: torch.Tensor) -> Image.Image:
        """å°†tensorè½¬æ¢ä¸ºPILå›¾åƒ - åŸåœ°æ“ä½œä¼˜åŒ–"""
        if len(image.shape) == 4:
            image = image[0]
        
        # é¿å…ä¸å¿…è¦çš„æ‹·è´ï¼Œç›´æ¥æ“ä½œ
        if image.device.type == "cuda":
            image = image.cpu()
        
        # ä½¿ç”¨ numpy çš„åŸåœ°æ“ä½œ
        image_np = image.numpy()
        np.multiply(image_np, 255, out=image_np)
        image_np = image_np.astype(np.uint8)
        
        return Image.fromarray(image_np)
    
    @staticmethod
    def resize_image(
        pil_image: Image.Image, 
        max_size: int = 1024,
        resample: Image.Resampling = Image.Resampling.BILINEAR
    ) -> Image.Image:
        """ç­‰æ¯”ä¾‹ç¼©æ”¾å›¾åƒ"""
        width, height = pil_image.size
        
        if width <= max_size and height <= max_size:
            return pil_image
        
        scale = max_size / max(width, height)
        new_width = int(width * scale)
        new_height = int(height * scale)
        
        return pil_image.resize((new_width, new_height), resample)
    
    @staticmethod
    def prepare_single(
        image: Union[torch.Tensor, Image.Image], 
        max_size: int = 1024
    ) -> Image.Image:
        """å‡†å¤‡å•å¼ å›¾åƒ"""
        if isinstance(image, torch.Tensor):
            pil_image = ImageProcessor.tensor_to_pil(image)
        else:
            pil_image = image
        
        # ç¡®ä¿æ˜¯RGBæ¨¡å¼
        if pil_image.mode != "RGB":
            pil_image = pil_image.convert("RGB")
        
        return ImageProcessor.resize_image(pil_image, max_size)


# ============================================================================
# é‡åŒ–é…ç½®
# ============================================================================

class QuantizationConfig:
    """é‡åŒ–é…ç½®ç®¡ç†"""
    
    @staticmethod
    def get_config(precision: str) -> Optional[BitsAndBytesConfig]:
        """è·å–é‡åŒ–é…ç½®"""
        if precision == "int8":
            if not _deps.bitsandbytes_available:
                raise QuantizationError(
                    "INT8é‡åŒ–éœ€è¦bitsandbytesåº“\n"
                    "å®‰è£…: pip install bitsandbytes"
                )
            return BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_enable_fp32_cpu_offload=True,
            )
        
        elif precision == "int4":
            if not _deps.bitsandbytes_available:
                raise QuantizationError(
                    "INT4é‡åŒ–éœ€è¦bitsandbytesåº“\n"
                    "å®‰è£…: pip install bitsandbytes"
                )
            # ä¼˜å…ˆä½¿ç”¨bf16ä½œä¸ºè®¡ç®—ç±»å‹
            compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
            return BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=compute_dtype,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )
        
        return None
    
    @staticmethod
    def get_memory_estimate(precision: str, model_params_b: float = 7.0) -> str:
        """ä¼°ç®—å†…å­˜ä½¿ç”¨"""
        estimates = {
            "bf16": model_params_b * 2,
            "int8": model_params_b * 1,
            "int4": model_params_b * 0.5,
        }
        mem = estimates.get(precision, model_params_b * 2)
        return f"~{mem:.1f}GB VRAM"


# ============================================================================
# æ¨¡å‹åŠ è½½å™¨
# ============================================================================

class ModelLoader:
    """ä¼˜åŒ–çš„æ¨¡å‹åŠ è½½å™¨"""
    
    @staticmethod
    def scan_models(qwen_dir: str) -> List[str]:
        """æ‰«æå¯ç”¨æ¨¡å‹"""
        available = []
        
        if not os.path.exists(qwen_dir):
            return ["No models found in models/Qwen/"]
        
        for item in os.listdir(qwen_dir):
            model_path = os.path.join(qwen_dir, item)
            
            if not os.path.isdir(model_path):
                continue
            
            config_path = os.path.join(model_path, "config.json")
            if not os.path.exists(config_path):
                continue
            
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                model_type = config.get("model_type", "").lower()
                
                if "vl" in model_type or "VL" in item:
                    available.append(item)
                else:
                    available.append(f"[éVL] {item}")
            except (json.JSONDecodeError, IOError):
                available.append(f"[é…ç½®é”™è¯¯] {item}")
        
        return available if available else ["No models found in models/Qwen/"]
    
    @staticmethod
    def get_model_info(model_path: str) -> ModelInfo:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        config_path = os.path.join(model_path, "config.json")
        
        if not os.path.exists(config_path):
            raise ModelNotFoundError(f"Config not found: {config_path}")
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
        except json.JSONDecodeError as e:
            raise ModelLoadError(f"Invalid config.json: {e}")
        
        model_type = config.get("model_type", "").lower()
        name = os.path.basename(model_path)
        is_vl = "vl" in model_type or "VL" in name
        
        return ModelInfo(
            name=name,
            path=model_path,
            model_type=model_type,
            is_vl_model=is_vl,
            config=config
        )
    
    @staticmethod
    def get_loading_params(
        device: str, 
        attention_mode: str, 
        precision: str
    ) -> Dict[str, Any]:
        """è·å–æ¨¡å‹åŠ è½½å‚æ•°"""
        params = {
            "trust_remote_code": True,
            "low_cpu_mem_usage": True,
        }
        
        if device == "cuda" and torch.cuda.is_available():
            params["device_map"] = "auto"
            params["torch_dtype"] = get_dtype_from_precision(precision, device)
            
            # é‡åŒ–æ¨¡å¼ä¸‹å¼ºåˆ¶ä½¿ç”¨SDPAï¼ŒFlashAttention2ä¸é‡åŒ–ä¸å…¼å®¹
            is_quantized = precision in ("int8", "int4")
            if is_quantized:
                if attention_mode == "flash_attention_2":
                    print("[Qwen Captioner] âš  é‡åŒ–æ¨¡å¼ä¸æ”¯æŒFlashAttention2ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°SDPA")
                params["attn_implementation"] = "sdpa"
            else:
                params["attn_implementation"] = get_attention_implementation(attention_mode)
            
            # é‡åŒ–é…ç½®
            quant_config = QuantizationConfig.get_config(precision)
            if quant_config is not None:
                params["quantization_config"] = quant_config
                print(f"[Qwen Captioner] Using {precision.upper()} quantization")
        else:
            params["device_map"] = "cpu"
            params["torch_dtype"] = torch.float32
        
        return params
    
    @staticmethod
    def load_processor(model_path: str) -> Any:
        """åŠ è½½å¤„ç†å™¨"""
        return AutoProcessor.from_pretrained(
            model_path,
            trust_remote_code=True
        )
    
    @staticmethod
    def load_model(
        model_info: ModelInfo,
        device: str,
        attention_mode: str,
        precision: str
    ) -> Tuple[Any, Any, Any]:
        """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨"""
        
        model_path = model_info.path
        model_type = model_info.model_type
        params = ModelLoader.get_loading_params(device, attention_mode, precision)
        
        model = None
        processor = None
        tokenizer = None
        
        # å¹¶è¡ŒåŠ è½½å¤„ç†å™¨
        processor_result = [None]
        processor_error = [None]
        
        def load_processor_async():
            try:
                processor_result[0] = ModelLoader.load_processor(model_path)
            except Exception as e:
                processor_error[0] = e
        
        processor_thread = threading.Thread(target=load_processor_async)
        processor_thread.start()
        
        print(f"[Qwen Captioner] Loading model with {precision.upper()} precision...")
        
        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åŠ è½½æ–¹å¼
            if "qwen2_5_vl" in model_type and _deps.qwen2_5vl_available:
                print("[Qwen Captioner] Loading Qwen2.5-VL model...")
                model = Qwen2_5VLForConditionalGeneration.from_pretrained(
                    model_path, **params
                )
            elif "qwen2_vl" in model_type and _deps.qwen2vl_available:
                print("[Qwen Captioner] Loading Qwen2-VL model...")
                model = Qwen2VLForConditionalGeneration.from_pretrained(
                    model_path, **params
                )
            else:
                print("[Qwen Captioner] Loading with AutoModel...")
                try:
                    from transformers import AutoModelForVision2Seq
                    model = AutoModelForVision2Seq.from_pretrained(
                        model_path, **params
                    )
                except (ImportError, ValueError):
                    model = AutoModelForCausalLM.from_pretrained(
                        model_path, **params
                    )
                    
        except torch.cuda.OutOfMemoryError as e:
            raise CUDAOutOfMemoryError(
                f"GPUå†…å­˜ä¸è¶³ï¼Œæ— æ³•åŠ è½½æ¨¡å‹ã€‚\n"
                f"å»ºè®®ï¼š\n"
                f"1. ä½¿ç”¨INT8æˆ–INT4é‡åŒ–\n"
                f"2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹\n"
                f"3. å…³é—­å…¶ä»–GPUç¨‹åº\n"
                f"åŸå§‹é”™è¯¯: {e}"
            )
        except Exception as e:
            raise ModelLoadError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        # ç­‰å¾…å¤„ç†å™¨åŠ è½½å®Œæˆ
        processor_thread.join()
        
        if processor_error[0]:
            raise ModelLoadError(f"å¤„ç†å™¨åŠ è½½å¤±è´¥: {processor_error[0]}")
        
        processor = processor_result[0]
        
        # è®¾ç½®è¯„ä¼°æ¨¡å¼
        model.eval()
        
        # CUDAä¼˜åŒ–
        if device == "cuda":
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
        
        return model, processor, tokenizer


# ============================================================================
# æŒ‡ä»¤ç”Ÿæˆå™¨
# ============================================================================

class InstructionGenerator:
    """æŒ‡ä»¤ç”Ÿæˆå™¨"""
    
    INSTRUCTIONS = {
        "ä¸­æ–‡": {
            "detailed": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ï¼ŒåŒ…æ‹¬æ‰€æœ‰çš„ç‰©ä½“ã€äººç‰©ã€é¢œè‰²ã€çº¹ç†ã€æ„å›¾ã€å…‰çº¿å’Œæ°›å›´ã€‚è¯·å…·ä½“è¯´æ˜ç©ºé—´å…³ç³»å’Œè§†è§‰å…ƒç´ ã€‚",
            "brief": "è¯·ç®€æ´åœ°æè¿°å›¾ç‰‡çš„ä¸»è¦ä¸»é¢˜å’Œå…³é”®å…ƒç´ ã€‚",
            "technical": "è¯·ä»æŠ€æœ¯è§’åº¦æè¿°è¿™å¼ å›¾ç‰‡ï¼šæ„å›¾ã€å…‰çº¿ã€è‰²å½©æ­é…ã€æ‹æ‘„è§’åº¦ã€æ™¯æ·±ä»¥åŠä»»ä½•åæœŸå¤„ç†æ•ˆæœã€‚",
            "artistic": "è¯·ä»è‰ºæœ¯è§’åº¦æè¿°è¿™å¼ å›¾ç‰‡ï¼Œé‡ç‚¹å…³æ³¨è‰ºæœ¯é£æ ¼ã€æƒ…ç»ªã€ç¾å­¦ç‰¹è´¨å’Œæƒ…æ„Ÿå½±å“ã€‚åŒ…æ‹¬è‰ºæœ¯æŠ€å·§å’Œè§†è§‰å™äº‹çš„ç»†èŠ‚ã€‚",
        },
        "English": {
            "detailed": "Describe this image in great detail, including all objects, people, colors, textures, composition, lighting, and atmosphere. Be specific about spatial relationships and visual elements.",
            "brief": "Provide a concise description of the main subject and key elements in this image.",
            "technical": "Describe this image focusing on technical aspects: composition, lighting, color palette, camera angle, depth of field, and any post-processing effects.",
            "artistic": "Describe this image with focus on artistic style, mood, aesthetic qualities, and emotional impact. Include details about artistic techniques and visual storytelling.",
        }
    }
    
    @classmethod
    def get_instruction(
        cls, 
        prompt_type: str, 
        language: str, 
        custom_instruction: str = ""
    ) -> str:
        """è·å–æŒ‡ä»¤"""
        if prompt_type == "custom":
            return custom_instruction or cls.INSTRUCTIONS[language].get("detailed", "")
        
        return cls.INSTRUCTIONS.get(language, cls.INSTRUCTIONS["English"]).get(
            prompt_type, 
            cls.INSTRUCTIONS["English"]["detailed"]
        )


# ============================================================================
# ç”Ÿæˆå™¨
# ============================================================================

class CaptionGenerator:
    """æè¿°ç”Ÿæˆå™¨ - å†…å­˜ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, loaded_model: LoadedModel):
        self.model = loaded_model.model
        self.processor = loaded_model.processor
        self.tokenizer = loaded_model.tokenizer
        self.device = loaded_model.device
        self.dtype = loaded_model.dtype
        self.quantization = loaded_model.quantization
    
    def _get_pad_token_id(self) -> int:
        """è·å–pad token id"""
        if self.processor and hasattr(self.processor, 'tokenizer'):
            return self.processor.tokenizer.pad_token_id
        if self.tokenizer:
            return self.tokenizer.pad_token_id
        return 0
    
    def _get_eos_token_id(self) -> int:
        """è·å–eos token id"""
        if self.processor and hasattr(self.processor, 'tokenizer'):
            return self.processor.tokenizer.eos_token_id
        if self.tokenizer:
            return self.tokenizer.eos_token_id
        return 0
    
    def _prepare_inputs_inplace(
        self, 
        pil_image: Image.Image, 
        instruction: str
    ) -> Dict[str, torch.Tensor]:
        """å‡†å¤‡æ¨¡å‹è¾“å…¥ - åŸåœ°æ“ä½œä¼˜åŒ–"""
        if self.processor and hasattr(self.processor, 'apply_chat_template'):
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": instruction}
                    ]
                }
            ]
            
            text = self.processor.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.processor(
                text=[text],
                images=[pil_image],
                return_tensors="pt"
            )
        elif self.tokenizer and hasattr(self.tokenizer, 'from_list_format'):
            query = self.tokenizer.from_list_format([
                {'image': pil_image},
                {'text': instruction}
            ])
            inputs = self.tokenizer(query, return_tensors='pt')
        else:
            raise GenerationError("ä¸æ”¯æŒçš„æ¨¡å‹æ ¼å¼")
        
        # åŸåœ°ç§»åŠ¨åˆ°è®¾å¤‡ï¼Œé¿å…é¢å¤–å†…å­˜åˆ†é…
        for k in inputs:
            if isinstance(inputs[k], torch.Tensor):
                inputs[k] = inputs[k].to(self.device, non_blocking=True)
        
        return inputs
    
    def generate(
        self,
        pil_image: Image.Image,
        instruction: str,
        max_length: int = 256,
        temperature: float = 0.7,
        num_beams: int = 1,
        use_cache: bool = True
    ) -> str:
        """ç”Ÿæˆå›¾åƒæè¿° - å†…å­˜ä¼˜åŒ–ç‰ˆ"""
        
        inputs = None
        outputs = None
        
        try:
            # å‡†å¤‡è¾“å…¥
            inputs = self._prepare_inputs_inplace(pil_image, instruction)
            
            # ç”Ÿæˆé…ç½®
            # é‡åŒ–æ¨¡å¼ä¸‹ç¦ç”¨é‡‡æ ·ï¼Œé¿å…multinomial CUDAé”™è¯¯
            is_quantized = self.quantization in ("int8", "int4")
            do_sample = temperature > 0.1 and num_beams == 1 and not is_quantized
            
            if is_quantized and temperature > 0.1:
                print("[Qwen Captioner] âš  é‡åŒ–æ¨¡å¼ä¸‹å·²ç¦ç”¨é‡‡æ ·ï¼Œä½¿ç”¨è´ªå©ªè§£ç ")
            
            generation_config = GenerationConfig(
                max_new_tokens=max_length,
                temperature=temperature if do_sample else 1.0,
                do_sample=do_sample,
                num_beams=num_beams,
                use_cache=use_cache,
                pad_token_id=self._get_pad_token_id(),
                eos_token_id=self._get_eos_token_id(),
            )
            
            # æ··åˆç²¾åº¦æ¨ç†ï¼ˆä½¿ç”¨æ–°APIï¼Œå…¼å®¹CPUï¼‰
            autocast_enabled = self.device == "cuda"
            autocast_device = "cuda" if autocast_enabled else "cpu"
            
            with torch.amp.autocast(device_type=autocast_device, enabled=autocast_enabled):
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs, 
                        generation_config=generation_config
                    )
            
            # è§£ç 
            caption = self._decode_output(outputs, instruction)
            
            return caption
            
        finally:
            # ç§¯ææ¸…ç†ä¸­é—´å¼ é‡
            if inputs is not None:
                for k in list(inputs.keys()):
                    del inputs[k]
                del inputs
            
            if outputs is not None:
                del outputs
            
            # æ¸…ç†CUDAç¼“å­˜
            if self.device == "cuda":
                torch.cuda.empty_cache()
    
    def _decode_output(self, outputs: torch.Tensor, instruction: str) -> str:
        """è§£ç è¾“å‡º"""
        # ç§»åŠ¨åˆ°CPUä»¥é‡Šæ”¾GPUå†…å­˜
        outputs_cpu = outputs.cpu()
        
        # ç»Ÿä¸€çš„è§£ç é€»è¾‘
        decoder = (
            self.processor if self.processor and hasattr(self.processor, 'decode')
            else self.tokenizer if self.tokenizer
            else self.processor.tokenizer if self.processor and hasattr(self.processor, 'tokenizer')
            else None
        )
        
        if decoder is None:
            raise GenerationError("æ— æ³•æ‰¾åˆ°è§£ç å™¨")
        
        response = decoder.decode(outputs_cpu[0], skip_special_tokens=True)
        
        del outputs_cpu
        
        # æå–å®é™…å›ç­”
        if instruction in response:
            caption = response.split(instruction)[-1].strip()
        elif '\n' in response:
            caption = response.split('\n')[-1].strip()
        else:
            caption = response.strip()
        
        return caption


# ============================================================================
# ä¸»èŠ‚ç‚¹ç±»
# ============================================================================

class QwenImageCaptioner:
    """
    ä¼˜åŒ–ç‰ˆComfyUIèŠ‚ç‚¹ - Qwenå›¾åƒæè¿°ç”Ÿæˆ v2
    
    ç‰¹æ€§ï¼š
    - é‡åŒ–æ¨ç† (BF16/INT8/INT4)
    - åŸåœ°å¼ é‡æ“ä½œä¼˜åŒ–
    - LRUå¤šæ¨¡å‹ç¼“å­˜
    - FlashAttention2/SDPAåŠ é€Ÿ
    - è¯¦ç»†æ€§èƒ½ç»Ÿè®¡
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        qwen_dir = os.path.join(folder_paths.models_dir, "Qwen")
        available_models = ModelLoader.scan_models(qwen_dir)
        
        # ç²¾åº¦é€‰é¡¹ï¼ˆç”¨æˆ·è‡ªè¡Œé€‰æ‹©ï¼Œå¦‚æœé€‰æ‹©int8/int4éœ€è¦å®‰è£…bitsandbytesï¼‰
        precision_options = ["bf16", "int8", "int4"]
        
        return {
            "required": {
                "image": ("IMAGE",),
                "model_name": (available_models,),
                "prompt_type": (["detailed", "brief", "technical", "artistic", "custom"],),
                "language": (["English", "ä¸­æ–‡"],),
                "device": (["auto", "cuda", "cpu"],),
                "precision": (precision_options, {
                    "default": "bf16"
                }),
                "max_length": ("INT", {
                    "default": 256,
                    "min": 32,
                    "max": 2048,
                    "step": 32
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1
                }),
                "auto_unload": ("BOOLEAN", {
                    "default": True,
                    "label_on": "Auto Unload",
                    "label_off": "Keep Loaded"
                }),
                "attention_mode": (["auto", "flash_attention_2", "sdpa", "eager"], {
                    "default": "auto"
                }),
            },
            "optional": {
                "custom_instruction": ("STRING", {
                    "multiline": True,
                    "default": "Describe this image in detail for use as a prompt in image generation."
                }),
                "max_image_size": ("INT", {
                    "default": 1024,
                    "min": 512,
                    "max": 2048,
                    "step": 128
                }),
                "num_beams": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 5,
                    "step": 1
                }),
                "use_cache": ("BOOLEAN", {
                    "default": True,
                    "label_on": "Use KV Cache",
                    "label_off": "No Cache"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("caption",)
    FUNCTION = "generate_caption"
    CATEGORY = "ğŸ³Pond/Qwen"
    
    def __init__(self):
        self.stats = PerformanceStats()
    
    def _determine_device(self, device: str) -> str:
        """ç¡®å®šè®¾å¤‡"""
        if device == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        return device
    
    def _get_cache_key(
        self, 
        model_name: str, 
        device: str, 
        attention_mode: str, 
        precision: str
    ) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"{model_name}_{device}_{attention_mode}_{precision}"
    
    def _load_or_get_model(
        self,
        model_name: str,
        device: str,
        attention_mode: str,
        precision: str,
        auto_unload: bool
    ) -> LoadedModel:
        """åŠ è½½æˆ–è·å–ç¼“å­˜çš„æ¨¡å‹"""
        
        # éªŒè¯æ¨¡å‹åç§°
        if model_name.startswith("[éVL]"):
            raise InvalidModelTypeError(
                "è¿™ä¸æ˜¯è§†è§‰è¯­è¨€(VL)æ¨¡å‹ï¼\n"
                "å›¾åƒæè¿°éœ€è¦ä½¿ç”¨Qwen-VLç³»åˆ—æ¨¡å‹ã€‚"
            )
        
        if model_name.startswith("[é…ç½®é”™è¯¯]"):
            raise ModelLoadError(f"æ¨¡å‹é…ç½®æ–‡ä»¶æŸå: {model_name}")
        
        cache_key = self._get_cache_key(model_name, device, attention_mode, precision)
        
        # æ£€æŸ¥ç¼“å­˜
        cached = _model_cache.get(cache_key)
        if cached is not None:
            print(f"[Qwen Captioner] Using cached model: {model_name}")
            return cached
        
        # åŠ è½½æ–°æ¨¡å‹
        model_path = os.path.join(folder_paths.models_dir, "Qwen", model_name)
        
        if not os.path.exists(model_path):
            raise ModelNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        
        model_info = ModelLoader.get_model_info(model_path)
        
        if not model_info.is_vl_model:
            raise InvalidModelTypeError(
                f"'{model_name}' ä¸æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹ã€‚\n"
                f"æ£€æµ‹åˆ°çš„ç±»å‹: {model_info.model_type}"
            )
        
        # é‡ç½®å†…å­˜ç»Ÿè®¡
        reset_gpu_memory_stats()
        
        start_time = time.perf_counter()
        
        model, processor, tokenizer = ModelLoader.load_model(
            model_info, device, attention_mode, precision
        )
        
        load_time = time.perf_counter() - start_time
        
        loaded_model = LoadedModel(
            model=model,
            processor=processor,
            tokenizer=tokenizer,
            device=device,
            dtype=get_dtype_from_precision(precision, device),
            quantization=precision,
            load_time=load_time
        )
        
        # ç¼“å­˜æ¨¡å‹ï¼ˆå¦‚æœä¸æ˜¯è‡ªåŠ¨å¸è½½æ¨¡å¼ï¼‰
        if not auto_unload:
            _model_cache.put(cache_key, loaded_model)
        
        print(f"[Qwen Captioner] Model loaded in {load_time:.2f}s ({precision.upper()})")
        
        return loaded_model
    
    def generate_caption(
        self,
        image: torch.Tensor,
        model_name: str,
        prompt_type: str,
        language: str,
        device: str,
        precision: str,
        max_length: int,
        temperature: float,
        auto_unload: bool = True,
        attention_mode: str = "auto",
        custom_instruction: str = "",
        max_image_size: int = 1024,
        num_beams: int = 1,
        use_cache: bool = True,
    ) -> Tuple[str]:
        """ç”Ÿæˆå›¾åƒæè¿°"""
        
        total_start = time.perf_counter()
        loaded_model = None
        
        try:
            # ç¡®å®šè®¾å¤‡
            device = self._determine_device(device)
            
            # é‡ç½®å†…å­˜ç»Ÿè®¡
            reset_gpu_memory_stats()
            
            # åŠ è½½æ¨¡å‹
            load_start = time.perf_counter()
            loaded_model = self._load_or_get_model(
                model_name, device, attention_mode, 
                precision, auto_unload
            )
            self.stats.model_load_time = time.perf_counter() - load_start
            
            # å‡†å¤‡å›¾åƒ
            prep_start = time.perf_counter()
            pil_image = ImageProcessor.prepare_single(image, max_image_size)
            self.stats.image_prep_time = time.perf_counter() - prep_start
            
            # è·å–æŒ‡ä»¤
            instruction = InstructionGenerator.get_instruction(
                prompt_type, language, custom_instruction
            )
            
            # åˆ›å»ºç”Ÿæˆå™¨
            generator = CaptionGenerator(loaded_model)
            
            # ç”Ÿæˆæè¿°
            gen_start = time.perf_counter()
            caption = generator.generate(
                pil_image, instruction, max_length,
                temperature, num_beams, use_cache
            )
            self.stats.generation_time = time.perf_counter() - gen_start
            
            # è®¡ç®—æ€»æ—¶é—´å’Œå†…å­˜
            self.stats.total_time = time.perf_counter() - total_start
            self.stats.peak_memory_mb = get_gpu_memory_usage()
            
            # æ‰“å°æ€§èƒ½ç»Ÿè®¡
            print(f"[Qwen Captioner] {self.stats}")
            
            # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
            attn_impl = get_attention_implementation(attention_mode)
            print(f"[Qwen Captioner] Config: {precision.upper()}, {attn_impl}")
            
            return (caption,)
            
        except QuantizationError as e:
            error_msg = str(e)
            print(f"[Qwen Captioner] Quantization Error: {error_msg}")
            return (f"Error: {error_msg}",)
            
        except CUDAOutOfMemoryError as e:
            error_msg = str(e)
            print(f"[Qwen Captioner] CUDA OOM: {error_msg}")
            clear_cuda_memory()
            return (f"Error: {error_msg}",)
            
        except (ModelNotFoundError, ModelLoadError, InvalidModelTypeError) as e:
            error_msg = str(e)
            print(f"[Qwen Captioner] Model Error: {error_msg}")
            return (f"Error: {error_msg}",)
            
        except GenerationError as e:
            error_msg = str(e)
            print(f"[Qwen Captioner] Generation Error: {error_msg}")
            return (f"Error: {error_msg}",)
            
        except Exception as e:
            error_msg = f"æœªçŸ¥é”™è¯¯: {type(e).__name__}: {str(e)}"
            print(f"[Qwen Captioner] {error_msg}")
            import traceback
            traceback.print_exc()
            return (f"Error: {error_msg}",)
            
        finally:
            # è‡ªåŠ¨å¸è½½
            if auto_unload and loaded_model is not None:
                print("[Qwen Captioner] Auto-unloading model...")
                if loaded_model.model is not None:
                    del loaded_model.model
                if loaded_model.processor is not None:
                    del loaded_model.processor
                if loaded_model.tokenizer is not None:
                    del loaded_model.tokenizer
                clear_cuda_memory()


# ============================================================================
# èŠ‚ç‚¹æ³¨å†Œ
# ============================================================================

NODE_CLASS_MAPPINGS = {
    "QwenImageCaptioner": QwenImageCaptioner,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "QwenImageCaptioner": "ğŸ³ Qwen Image Captioner (Optimized)",
}