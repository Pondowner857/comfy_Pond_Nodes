import torch
import numpy as np
import os
import sys
from PIL import Image
import folder_paths

# å°è¯•å¯¼å…¥ultralytics
try:
    from ultralytics import YOLO
    from ultralytics import __version__ as ultralytics_version
    print(f"Ultralyticsç‰ˆæœ¬: {ultralytics_version}")
except ImportError:
    print("è­¦å‘Š: æœªå®‰è£…ultralyticsåº“ï¼Œè¯·è¿è¡Œ: pip install ultralytics>=8.2.0")
    YOLO = None

class YoloV11BboxesCropNode:
    """YOLOv11ä¸“ç”¨æ£€æµ‹å’Œè£å‰ªèŠ‚ç‚¹"""

    def __init__(self):
        self.model = None
        self.current_model_name = None
        self.device = None
        
    @classmethod
    def INPUT_TYPES(cls):
        # è·å–YOLOæ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„
        yolo_models_dir = os.path.join(folder_paths.models_dir, "yolo")
        
        # è·å–å¯ç”¨çš„YOLOv11æ¨¡å‹åˆ—è¡¨
        model_files = []
        if os.path.exists(yolo_models_dir):
            for file in os.listdir(yolo_models_dir):
                # ç­›é€‰v11æ¨¡å‹
                if file.endswith(('.pt', '.onnx')) and ('v11' in file.lower() or 'yolo11' in file.lower()):
                    model_files.append(file)
        
        # æ·»åŠ é»˜è®¤YOLOv11æ¨¡å‹é€‰é¡¹
        default_models = ["yolo11n.pt", "yolo11s.pt", "yolo11m.pt", "yolo11l.pt", "yolo11x.pt"]
        for model in default_models:
            if model not in model_files:
                model_files.append(model + " (éœ€ä¸‹è½½)")
        
        if not model_files:
            model_files = ["è¯·å°†YOLOv11æ¨¡å‹æ”¾å…¥models/yoloæ–‡ä»¶å¤¹"]
        
        return {
            "required": {
                "image": ("IMAGE", {"display": "è¾“å…¥å›¾åƒ"}),
                "model_name": (model_files, {
                    "default": model_files[0] if model_files else "yolo11n.pt",
                    "display": "YOLOv11æ¨¡å‹"
                }),
                "device": (["auto", "cpu", "cuda", "mps"], {
                    "default": "auto",
                    "display": "è¿è¡Œè®¾å¤‡"
                }),
                "confidence": ("FLOAT", {
                    "default": 0.25,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "display": "ç½®ä¿¡åº¦é˜ˆå€¼"
                }),
                "iou_threshold": ("FLOAT", {
                    "default": 0.45,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "display": "IOUé˜ˆå€¼"
                }),
                "imgsz": ("INT", {
                    "default": 640,
                    "min": 320,
                    "max": 1280,
                    "step": 32,
                    "display": "æ¨ç†å°ºå¯¸"
                }),
                "max_det": ("INT", {
                    "default": 300,
                    "min": 1,
                    "max": 1000,
                    "step": 10,
                    "display": "æœ€å¤§æ£€æµ‹æ•°"
                }),
                "class_filter": ("STRING", {
                    "default": "å…¨éƒ¨",
                    "multiline": False,
                    "display": "ç±»åˆ«è¿‡æ»¤",
                    "tooltip": "è¦æ£€æµ‹çš„ç±»åˆ«ï¼Œç”¨é€—å·åˆ†éš”ï¼ˆå¦‚ï¼šäºº,æ±½è½¦ï¼‰æˆ–'å…¨éƒ¨'æ£€æµ‹æ‰€æœ‰"
                }),
                "square_size": ("FLOAT", {
                    "default": 100.0,
                    "min": 10.0,
                    "max": 200.0,
                    "step": 1.0,
                    "display": "æ–¹æ¡†å¤§å°%",
                    "tooltip": "åŸºäºæ£€æµ‹å¯¹è±¡å¤§å°çš„ç™¾åˆ†æ¯”è°ƒæ•´"
                }),
                "object_margin": ("FLOAT", {
                    "default": 1.5,
                    "min": 1.0,
                    "max": 3.0,
                    "step": 0.1,
                    "display": "å¯¹è±¡è¾¹è·ç³»æ•°",
                    "tooltip": "åœ¨æ£€æµ‹å¯¹è±¡å‘¨å›´æ·»åŠ çš„é¢å¤–è¾¹è·ç³»æ•°"
                }),
                "vertical_offset": ("FLOAT", {
                    "default": 0.0,
                    "min": -50.0,
                    "max": 50.0,
                    "step": 1.0,
                    "display": "å‚ç›´åç§»%"
                }),
                "horizontal_offset": ("FLOAT", {
                    "default": 0.0,
                    "min": -50.0,
                    "max": 50.0,
                    "step": 1.0,
                    "display": "æ°´å¹³åç§»%"
                }),
                "sort_by": (["é»˜è®¤", "ä»å·¦åˆ°å³", "ä»å³åˆ°å·¦", "ä»ä¸Šåˆ°ä¸‹", "ä»ä¸‹åˆ°ä¸Š", 
                           "ç½®ä¿¡åº¦é™åº", "ç½®ä¿¡åº¦å‡åº", "é¢ç§¯é™åº", "é¢ç§¯å‡åº"], {
                    "default": "ä»å·¦åˆ°å³",
                    "display": "æ’åºæ–¹å¼"
                }),
                "crop_mode": (["å…¨éƒ¨å¯¹è±¡", "å•ä¸ªå¯¹è±¡", "æŒ‰ç±»åˆ«", "æ‰¹é‡å¤„ç†"], {
                    "default": "å…¨éƒ¨å¯¹è±¡",
                    "display": "è£å‰ªæ¨¡å¼"
                }),
                "object_index": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                    "display": "å¯¹è±¡ç´¢å¼•"
                }),
                "augment": ("BOOLEAN", {
                    "default": False,
                    "display": "æµ‹è¯•æ—¶å¢å¼º"
                }),
                "agnostic_nms": ("BOOLEAN", {
                    "default": False,
                    "display": "ç±»åˆ«æ— å…³NMS"
                })
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK", "BBOXES", "STRING", "INT", "FLOAT")
    RETURN_NAMES = ("è£å‰ªå›¾åƒ", "é®ç½©", "è¾¹ç•Œæ¡†", "æ£€æµ‹ä¿¡æ¯", "æ£€æµ‹æ•°é‡", "å¹³å‡ç½®ä¿¡åº¦")
    OUTPUT_IS_LIST = (True, False, False, False, False, False)
    FUNCTION = "detect_and_crop_v11"
    CATEGORY = "ğŸ³Pond/yolo"
    DESCRIPTION = "YOLOv11ä¸“ç”¨ç‰ˆæœ¬ï¼Œæ”¯æŒæœ€æ–°çš„YOLOv11æ¨¡å‹ç‰¹æ€§ï¼ŒåŒ…æ‹¬æ”¹è¿›çš„æ£€æµ‹æ€§èƒ½å’Œæ›´å¤šå‚æ•°æ§åˆ¶"

    def get_device(self, device_preference):
        """æ™ºèƒ½é€‰æ‹©è®¾å¤‡"""
        if device_preference == "auto":
            if torch.cuda.is_available():
                return "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                return "mps"
            else:
                return "cpu"
        return device_preference

    def load_model_v11(self, model_name, device="auto"):
        """åŠ è½½YOLOv11æ¨¡å‹"""
        if YOLO is None:
            raise ImportError("è¯·å®‰è£…ultralyticsåº“: pip install ultralytics>=8.2.0")
        
        # å¤„ç†è®¾å¤‡é€‰æ‹©
        actual_device = self.get_device(device)
        
        # å¦‚æœå·²ç»åŠ è½½äº†ç›¸åŒçš„æ¨¡å‹å’Œè®¾å¤‡ï¼Œç›´æ¥è¿”å›
        if (self.model is not None and 
            self.current_model_name == model_name and 
            self.device == actual_device):
            return self.model
        
        # æ¸…ç†"éœ€ä¸‹è½½"æ ‡è®°
        clean_model_name = model_name.replace(" (éœ€ä¸‹è½½)", "")
        
        # æ„å»ºæ¨¡å‹è·¯å¾„
        model_path = os.path.join(folder_paths.models_dir, "yolo", clean_model_name)
        
        # å¦‚æœæœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•è‡ªåŠ¨ä¸‹è½½å®˜æ–¹æ¨¡å‹
        if not os.path.exists(model_path) and clean_model_name in ["yolo11n.pt", "yolo11s.pt", 
                                                                   "yolo11m.pt", "yolo11l.pt", "yolo11x.pt"]:
            print(f"è‡ªåŠ¨ä¸‹è½½YOLOv11æ¨¡å‹: {clean_model_name}")
            try:
                # è®¾ç½®ç¯å¢ƒå˜é‡
                os.environ['YOLO_VERBOSE'] = 'False'
                
                # ç›´æ¥ä½¿ç”¨æ¨¡å‹åç§°ï¼Œè®©ultralyticsè‡ªåŠ¨ä¸‹è½½
                self.model = YOLO(clean_model_name)
                self.model.to(actual_device)
                self.current_model_name = model_name
                self.device = actual_device
                
                # ä¿å­˜åˆ°æœ¬åœ°ä»¥ä¾›åç»­ä½¿ç”¨
                save_path = model_path
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                self.model.save(save_path)
                print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
                
                return self.model
            except Exception as e:
                print(f"è‡ªåŠ¨ä¸‹è½½å¤±è´¥: {e}")
                raise FileNotFoundError(f"æ— æ³•ä¸‹è½½æˆ–åŠ è½½æ¨¡å‹: {clean_model_name}")
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
        
        try:
            print(f"åŠ è½½YOLOv11æ¨¡å‹: {clean_model_name} åœ¨è®¾å¤‡: {actual_device}")
            
            # è®¾ç½®ç¯å¢ƒå˜é‡
            os.environ['YOLO_VERBOSE'] = 'False'
            
            # åŠ è½½æ¨¡å‹
            self.model = YOLO(model_path)
            self.model.to(actual_device)
            
            # éªŒè¯æ˜¯å¦ä¸ºv11æ¨¡å‹
            if hasattr(self.model, 'model') and hasattr(self.model.model, 'yaml'):
                model_yaml = self.model.model.yaml
                if isinstance(model_yaml, dict) and 'model' in model_yaml:
                    model_info = model_yaml.get('model', '')
                    print(f"æ¨¡å‹ä¿¡æ¯: {model_info}")
            
            self.current_model_name = model_name
            self.device = actual_device
            
            return self.model
            
        except Exception as e:
            print(f"åŠ è½½YOLOv11æ¨¡å‹å¤±è´¥: {e}")
            raise RuntimeError(f"åŠ è½½YOLOv11æ¨¡å‹å¤±è´¥: {str(e)}")

    def tensor_to_pil(self, tensor):
        """å°†tensorè½¬æ¢ä¸ºPILå›¾åƒ"""
        if len(tensor.shape) == 4:
            tensor = tensor[0]
        
        # ä»CHWæˆ–HWCæ ¼å¼è½¬æ¢ä¸ºHWC
        if tensor.shape[0] == 3 or tensor.shape[0] == 1:
            tensor = tensor.permute(1, 2, 0)
        
        # è½¬æ¢ä¸ºnumpy
        img = tensor.cpu().numpy()
        
        # ç¡®ä¿å€¼åœ¨0-255èŒƒå›´å†…
        if img.max() <= 1.0:
            img = img * 255
        
        img = img.astype(np.uint8)
        
        # å¦‚æœæ˜¯å•é€šé“ï¼Œè½¬æ¢ä¸ºRGB
        if img.shape[2] == 1:
            img = np.repeat(img, 3, axis=2)
        
        return Image.fromarray(img)

    def pil_to_tensor(self, pil_img):
        """å°†PILå›¾åƒè½¬æ¢ä¸ºtensor"""
        img = np.array(pil_img).astype(np.float32) / 255.0
        tensor = torch.from_numpy(img).float()
        return tensor

    def get_class_names_v11(self):
        """è·å–YOLOv11çš„ç±»åˆ«åç§°æ˜ å°„"""
        # COCOæ•°æ®é›†çš„80ä¸ªç±»åˆ«ï¼ˆYOLOv11é»˜è®¤ï¼‰
        class_name_cn = {
            'person': 'äºº', 'bicycle': 'è‡ªè¡Œè½¦', 'car': 'æ±½è½¦', 'motorcycle': 'æ‘©æ‰˜è½¦',
            'airplane': 'é£æœº', 'bus': 'å…¬äº¤è½¦', 'train': 'ç«è½¦', 'truck': 'å¡è½¦',
            'boat': 'èˆ¹', 'traffic light': 'äº¤é€šç¯', 'fire hydrant': 'æ¶ˆé˜²æ “',
            'stop sign': 'åœæ­¢æ ‡å¿—', 'parking meter': 'åœè½¦è®¡æ—¶å™¨', 'bench': 'é•¿æ¤…',
            'bird': 'é¸Ÿ', 'cat': 'çŒ«', 'dog': 'ç‹—', 'horse': 'é©¬', 'sheep': 'ç¾Š',
            'cow': 'ç‰›', 'elephant': 'å¤§è±¡', 'bear': 'ç†Š', 'zebra': 'æ–‘é©¬',
            'giraffe': 'é•¿é¢ˆé¹¿', 'backpack': 'èƒŒåŒ…', 'umbrella': 'é›¨ä¼',
            'handbag': 'æ‰‹æåŒ…', 'tie': 'é¢†å¸¦', 'suitcase': 'æ‰‹æç®±',
            'frisbee': 'é£ç›˜', 'skis': 'æ»‘é›ªæ¿', 'snowboard': 'æ»‘é›ªæ¿',
            'sports ball': 'è¿åŠ¨çƒ', 'kite': 'é£ç­', 'baseball bat': 'æ£’çƒæ£’',
            'baseball glove': 'æ£’çƒæ‰‹å¥—', 'skateboard': 'æ»‘æ¿', 'surfboard': 'å†²æµªæ¿',
            'tennis racket': 'ç½‘çƒæ‹', 'bottle': 'ç“¶å­', 'wine glass': 'é…’æ¯',
            'cup': 'æ¯å­', 'fork': 'å‰å­', 'knife': 'åˆ€', 'spoon': 'å‹ºå­',
            'bowl': 'ç¢—', 'banana': 'é¦™è•‰', 'apple': 'è‹¹æœ', 'sandwich': 'ä¸‰æ˜æ²»',
            'orange': 'æ©™å­', 'broccoli': 'è¥¿å…°èŠ±', 'carrot': 'èƒ¡èåœ',
            'hot dog': 'çƒ­ç‹—', 'pizza': 'æŠ«è¨', 'donut': 'ç”œç”œåœˆ', 'cake': 'è›‹ç³•',
            'chair': 'æ¤…å­', 'couch': 'æ²™å‘', 'potted plant': 'ç›†æ ½',
            'bed': 'åºŠ', 'dining table': 'é¤æ¡Œ', 'toilet': 'é©¬æ¡¶', 'tv': 'ç”µè§†',
            'laptop': 'ç¬”è®°æœ¬ç”µè„‘', 'mouse': 'é¼ æ ‡', 'remote': 'é¥æ§å™¨',
            'keyboard': 'é”®ç›˜', 'cell phone': 'æ‰‹æœº', 'microwave': 'å¾®æ³¢ç‚‰',
            'oven': 'çƒ¤ç®±', 'toaster': 'çƒ¤é¢åŒ…æœº', 'sink': 'æ°´æ§½',
            'refrigerator': 'å†°ç®±', 'book': 'ä¹¦', 'clock': 'æ—¶é’Ÿ', 'vase': 'èŠ±ç“¶',
            'scissors': 'å‰ªåˆ€', 'teddy bear': 'æ³°è¿ªç†Š', 'hair drier': 'å¹é£æœº',
            'toothbrush': 'ç‰™åˆ·'
        }
        return class_name_cn

    def filter_detections_v11(self, results, class_filter):
        """æ ¹æ®ç±»åˆ«è¿‡æ»¤YOLOv11æ£€æµ‹ç»“æœ"""
        if not results or len(results) == 0:
            return []
        
        result = results[0]
        
        # æ£€æŸ¥YOLOv11çš„è¾“å‡ºæ ¼å¼
        if not hasattr(result, 'boxes') or result.boxes is None:
            return []
        
        filtered_boxes = []
        class_name_cn = self.get_class_names_v11()
        
        # åˆ›å»ºåå‘æ˜ å°„ï¼ˆä¸­æ–‡åˆ°è‹±æ–‡ï¼‰
        class_name_en = {v: k for k, v in class_name_cn.items()}
        
        # å¤„ç†æ£€æµ‹ç»“æœ
        boxes = result.boxes
        
        # æ£€æµ‹æ‰€æœ‰ç±»åˆ«
        if class_filter.lower() == 'all' or class_filter == 'å…¨éƒ¨':
            for i in range(len(boxes)):
                xyxy = boxes.xyxy[i].cpu().numpy()
                conf = boxes.conf[i].cpu().numpy()
                cls = int(boxes.cls[i].cpu().numpy())
                
                # è·å–ç±»åˆ«åç§°
                class_name = result.names[cls] if hasattr(result, 'names') else str(cls)
                class_name_display = class_name_cn.get(class_name, class_name)
                
                filtered_boxes.append({
                    'bbox': xyxy.tolist(),
                    'confidence': float(conf),
                    'class': class_name,
                    'class_display': class_name_display,
                    'class_id': cls
                })
        else:
            # è§£æè¦æ£€æµ‹çš„ç±»åˆ«
            target_classes = [c.strip().lower() for c in class_filter.split(',')]
            
            # è½¬æ¢ä¸­æ–‡ç±»åˆ«ååˆ°è‹±æ–‡
            target_classes_en = []
            for tc in target_classes:
                if tc in class_name_en:
                    target_classes_en.append(class_name_en[tc])
                else:
                    target_classes_en.append(tc)
            
            for i in range(len(boxes)):
                cls = int(boxes.cls[i].cpu().numpy())
                class_name = result.names[cls] if hasattr(result, 'names') else str(cls)
                
                if class_name.lower() in target_classes_en:
                    xyxy = boxes.xyxy[i].cpu().numpy()
                    conf = boxes.conf[i].cpu().numpy()
                    class_name_display = class_name_cn.get(class_name, class_name)
                    
                    filtered_boxes.append({
                        'bbox': xyxy.tolist(),
                        'confidence': float(conf),
                        'class': class_name,
                        'class_display': class_name_display,
                        'class_id': cls
                    })
        
        return filtered_boxes

    def sort_detections(self, detections, sort_by):
        """æ ¹æ®æŒ‡å®šæ–¹å¼å¯¹æ£€æµ‹ç»“æœæ’åº"""
        if not detections or sort_by == "é»˜è®¤":
            return detections
        
        if sort_by == "ä»å·¦åˆ°å³":
            return sorted(detections, key=lambda d: (d['bbox'][0] + d['bbox'][2]) / 2)
        elif sort_by == "ä»å³åˆ°å·¦":
            return sorted(detections, key=lambda d: (d['bbox'][0] + d['bbox'][2]) / 2, reverse=True)
        elif sort_by == "ä»ä¸Šåˆ°ä¸‹":
            return sorted(detections, key=lambda d: (d['bbox'][1] + d['bbox'][3]) / 2)
        elif sort_by == "ä»ä¸‹åˆ°ä¸Š":
            return sorted(detections, key=lambda d: (d['bbox'][1] + d['bbox'][3]) / 2, reverse=True)
        elif sort_by == "ç½®ä¿¡åº¦é™åº":
            return sorted(detections, key=lambda d: d['confidence'], reverse=True)
        elif sort_by == "ç½®ä¿¡åº¦å‡åº":
            return sorted(detections, key=lambda d: d['confidence'])
        elif sort_by == "é¢ç§¯é™åº":
            def get_area(d):
                x1, y1, x2, y2 = d['bbox']
                return (x2 - x1) * (y2 - y1)
            return sorted(detections, key=get_area, reverse=True)
        elif sort_by == "é¢ç§¯å‡åº":
            def get_area(d):
                x1, y1, x2, y2 = d['bbox']
                return (x2 - x1) * (y2 - y1)
            return sorted(detections, key=get_area)
        
        return detections

    def crop_single_object(self, image, detection, square_size, object_margin, 
                          vertical_offset, horizontal_offset, original_height, 
                          original_width, scale_x=1.0, scale_y=1.0):
        """æ™ºèƒ½è£å‰ªå•ä¸ªæ£€æµ‹å¯¹è±¡"""
        height, width = image.shape[:2]
        
        # è·å–æ£€æµ‹æ¡†åæ ‡
        xmin, ymin, xmax, ymax = detection['bbox']
        
        # è®¡ç®—æ£€æµ‹åˆ°çš„å¯¹è±¡æ¡†çš„å®½åº¦å’Œé«˜åº¦
        object_width = xmax - xmin
        object_height = ymax - ymin
        object_size = max(object_width, object_height)
        
        # æ ¹æ®å¯¹è±¡å°ºå¯¸è®¡ç®—åˆé€‚çš„æ–¹å—å¤§å°
        actual_square_size = object_size * object_margin
        actual_square_size = actual_square_size * (square_size / 100.0)
        
        # è®¡ç®—è¾¹ç•Œæ¡†çš„ä¸­å¿ƒ
        center_x = (xmin + xmax) / 2
        center_y = (ymin + ymax) / 2
        
        # ä½¿ç”¨å®é™…çš„æ–¹å—å¤§å°
        half_size = actual_square_size / 2
        
        # è®¡ç®—åç§»åçš„åæ ‡
        vertical_offset_px = actual_square_size * vertical_offset / 100
        horizontal_offset_px = actual_square_size * horizontal_offset / 100
        
        # è®¡ç®—æ–°çš„è¾¹ç•Œæ¡†åæ ‡ï¼Œä¿æŒæ­£æ–¹å½¢å¤§å°
        x1_new = max(0, int(center_x - half_size + horizontal_offset_px))
        x2_new = min(width, int(center_x + half_size + horizontal_offset_px))
        y1_new = max(0, int(center_y - half_size + vertical_offset_px))
        y2_new = min(height, int(center_y + half_size + vertical_offset_px))
        
        # ç¡®ä¿è£å‰ªåŒºåŸŸæ˜¯æ­£æ–¹å½¢
        crop_width = x2_new - x1_new
        crop_height = y2_new - y1_new
        
        if crop_width != crop_height:
            target_size = min(crop_width, crop_height)
            center_x_new = (x1_new + x2_new) / 2
            center_y_new = (y1_new + y2_new) / 2
            half_target = target_size / 2
            
            x1_new = max(0, int(center_x_new - half_target))
            x2_new = min(width, int(center_x_new + half_target))
            y1_new = max(0, int(center_y_new - half_target))
            y2_new = min(height, int(center_y_new + half_target))
        
        # è£å‰ªå›¾åƒ
        cropped = image[y1_new:y2_new, x1_new:x2_new]
        
        # åˆ›å»ºåŸå›¾å°ºå¯¸çš„é®ç½©
        mask = np.zeros((original_height, original_width), dtype=np.float32)
        
        # è®¡ç®—åœ¨åŸå§‹å°ºå¯¸ä¸Šçš„åæ ‡
        mask_x1 = int(x1_new * scale_x)
        mask_y1 = int(y1_new * scale_y)
        mask_x2 = int(x2_new * scale_x)
        mask_y2 = int(y2_new * scale_y)
        
        # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
        mask_x1 = max(0, min(mask_x1, original_width))
        mask_y1 = max(0, min(mask_y1, original_height))
        mask_x2 = max(mask_x1, min(mask_x2, original_width))
        mask_y2 = max(mask_y1, min(mask_y2, original_height))
        
        if mask_x2 > mask_x1 and mask_y2 > mask_y1:
            mask[mask_y1:mask_y2, mask_x1:mask_x2] = 1.0
        
        return cropped, mask, (x1_new, y1_new, x2_new, y2_new)

    def detect_and_crop_v11(self, image, model_name, device, confidence, iou_threshold,
                           imgsz, max_det, class_filter, square_size, object_margin, 
                           vertical_offset, horizontal_offset, sort_by, crop_mode, 
                           object_index, augment, agnostic_nms):
        """æ‰§è¡ŒYOLOv11æ£€æµ‹å’Œè£å‰ª"""
        
        # ç¡®ä¿å›¾åƒæ˜¯4ç»´çš„
        if len(image.shape) == 3:
            image = image.unsqueeze(0)
        
        # ä¿å­˜åŸå§‹å›¾åƒå°ºå¯¸
        batch_size, tensor_height, tensor_width, channels = image.shape
        
        # åŠ è½½æ¨¡å‹
        try:
            model = self.load_model_v11(model_name, device)
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            empty_mask = torch.zeros((1, tensor_height, tensor_width), dtype=torch.float32)
            empty_bboxes = []
            return ([image], empty_mask, empty_bboxes, f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", 0, 0.0)
        
        # è½¬æ¢ä¸ºPILå›¾åƒè¿›è¡Œæ£€æµ‹
        pil_img = self.tensor_to_pil(image)
        
        # æ‰§è¡ŒYOLOv11æ£€æµ‹
        try:
            results = model.predict(
                source=pil_img,
                conf=confidence,
                iou=iou_threshold,
                imgsz=imgsz,
                max_det=max_det,
                augment=augment,
                agnostic_nms=agnostic_nms,
                verbose=False
            )
        except Exception as e:
            print(f"æ£€æµ‹å¤±è´¥: {e}")
            empty_mask = torch.zeros((1, tensor_height, tensor_width), dtype=torch.float32)
            empty_bboxes = []
            return ([image], empty_mask, empty_bboxes, f"æ£€æµ‹å¤±è´¥: {str(e)}", 0, 0.0)
        
        # è¿‡æ»¤æ£€æµ‹ç»“æœ
        detections = self.filter_detections_v11(results, class_filter)
        
        # å¯¹æ£€æµ‹ç»“æœè¿›è¡Œæ’åº
        detections = self.sort_detections(detections, sort_by)
        
        if not detections:
            empty_mask = torch.zeros((1, tensor_height, tensor_width), dtype=torch.float32)
            empty_bboxes = []
            return ([image], empty_mask, empty_bboxes, "æœªæ£€æµ‹åˆ°å¯¹è±¡", 0, 0.0)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„è¿›è¡Œå¤„ç†
        img_np = np.array(pil_img)
        
        # ä½¿ç”¨tensorçš„åŸå§‹å°ºå¯¸ä½œä¸ºé®ç½©å°ºå¯¸
        original_height, original_width = tensor_height, tensor_width
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        pil_height, pil_width = img_np.shape[:2]
        scale_x = original_width / pil_width
        scale_y = original_height / pil_height
        
        # æ ¹æ®è£å‰ªæ¨¡å¼é€‰æ‹©è¦å¤„ç†çš„å¯¹è±¡
        if crop_mode == "å•ä¸ªå¯¹è±¡":
            if object_index >= len(detections):
                object_index = len(detections) - 1
            selected_detections = [detections[object_index]]
        elif crop_mode == "æŒ‰ç±»åˆ«":
            # æŒ‰ç±»åˆ«åˆ†ç»„ï¼Œæ¯ä¸ªç±»åˆ«é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„
            class_dict = {}
            for det in detections:
                cls = det['class']
                if cls not in class_dict or det['confidence'] > class_dict[cls]['confidence']:
                    class_dict[cls] = det
            selected_detections = list(class_dict.values())
        elif crop_mode == "æ‰¹é‡å¤„ç†":
            # æ‰¹é‡å¤„ç†æ¨¡å¼ï¼šé€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„å‰Nä¸ª
            selected_detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)[:10]
        else:  # å…¨éƒ¨å¯¹è±¡
            selected_detections = detections
        
        # è£å‰ªé€‰ä¸­çš„å¯¹è±¡
        cropped_images = []
        bboxes = []
        detection_info = []
        total_confidence = 0.0
        
        # åˆ›å»ºä¸€ä¸ªä¸åŸå›¾å°ºå¯¸ç›¸åŒçš„åˆå¹¶é®ç½©
        combined_mask = torch.zeros((original_height, original_width), dtype=torch.float32)
        
        for i, det in enumerate(selected_detections):
            cropped, mask, bbox = self.crop_single_object(
                img_np, det, square_size, object_margin, 
                vertical_offset, horizontal_offset,
                original_height, original_width, scale_x, scale_y
            )
            
            # è½¬æ¢ä¸ºtensor
            cropped_tensor = self.pil_to_tensor(Image.fromarray(cropped))
            mask_tensor = torch.from_numpy(mask).float()
            
            # ä¸ºè£å‰ªçš„å›¾åƒæ·»åŠ æ‰¹æ¬¡ç»´åº¦
            cropped_images.append(cropped_tensor.unsqueeze(0))
            
            # åˆå¹¶é®ç½©
            combined_mask = torch.maximum(combined_mask, mask_tensor)
            
            bboxes.append(bbox)
            total_confidence += det['confidence']
            
            # è®°å½•æ£€æµ‹ä¿¡æ¯
            center_x = int((det['bbox'][0] + det['bbox'][2]) / 2)
            center_y = int((det['bbox'][1] + det['bbox'][3]) / 2)
            object_size = max(det['bbox'][2] - det['bbox'][0], det['bbox'][3] - det['bbox'][1])
            info = f"[{i}] {det['class_display']} (ç½®ä¿¡åº¦: {det['confidence']:.2f}, ä¸­å¿ƒ: x={center_x},y={center_y}, å°ºå¯¸: {object_size:.0f})"
            detection_info.append(info)
        
        # ä¸ºåˆå¹¶é®ç½©æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        final_mask = combined_mask.unsqueeze(0)
        final_mask = final_mask.clamp(0.0, 1.0)
        
        # ç¡®ä¿è¾¹ç•Œæ¡†æ ¼å¼æ­£ç¡®
        final_bboxes = []
        for bbox in bboxes:
            if isinstance(bbox, (list, tuple)) and len(bbox) == 4:
                final_bboxes.append(list(bbox))
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = total_confidence / len(selected_detections) if selected_detections else 0.0
        
        # ç”Ÿæˆæ£€æµ‹ä¿¡æ¯å­—ç¬¦ä¸²
        info_str = f"YOLOv11 æ£€æµ‹ç»“æœ\n"
        info_str += f"æ¨¡å‹: {model_name} | è®¾å¤‡: {self.device}\n"
        info_str += f"æ£€æµ‹åˆ° {len(detections)} ä¸ªå¯¹è±¡ï¼Œè£å‰ªäº† {len(selected_detections)} ä¸ª\n"
        info_str += f"å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}\n"
        info_str += f"æ¨ç†å°ºå¯¸: {imgsz} | IOUé˜ˆå€¼: {iou_threshold}\n"
        info_str += f"è£å‰ªè®¾ç½®: å¤§å°={square_size}%, è¾¹è·={object_margin}x, å‚ç›´åç§»={vertical_offset}%, æ°´å¹³åç§»={horizontal_offset}%\n"
        info_str += "\n".join(detection_info)
        
        # è¿”å›è£å‰ªçš„å›¾åƒåˆ—è¡¨å’Œåˆå¹¶çš„é®ç½©
        return (cropped_images, final_mask, final_bboxes, info_str, len(selected_detections), avg_confidence)

# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "YoloV11BboxesCropNode": YoloV11BboxesCropNode
}

# èŠ‚ç‚¹æ˜¾ç¤ºåç§°æ˜ å°„
NODE_DISPLAY_NAME_MAPPINGS = {
    "YoloV11BboxesCropNode": "ğŸ³YOLOv11æ™ºèƒ½è£å‰ª"
}