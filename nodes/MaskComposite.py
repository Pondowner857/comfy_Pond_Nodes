import torch
import torch.nn.functional as F
import numpy as np
from PIL import Image
import cv2

class AdvancedMaskImageComposite:
    
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "background_image": ("IMAGE", {"tooltip": "èƒŒæ™¯å›¾åƒ - ä½œä¸ºåº•å±‚çš„å›¾åƒ"}),  
                "subject_image": ("IMAGE", {"tooltip": "ä¸»ä½“å›¾åƒ - è¦æ‹¼æ¥çš„å›¾åƒ"}),     
                "subject_mask": ("MASK", {"tooltip": "ä¸»ä½“é®ç½© - ç”¨äºæŠ å–ä¸»ä½“çš„é®ç½©"}),
                "position_mask": ("MASK", {"tooltip": "ä½ç½®é®ç½© - ç™½è‰²åŒºåŸŸè¡¨ç¤ºæ‹¼æ¥ä½ç½®"}),       
                "alignment": (["ä¸­", "ä¸Š", "ä¸‹", "å·¦", "å³"], {
                    "default": "ä¸­",
                    "tooltip": "å¯¹é½æ–¹å¼ - åœ¨ç›®æ ‡åŒºåŸŸå†…çš„å¯¹é½ä½ç½®"
                }),
                "scale_factor": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.05,
                    "tooltip": "ç¼©æ”¾å› å­ - æ§åˆ¶ä¸»ä½“ç›¸å¯¹äºç›®æ ‡åŒºåŸŸçš„å¤§å°"
                }),
            }
        }
    
    # æ·»åŠ è¾“å…¥å‚æ•°çš„ä¸­æ–‡æ˜¾ç¤ºåç§°
    @classmethod
    def INPUT_NAMES(cls):
        return {
            "background_image": "èƒŒæ™¯å›¾åƒ",
            "subject_image": "ä¸»ä½“å›¾åƒ", 
            "subject_mask": "ä¸»ä½“é®ç½©",
            "position_mask": "ä½ç½®é®ç½©",
            "alignment": "å¯¹é½æ–¹å¼",
            "scale_factor": "ç¼©æ”¾å› å­",
        }
    
    RETURN_TYPES = ("IMAGE", "MASK")
    RETURN_NAMES = ("æ‹¼æ¥å›¾åƒ", "æœ€ç»ˆé®ç½©")
    FUNCTION = "advanced_composite"
    CATEGORY = "ğŸ³Pond/mask"
    OUTPUT_NODE = False

    
    def advanced_composite(self, background_image, subject_image, subject_mask, position_mask, alignment, scale_factor):
        """
        æ‰§è¡Œé®ç½©å›¾åƒæ‹¼æ¥çš„ä¸»è¦å‡½æ•°
        """
        # è½¬æ¢tensoråˆ°numpyæ•°ç»„
        bg_img = self.tensor_to_numpy(background_image)
        subj_img = self.tensor_to_numpy(subject_image)
        subj_mask_np = self.mask_tensor_to_numpy(subject_mask)
        pos_mask_np = self.mask_tensor_to_numpy(position_mask)
        
        # éªŒè¯å°ºå¯¸åŒ¹é…
        if not self.validate_dimensions(bg_img, pos_mask_np, subj_img, subj_mask_np):
            # å¦‚æœå°ºå¯¸ä¸åŒ¹é…ï¼Œè¿›è¡Œæ™ºèƒ½è°ƒæ•´
            bg_img, pos_mask_np, subj_img, subj_mask_np = self.smart_resize(
                bg_img, pos_mask_np, subj_img, subj_mask_np
            )
        
        # æ­¥éª¤1ï¼šç”¨ä¸»ä½“é®ç½©æŠ å–ä¸»ä½“
        extracted_subject = self.extract_subject(subj_img, subj_mask_np)
        
        # æ­¥éª¤2ï¼šåˆ†ææ‹¼æ¥ä½ç½®é®ç½©ï¼Œè·å–ç›®æ ‡åŒºåŸŸ
        target_bbox = self.get_position_bbox(pos_mask_np)
        if target_bbox is None:
            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ç™½è‰²åŒºåŸŸï¼Œè¿”å›åŸèƒŒæ™¯
            return (self.numpy_to_tensor(bg_img), self.numpy_to_mask_tensor(pos_mask_np))
        
        # æ­¥éª¤3ï¼šå°†æŠ å–çš„ä¸»ä½“ç¼©æ”¾åˆ°ç›®æ ‡å°ºå¯¸ï¼ˆé€‚åº”æ¨¡å¼ï¼‰
        scaled_subject, scaled_mask = self.scale_subject_to_target(
            extracted_subject, subj_mask_np, target_bbox, alignment, scale_factor, bg_img.shape
        )
        
        # æ­¥éª¤4ï¼šæ‰§è¡Œæœ€ç»ˆæ‹¼æ¥
        result_img = self.blend_images(bg_img, scaled_subject, scaled_mask)
        
        # è½¬æ¢å›tensoræ ¼å¼
        result_tensor = self.numpy_to_tensor(result_img)
        final_mask_tensor = self.numpy_to_mask_tensor(scaled_mask)
        
        return (result_tensor, final_mask_tensor)
    
    def validate_dimensions(self, bg_img, pos_mask, subj_img, subj_mask):
        """éªŒè¯è¾“å…¥å°ºå¯¸æ˜¯å¦ç¬¦åˆè¦æ±‚"""
        bg_h, bg_w = bg_img.shape[:2]
        pos_h, pos_w = pos_mask.shape[:2]
        subj_h, subj_w = subj_img.shape[:2]
        mask_h, mask_w = subj_mask.shape[:2]
        
        bg_match = (bg_h == pos_h and bg_w == pos_w)
        subj_match = (subj_h == mask_h and subj_w == mask_w)
        
        return bg_match and subj_match
    
    def smart_resize(self, bg_img, pos_mask, subj_img, subj_mask):
        """æ™ºèƒ½è°ƒæ•´å°ºå¯¸ä»¥ç¬¦åˆè¦æ±‚"""
        # ä»¥èƒŒæ™¯å›¾åƒå°ºå¯¸ä¸ºåŸºå‡†è°ƒæ•´ä½ç½®é®ç½©
        bg_h, bg_w = bg_img.shape[:2]
        pos_mask_resized = cv2.resize(pos_mask, (bg_w, bg_h))
        
        # ä»¥ä¸»ä½“å›¾åƒå°ºå¯¸ä¸ºåŸºå‡†è°ƒæ•´ä¸»ä½“é®ç½©
        subj_h, subj_w = subj_img.shape[:2]
        subj_mask_resized = cv2.resize(subj_mask, (subj_w, subj_h))
        
        return bg_img, pos_mask_resized, subj_img, subj_mask_resized
    
    def extract_subject(self, subject_img, subject_mask):
        """ä½¿ç”¨é®ç½©ä»ä¸»ä½“å›¾åƒä¸­æŠ å–ä¸»ä½“"""
        # ç¡®ä¿é®ç½©æœ‰æ­£ç¡®çš„ç»´åº¦
        if len(subject_mask.shape) == 2:
            mask_3d = np.expand_dims(subject_mask, axis=2)
            mask_3d = np.repeat(mask_3d, 3, axis=2)
        else:
            mask_3d = subject_mask
        
        # æŠ å–ä¸»ä½“ï¼Œä¿æŒé€æ˜èƒŒæ™¯
        extracted = subject_img * mask_3d
        
        return extracted
    
    def get_position_bbox(self, position_mask):
        """ä»ä½ç½®é®ç½©ä¸­è·å–ç™½è‰²åŒºåŸŸçš„è¾¹ç•Œæ¡†"""
        # äºŒå€¼åŒ–é®ç½©
        binary_mask = (position_mask > 0.5).astype(np.uint8)
        
        # æŸ¥æ‰¾è½®å»“
        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            return None
        
        # æ‰¾åˆ°æœ€å¤§çš„è½®å»“
        largest_contour = max(contours, key=cv2.contourArea)
        
        # è·å–è¾¹ç•Œæ¡†
        x, y, w, h = cv2.boundingRect(largest_contour)
        
        return {
            'x': x, 'y': y, 'width': w, 'height': h,
            'x2': x + w, 'y2': y + h
        }
    
    def get_subject_bbox(self, mask):
        """è·å–ä¸»ä½“çš„è¾¹ç•Œæ¡†"""
        binary_mask = (mask > 0.1).astype(np.uint8)
        coords = np.column_stack(np.where(binary_mask > 0))
        
        if len(coords) == 0:
            return None
        
        y_min, x_min = coords.min(axis=0)
        y_max, x_max = coords.max(axis=0)
        
        return {
            'x': x_min, 'y': y_min,
            'width': x_max - x_min + 1,
            'height': y_max - y_min + 1,
            'x2': x_max + 1, 'y2': y_max + 1
        }
    
    def scale_subject_to_target(self, extracted_subject, subject_mask, target_bbox, alignment, scale_factor, bg_shape):
        """å°†æŠ å–çš„ä¸»ä½“ç¼©æ”¾åˆ°ç›®æ ‡åŒºåŸŸï¼ˆé€‚åº”æ¨¡å¼ï¼‰"""
        target_w, target_h = target_bbox['width'], target_bbox['height']
        target_x, target_y = target_bbox['x'], target_bbox['y']
        bg_h, bg_w = bg_shape[:2]
        
        # è·å–ä¸»ä½“çš„æœ‰æ•ˆåŒºåŸŸ
        subject_bbox = self.get_subject_bbox(subject_mask)
        if subject_bbox is None:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆä¸»ä½“åŒºåŸŸï¼Œè¿”å›å’ŒèƒŒæ™¯ç›¸åŒå°ºå¯¸çš„ç©ºå›¾åƒ
            empty_img = np.zeros((bg_h, bg_w, 3), dtype=np.float32)
            empty_mask = np.zeros((bg_h, bg_w), dtype=np.float32)
            return empty_img, empty_mask
        
        # è£å‰ªä¸»ä½“åˆ°æœ‰æ•ˆåŒºåŸŸ
        cropped_subject = extracted_subject[
            subject_bbox['y']:subject_bbox['y2'],
            subject_bbox['x']:subject_bbox['x2']
        ]
        cropped_mask = subject_mask[
            subject_bbox['y']:subject_bbox['y2'],
            subject_bbox['x']:subject_bbox['x2']
        ]
        
        # è®¡ç®—åˆå§‹ç¼©æ”¾æ¯”ä¾‹ï¼ˆé€‚åº”ç›®æ ‡åŒºåŸŸï¼‰å¹¶åº”ç”¨ç¼©æ”¾å› å­
        subj_h, subj_w = cropped_subject.shape[:2]
        scale_w = target_w / subj_w * scale_factor
        scale_h = target_h / subj_h * scale_factor
        scale = min(scale_w, scale_h)
        
        # è®¡ç®—ç¼©æ”¾åçš„å°ºå¯¸
        new_w = int(subj_w * scale)
        new_h = int(subj_h * scale)
        
        # åˆ›å»ºå’ŒèƒŒæ™¯å›¾åƒç›¸åŒå°ºå¯¸çš„æœ€ç»ˆå›¾åƒå’Œé®ç½©
        final_subject = np.zeros((bg_h, bg_w, 3), dtype=np.float32)
        final_mask = np.zeros((bg_h, bg_w), dtype=np.float32)
        
        # æ ¹æ®å¯¹é½æ–¹å¼è®¡ç®—æ”¾ç½®ä½ç½®
        if alignment == "ä¸­":
            placement_x = target_x + (target_w - new_w) // 2
            placement_y = target_y + (target_h - new_h) // 2
        elif alignment == "ä¸Š":
            placement_x = target_x + (target_w - new_w) // 2
            placement_y = target_y
        elif alignment == "ä¸‹":
            placement_x = target_x + (target_w - new_w) // 2
            placement_y = target_y + target_h - new_h
        elif alignment == "å·¦":
            placement_x = target_x
            placement_y = target_y + (target_h - new_h) // 2
        elif alignment == "å³":
            placement_x = target_x + target_w - new_w
            placement_y = target_y + (target_h - new_h) // 2
        else:  # é»˜è®¤å±…ä¸­
            placement_x = target_x + (target_w - new_w) // 2
            placement_y = target_y + (target_h - new_h) // 2
        
        # æ£€æŸ¥æ˜¯å¦ä¼šè¶…å‡ºè¾¹ç•Œï¼Œå¦‚æœä¼šï¼Œåˆ™éœ€è¦è¿›ä¸€æ­¥ç¼©å°
        if placement_x < 0 or placement_y < 0 or placement_x + new_w > bg_w or placement_y + new_h > bg_h:
            # è°ƒæ•´ä½ç½®åˆ°åˆæ³•èŒƒå›´
            if placement_x < 0:
                placement_x = 0
            if placement_y < 0:
                placement_y = 0
            
            # è®¡ç®—å¯ç”¨ç©ºé—´
            max_w = bg_w - placement_x
            max_h = bg_h - placement_y
            
            # å¦‚æœå½“å‰å°ºå¯¸è¶…å‡ºå¯ç”¨ç©ºé—´ï¼Œé‡æ–°è®¡ç®—ç¼©æ”¾
            if new_w > max_w or new_h > max_h:
                extra_scale_w = max_w / new_w
                extra_scale_h = max_h / new_h
                extra_scale = min(extra_scale_w, extra_scale_h)
                
                # åº”ç”¨é¢å¤–ç¼©æ”¾
                new_w = int(new_w * extra_scale)
                new_h = int(new_h * extra_scale)
        
        # ç¡®ä¿ä½ç½®åœ¨æœ‰æ•ˆèŒƒå›´å†…
        placement_x = max(0, min(placement_x, bg_w - new_w))
        placement_y = max(0, min(placement_y, bg_h - new_h))
        
        # ç¼©æ”¾ä¸»ä½“å’Œé®ç½©åˆ°æœ€ç»ˆå°ºå¯¸
        scaled_subject = cv2.resize(cropped_subject, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
        scaled_mask = cv2.resize(cropped_mask, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
        
        # æ”¾ç½®ä¸»ä½“ï¼ˆç°åœ¨ä¿è¯ä¸ä¼šè¶…å‡ºè¾¹ç•Œï¼‰
        end_x = placement_x + new_w
        end_y = placement_y + new_h
        
        final_subject[placement_y:end_y, placement_x:end_x] = scaled_subject
        final_mask[placement_y:end_y, placement_x:end_x] = scaled_mask
        
        return final_subject, final_mask
    
    def tensor_to_numpy(self, tensor):
        """å°†ComfyUIçš„å›¾åƒtensorè½¬æ¢ä¸ºnumpyæ•°ç»„"""
        if len(tensor.shape) == 4:  # batch dimension
            tensor = tensor[0]
        
        # ä»CHWæˆ–HWCæ ¼å¼è½¬æ¢ä¸ºHWC
        if tensor.shape[0] == 3 or tensor.shape[0] == 1:  # CHWæ ¼å¼
            tensor = tensor.permute(1, 2, 0)
        
        # è½¬æ¢ä¸ºnumpyå¹¶ç¡®ä¿æ•°æ®ç±»å‹
        img = tensor.cpu().numpy()
        if img.dtype != np.float32:
            img = img.astype(np.float32)
        
        # ç¡®ä¿å€¼åœ¨0-1èŒƒå›´å†…
        img = np.clip(img, 0, 1)
        
        return img
    
    def mask_tensor_to_numpy(self, mask_tensor):
        """å°†é®ç½©tensorè½¬æ¢ä¸ºnumpyæ•°ç»„"""
        if len(mask_tensor.shape) == 3:  # ç§»é™¤batch dimension
            mask_tensor = mask_tensor[0]
        
        mask = mask_tensor.cpu().numpy()
        if mask.dtype != np.float32:
            mask = mask.astype(np.float32)
        
        # ç¡®ä¿å€¼åœ¨0-1èŒƒå›´å†…
        mask = np.clip(mask, 0, 1)
        
        return mask
    
    def numpy_to_tensor(self, img):
        """å°†numpyæ•°ç»„è½¬æ¢ä¸ºComfyUIçš„å›¾åƒtensoræ ¼å¼"""
        # ç¡®ä¿æ˜¯HWCæ ¼å¼
        if len(img.shape) == 2:  # ç°åº¦å›¾
            img = np.expand_dims(img, axis=2)
        
        # è½¬æ¢ä¸ºtensor
        tensor = torch.from_numpy(img).float()
        
        # æ·»åŠ batch dimension
        tensor = tensor.unsqueeze(0)
        
        return tensor
    
    def numpy_to_mask_tensor(self, mask):
        """å°†numpyé®ç½©æ•°ç»„è½¬æ¢ä¸ºComfyUIçš„é®ç½©tensoræ ¼å¼"""
        # ç¡®ä¿æ˜¯2Dæ ¼å¼ï¼ˆç§»é™¤å¤šä½™çš„é€šé“ç»´åº¦ï¼‰
        if len(mask.shape) == 3:
            mask = mask[:, :, 0]
        
        # è½¬æ¢ä¸ºtensor
        tensor = torch.from_numpy(mask).float()
        
        # æ·»åŠ batch dimension
        tensor = tensor.unsqueeze(0)
        
        return tensor
    
    def blend_images(self, background, subject, mask):
        """æ ¹æ®é®ç½©æ··åˆå›¾åƒï¼ˆæ­£å¸¸æ¨¡å¼ï¼‰"""
        # ç¡®ä¿æ‰€æœ‰å›¾åƒå°ºå¯¸ä¸€è‡´
        bg_h, bg_w = background.shape[:2]
        
        # å¦‚æœsubjectæˆ–maskå°ºå¯¸ä¸åŒ¹é…ï¼Œè°ƒæ•´ä¸ºèƒŒæ™¯å›¾åƒå°ºå¯¸
        if subject.shape[:2] != (bg_h, bg_w):
            subject = cv2.resize(subject, (bg_w, bg_h))
        
        if mask.shape[:2] != (bg_h, bg_w):
            mask = cv2.resize(mask, (bg_w, bg_h))
        
        # ç¡®ä¿é®ç½©æœ‰æ­£ç¡®çš„ç»´åº¦
        if len(mask.shape) == 2:
            mask = np.expand_dims(mask, axis=2)
        if mask.shape[2] == 1:
            mask = np.repeat(mask, 3, axis=2)
        
        # ä½¿ç”¨é®ç½©æ··åˆèƒŒæ™¯å’Œä¸»ä½“å›¾åƒ
        result = background * (1 - mask) + subject * mask
        
        # ç¡®ä¿å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
        result = np.clip(result, 0, 1)
        
        return result


# ComfyUIèŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "AdvancedMaskImageComposite": AdvancedMaskImageComposite,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "AdvancedMaskImageComposite": "ğŸ³é®ç½©å›¾åƒæ‹¼æ¥",
}