import torch
import torch.nn.functional as F
import numpy as np
from PIL import Image, ImageFilter
import cv2

class AdvancedMaskImageComposite:
    
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "background_image": ("IMAGE", {"tooltip": "èƒŒæ™¯å›¾åƒ - ä½œä¸ºåº•å±‚çš„å›¾åƒ"}),  
                "subject_image": ("IMAGE", {"tooltip": "ä¸»ä½“å›¾åƒ - è¦æ‹¼æ¥çš„å›¾åƒ"}),     
                "subject_mask": ("MASK", {"tooltip": "ä¸»ä½“é®ç½© - ç”¨äºæŠ å–ä¸»ä½“çš„é®ç½©"}),
                "position_mask": ("MASK", {"tooltip": "ä½ç½®é®ç½© - ç™½è‰²åŒºåŸŸè¡¨ç¤ºæ‹¼æ¥ä½ç½®"}),       
                "scale_mode": (["æ‹‰ä¼¸", "é€‚åº”", "å¡«å……"], {
                    "default": "é€‚åº”",
                    "tooltip": "ç¼©æ”¾æ¨¡å¼ï¼šæ‹‰ä¼¸=ç›´æ¥æ‹‰ä¼¸åˆ°ç›®æ ‡å°ºå¯¸ï¼Œé€‚åº”=ä¿æŒæ¯”ä¾‹é€‚åº”ç›®æ ‡ï¼Œå¡«å……=ä¿æŒæ¯”ä¾‹å¡«å……ç›®æ ‡"
                }),
                "alignment": (["å±…ä¸­", "å·¦ä¸Š", "å³ä¸Š", "å·¦ä¸‹", "å³ä¸‹"], {
                    "default": "å±…ä¸­",
                    "tooltip": "å¯¹é½æ–¹å¼ - åœ¨ç›®æ ‡åŒºåŸŸå†…çš„å¯¹é½ä½ç½®"
                }),
                "edge_blur": ("FLOAT", {
                    "default": 0.0,
                    "min": 0.0,
                    "max": 50.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "è¾¹ç¼˜æ¨¡ç³Š - æ•°å€¼è¶Šå¤§è¾¹ç¼˜è¶ŠæŸ”å’Œ"
                }),  
                "blend_mode": (["æ­£å¸¸", "å åŠ ", "æ»¤è‰²", "è¦†ç›–"], {
                    "default": "æ­£å¸¸",
                    "tooltip": "æ··åˆæ¨¡å¼ï¼šæ­£å¸¸=ç›´æ¥è¦†ç›–ï¼Œå åŠ =ç›¸ä¹˜æ•ˆæœï¼Œæ»¤è‰²=å¢äº®æ•ˆæœï¼Œè¦†ç›–=å¯¹æ¯”å¢å¼º"
                }),
                "feather_edge": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "è¾¹ç¼˜ç¾½åŒ– - æ˜¯å¦å¯¹è¾¹ç¼˜è¿›è¡Œç¾½åŒ–å¤„ç†ï¼Œè®©è¿‡æ¸¡æ›´è‡ªç„¶"
                }),
            }
        }
    
    # æ·»åŠ è¾“å…¥å‚æ•°çš„ä¸­æ–‡æ˜¾ç¤ºåç§°
    @classmethod
    def INPUT_NAMES(cls):
        return {
            "background_image": "èƒŒæ™¯å›¾åƒ",
            "subject_image": "ä¸»ä½“å›¾åƒ", 
            "subject_mask": "ä¸»ä½“é®ç½©",
            "position_mask": "ä½ç½®é®ç½©",
            "scale_mode": "ç¼©æ”¾æ¨¡å¼",
            "alignment": "å¯¹é½æ–¹å¼",
            "edge_blur": "è¾¹ç¼˜æ¨¡ç³Š",
            "blend_mode": "æ··åˆæ¨¡å¼",
            "feather_edge": "è¾¹ç¼˜ç¾½åŒ–"
        }
    
    RETURN_TYPES = ("IMAGE", "MASK")
    RETURN_NAMES = ("æ‹¼æ¥å›¾åƒ", "æœ€ç»ˆé®ç½©")
    FUNCTION = "advanced_composite"
    CATEGORY = "ğŸ³Pond/mask"
    OUTPUT_NODE = False
    
    # æ·»åŠ æè¿°ä¿¡æ¯
    DESCRIPTION = """
é®ç½©å›¾åƒæ‹¼æ¥èŠ‚ç‚¹ - æ™ºèƒ½å°†ä¸»ä½“å›¾åƒæ‹¼æ¥åˆ°èƒŒæ™¯å›¾åƒçš„æŒ‡å®šä½ç½®

ä½¿ç”¨æ–¹æ³•ï¼š
1. è¿æ¥èƒŒæ™¯å›¾åƒå’Œä¸»ä½“å›¾åƒ
2. æä¾›ä¸»ä½“é®ç½©æ¥æŠ å–ä¸»ä½“
3. æä¾›ä½ç½®é®ç½©æ¥æŒ‡å®šæ‹¼æ¥ä½ç½®ï¼ˆç™½è‰²åŒºåŸŸï¼‰
4. è°ƒæ•´ç¼©æ”¾æ¨¡å¼å’Œå¯¹é½æ–¹å¼
5. ä½¿ç”¨è¾¹ç¼˜å¤„ç†è®©æ‹¼æ¥æ›´è‡ªç„¶

æç¤ºï¼š
- ä½ç½®é®ç½©çš„ç™½è‰²åŒºåŸŸå†³å®šæ‹¼æ¥ä½ç½®
- è¾¹ç¼˜æ¨¡ç³Šå’Œç¾½åŒ–å¯ä»¥è®©æ‹¼æ¥æ›´è‡ªç„¶
- ä¸åŒçš„æ··åˆæ¨¡å¼é€‚åˆä¸åŒåœºæ™¯
"""
    
    def advanced_composite(self, background_image, subject_image, subject_mask, position_mask, 
                          scale_mode, alignment, edge_blur, blend_mode, feather_edge):
        """
        æ‰§è¡Œé®ç½©å›¾åƒæ‹¼æ¥çš„ä¸»è¦å‡½æ•°
        """
        # è½¬æ¢tensoråˆ°numpyæ•°ç»„
        bg_img = self.tensor_to_numpy(background_image)
        subj_img = self.tensor_to_numpy(subject_image)
        subj_mask_np = self.mask_tensor_to_numpy(subject_mask)
        pos_mask_np = self.mask_tensor_to_numpy(position_mask)
        
        # éªŒè¯å°ºå¯¸åŒ¹é…
        if not self.validate_dimensions(bg_img, pos_mask_np, subj_img, subj_mask_np):
            # å¦‚æœå°ºå¯¸ä¸åŒ¹é…ï¼Œè¿›è¡Œæ™ºèƒ½è°ƒæ•´
            bg_img, pos_mask_np, subj_img, subj_mask_np = self.smart_resize(
                bg_img, pos_mask_np, subj_img, subj_mask_np
            )
        
        # æ­¥éª¤1ï¼šç”¨ä¸»ä½“é®ç½©æŠ å–ä¸»ä½“
        extracted_subject = self.extract_subject(subj_img, subj_mask_np)
        
        # æ­¥éª¤2ï¼šåˆ†ææ‹¼æ¥ä½ç½®é®ç½©ï¼Œè·å–ç›®æ ‡åŒºåŸŸ
        target_bbox = self.get_position_bbox(pos_mask_np)
        if target_bbox is None:
            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ç™½è‰²åŒºåŸŸï¼Œè¿”å›åŸèƒŒæ™¯
            print("âš ï¸ è­¦å‘Šï¼šä½ç½®é®ç½©ä¸­æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„ç™½è‰²åŒºåŸŸï¼Œè¿”å›åŸèƒŒæ™¯å›¾åƒ")
            return (self.numpy_to_tensor(bg_img), self.numpy_to_tensor(pos_mask_np))
        
        # æ­¥éª¤3ï¼šå°†æŠ å–çš„ä¸»ä½“ç¼©æ”¾åˆ°ç›®æ ‡å°ºå¯¸
        scaled_subject, scaled_mask = self.scale_subject_to_target(
            extracted_subject, subj_mask_np, target_bbox, scale_mode, alignment, bg_img.shape
        )
        
        # æ­¥éª¤4ï¼šåº”ç”¨è¾¹ç¼˜å¤„ç†
        if feather_edge or edge_blur > 0:
            scaled_mask = self.apply_edge_processing(scaled_mask, edge_blur, feather_edge)
        
        # æ­¥éª¤5ï¼šæ‰§è¡Œæœ€ç»ˆæ‹¼æ¥
        result_img = self.blend_images(bg_img, scaled_subject, scaled_mask, blend_mode)
        
        # è½¬æ¢å›tensoræ ¼å¼
        result_tensor = self.numpy_to_tensor(result_img)
        final_mask_tensor = self.numpy_to_tensor(scaled_mask)
        
        return (result_tensor, final_mask_tensor)
    
    def validate_dimensions(self, bg_img, pos_mask, subj_img, subj_mask):
        """éªŒè¯è¾“å…¥å°ºå¯¸æ˜¯å¦ç¬¦åˆè¦æ±‚"""
        bg_h, bg_w = bg_img.shape[:2]
        pos_h, pos_w = pos_mask.shape[:2]
        subj_h, subj_w = subj_img.shape[:2]
        mask_h, mask_w = subj_mask.shape[:2]
        
        bg_match = (bg_h == pos_h and bg_w == pos_w)
        subj_match = (subj_h == mask_h and subj_w == mask_w)
        
        if not bg_match:
            print(f"ğŸ“ èƒŒæ™¯å›¾åƒå°ºå¯¸ ({bg_w}x{bg_h}) ä¸ä½ç½®é®ç½©å°ºå¯¸ ({pos_w}x{pos_h}) ä¸åŒ¹é…")
        if not subj_match:
            print(f"ğŸ“ ä¸»ä½“å›¾åƒå°ºå¯¸ ({subj_w}x{subj_h}) ä¸ä¸»ä½“é®ç½©å°ºå¯¸ ({mask_w}x{mask_h}) ä¸åŒ¹é…")
        
        return bg_match and subj_match
    
    def smart_resize(self, bg_img, pos_mask, subj_img, subj_mask):
        """æ™ºèƒ½è°ƒæ•´å°ºå¯¸ä»¥ç¬¦åˆè¦æ±‚"""
        print("ğŸ”§ æ­£åœ¨è‡ªåŠ¨è°ƒæ•´å°ºå¯¸...")
        
        # ä»¥èƒŒæ™¯å›¾åƒå°ºå¯¸ä¸ºåŸºå‡†è°ƒæ•´ä½ç½®é®ç½©
        bg_h, bg_w = bg_img.shape[:2]
        pos_mask_resized = cv2.resize(pos_mask, (bg_w, bg_h))
        
        # ä»¥ä¸»ä½“å›¾åƒå°ºå¯¸ä¸ºåŸºå‡†è°ƒæ•´ä¸»ä½“é®ç½©
        subj_h, subj_w = subj_img.shape[:2]
        subj_mask_resized = cv2.resize(subj_mask, (subj_w, subj_h))
        
        print("âœ… å°ºå¯¸è°ƒæ•´å®Œæˆ")
        return bg_img, pos_mask_resized, subj_img, subj_mask_resized
    
    def extract_subject(self, subject_img, subject_mask):
        """ä½¿ç”¨é®ç½©ä»ä¸»ä½“å›¾åƒä¸­æŠ å–ä¸»ä½“"""
        # ç¡®ä¿é®ç½©æœ‰æ­£ç¡®çš„ç»´åº¦
        if len(subject_mask.shape) == 2:
            mask_3d = np.expand_dims(subject_mask, axis=2)
            mask_3d = np.repeat(mask_3d, 3, axis=2)
        else:
            mask_3d = subject_mask
        
        # æŠ å–ä¸»ä½“ï¼Œä¿æŒé€æ˜èƒŒæ™¯
        extracted = subject_img * mask_3d
        
        return extracted
    
    def get_position_bbox(self, position_mask):
        """ä»ä½ç½®é®ç½©ä¸­è·å–ç™½è‰²åŒºåŸŸçš„è¾¹ç•Œæ¡†"""
        # äºŒå€¼åŒ–é®ç½©
        binary_mask = (position_mask > 0.5).astype(np.uint8)
        
        # æŸ¥æ‰¾è½®å»“
        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            return None
        
        # æ‰¾åˆ°æœ€å¤§çš„è½®å»“
        largest_contour = max(contours, key=cv2.contourArea)
        
        # è·å–è¾¹ç•Œæ¡†
        x, y, w, h = cv2.boundingRect(largest_contour)
        
        print(f"ğŸ“ æ£€æµ‹åˆ°æ‹¼æ¥ä½ç½®ï¼šx={x}, y={y}, å®½åº¦={w}, é«˜åº¦={h}")
        
        return {
            'x': x, 'y': y, 'width': w, 'height': h,
            'x2': x + w, 'y2': y + h
        }
    
    def scale_subject_to_target(self, extracted_subject, subject_mask, target_bbox, scale_mode, alignment, bg_shape):
        """å°†æŠ å–çš„ä¸»ä½“ç¼©æ”¾åˆ°ç›®æ ‡åŒºåŸŸ"""
        target_w, target_h = target_bbox['width'], target_bbox['height']
        bg_h, bg_w = bg_shape[:2]
        
        # è·å–ä¸»ä½“çš„æœ‰æ•ˆåŒºåŸŸ
        subject_bbox = self.get_subject_bbox(subject_mask)
        if subject_bbox is None:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆä¸»ä½“åŒºåŸŸï¼Œè¿”å›å’ŒèƒŒæ™¯ç›¸åŒå°ºå¯¸çš„ç©ºå›¾åƒ
            print("âš ï¸ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„ä¸»ä½“åŒºåŸŸ")
            empty_img = np.zeros((bg_h, bg_w, 3), dtype=np.float32)
            empty_mask = np.zeros((bg_h, bg_w), dtype=np.float32)
            return empty_img, empty_mask
        
        # è£å‰ªä¸»ä½“åˆ°æœ‰æ•ˆåŒºåŸŸ
        cropped_subject = extracted_subject[
            subject_bbox['y']:subject_bbox['y2'],
            subject_bbox['x']:subject_bbox['x2']
        ]
        cropped_mask = subject_mask[
            subject_bbox['y']:subject_bbox['y2'],
            subject_bbox['x']:subject_bbox['x2']
        ]
        
        print(f"ğŸ¯ ç¼©æ”¾æ¨¡å¼ï¼š{scale_mode}")
        
        # æ ¹æ®ç¼©æ”¾æ¨¡å¼å¤„ç†
        if scale_mode == "æ‹‰ä¼¸":
            # ç›´æ¥æ‹‰ä¼¸åˆ°ç›®æ ‡å°ºå¯¸
            scaled_subject = cv2.resize(cropped_subject, (target_w, target_h))
            scaled_mask = cv2.resize(cropped_mask, (target_w, target_h))
        elif scale_mode == "é€‚åº”":
            # ä¿æŒå®½é«˜æ¯”ï¼Œé€‚åº”ç›®æ ‡å°ºå¯¸
            scaled_subject, scaled_mask = self.scale_with_aspect_ratio(
                cropped_subject, cropped_mask, target_w, target_h, "fit"
            )
        else:  # å¡«å……
            # ä¿æŒå®½é«˜æ¯”ï¼Œå¡«å……ç›®æ ‡å°ºå¯¸
            scaled_subject, scaled_mask = self.scale_with_aspect_ratio(
                cropped_subject, cropped_mask, target_w, target_h, "fill"
            )
        
        # åˆ›å»ºå’ŒèƒŒæ™¯å›¾åƒç›¸åŒå°ºå¯¸çš„æœ€ç»ˆå›¾åƒå’Œé®ç½©
        final_subject = np.zeros((bg_h, bg_w, 3), dtype=np.float32)
        final_mask = np.zeros((bg_h, bg_w), dtype=np.float32)
        
        # æ ¹æ®å¯¹é½æ–¹å¼è®¡ç®—æ”¾ç½®ä½ç½®
        placement_x, placement_y = self.calculate_placement(
            target_bbox, scaled_subject.shape, alignment
        )
        
        print(f"ğŸ“ å¯¹é½æ–¹å¼ï¼š{alignment}ï¼Œæ”¾ç½®ä½ç½®ï¼š({placement_x}, {placement_y})")
        
        # æ·»åŠ è¾¹ç•Œæ£€æŸ¥å’Œå®‰å…¨è£å‰ª
        scaled_h, scaled_w = scaled_subject.shape[:2]
        
        # ç¡®ä¿æ”¾ç½®ä½ç½®ä¸ä¼šè¶…å‡ºèƒŒæ™¯å›¾åƒè¾¹ç•Œ
        placement_x = max(0, min(placement_x, bg_w - 1))
        placement_y = max(0, min(placement_y, bg_h - 1))
        
        # è®¡ç®—å®é™…å¯ä»¥æ”¾ç½®çš„åŒºåŸŸå¤§å°
        available_w = bg_w - placement_x
        available_h = bg_h - placement_y
        
        # å¦‚æœç¼©æ”¾åçš„å›¾åƒè¶…å‡ºå¯ç”¨ç©ºé—´ï¼Œéœ€è¦è£å‰ª
        actual_w = min(scaled_w, available_w)
        actual_h = min(scaled_h, available_h)
        
        # å¦‚æœéœ€è¦è£å‰ªï¼Œä»ç¼©æ”¾å›¾åƒçš„ä¸­å¿ƒå¼€å§‹è£å‰ª
        if actual_w < scaled_w or actual_h < scaled_h:
            crop_start_x = max(0, (scaled_w - actual_w) // 2)
            crop_start_y = max(0, (scaled_h - actual_h) // 2)
            
            scaled_subject_cropped = scaled_subject[
                crop_start_y:crop_start_y + actual_h,
                crop_start_x:crop_start_x + actual_w
            ]
            scaled_mask_cropped = scaled_mask[
                crop_start_y:crop_start_y + actual_h,
                crop_start_x:crop_start_x + actual_w
            ]
        else:
            scaled_subject_cropped = scaled_subject
            scaled_mask_cropped = scaled_mask
        
        # å®‰å…¨åœ°æ”¾ç½®ä¸»ä½“
        end_x = placement_x + actual_w
        end_y = placement_y + actual_h
        
        try:
            final_subject[placement_y:end_y, placement_x:end_x] = scaled_subject_cropped
            final_mask[placement_y:end_y, placement_x:end_x] = scaled_mask_cropped
        except ValueError as e:
            print(f"ğŸš¨ æ‹¼æ¥è­¦å‘Š: {e}")
            print(f"ç›®æ ‡åŒºåŸŸ: [{placement_y}:{end_y}, {placement_x}:{end_x}] = ({end_y-placement_y}, {end_x-placement_x})")
            print(f"æºå›¾åƒå°ºå¯¸: {scaled_subject_cropped.shape}")
            # å¦‚æœä»ç„¶å‡ºé”™ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„æ–¹æ³•
            min_h = min(end_y - placement_y, scaled_subject_cropped.shape[0])
            min_w = min(end_x - placement_x, scaled_subject_cropped.shape[1])
            final_subject[placement_y:placement_y+min_h, placement_x:placement_x+min_w] = scaled_subject_cropped[:min_h, :min_w]
            final_mask[placement_y:placement_y+min_h, placement_x:placement_x+min_w] = scaled_mask_cropped[:min_h, :min_w]
        
        return final_subject, final_mask
    
    def get_subject_bbox(self, mask):
        """è·å–ä¸»ä½“çš„è¾¹ç•Œæ¡†"""
        binary_mask = (mask > 0.1).astype(np.uint8)
        coords = np.column_stack(np.where(binary_mask > 0))
        
        if len(coords) == 0:
            return None
        
        y_min, x_min = coords.min(axis=0)
        y_max, x_max = coords.max(axis=0)
        
        return {
            'x': x_min, 'y': y_min,
            'width': x_max - x_min + 1,
            'height': y_max - y_min + 1,
            'x2': x_max + 1, 'y2': y_max + 1
        }
    
    def scale_with_aspect_ratio(self, img, mask, target_w, target_h, mode):
        """ä¿æŒå®½é«˜æ¯”çš„ç¼©æ”¾"""
        img_h, img_w = img.shape[:2]
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        scale_w = target_w / img_w
        scale_h = target_h / img_h
        
        if mode == "fit":
            scale = min(scale_w, scale_h)
        else:  # fill
            scale = max(scale_w, scale_h)
        
        # è®¡ç®—æ–°å°ºå¯¸
        new_w = int(img_w * scale)
        new_h = int(img_h * scale)
        
        # ç¼©æ”¾
        scaled_img = cv2.resize(img, (new_w, new_h))
        scaled_mask = cv2.resize(mask, (new_w, new_h))
        
        # å¦‚æœæ˜¯fitæ¨¡å¼ä¸”å°ºå¯¸å°äºç›®æ ‡ï¼Œéœ€è¦å±…ä¸­æ”¾ç½®
        if mode == "fit" and (new_w < target_w or new_h < target_h):
            final_img = np.zeros((target_h, target_w, img.shape[2]), dtype=img.dtype)
            final_mask = np.zeros((target_h, target_w), dtype=mask.dtype)
            
            start_x = (target_w - new_w) // 2
            start_y = (target_h - new_h) // 2
            
            final_img[start_y:start_y+new_h, start_x:start_x+new_w] = scaled_img
            final_mask[start_y:start_y+new_h, start_x:start_x+new_w] = scaled_mask
            
            return final_img, final_mask
        
        # å¦‚æœæ˜¯fillæ¨¡å¼ä¸”å°ºå¯¸å¤§äºç›®æ ‡ï¼Œéœ€è¦è£å‰ª
        elif mode == "fill" and (new_w > target_w or new_h > target_h):
            start_x = max(0, (new_w - target_w) // 2)
            start_y = max(0, (new_h - target_h) // 2)
            
            # ç¡®ä¿è£å‰ªåŒºåŸŸä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
            end_x = min(start_x + target_w, new_w)
            end_y = min(start_y + target_h, new_h)
            actual_w = end_x - start_x
            actual_h = end_y - start_y
            
            cropped_img = scaled_img[start_y:end_y, start_x:end_x]
            cropped_mask = scaled_mask[start_y:end_y, start_x:end_x]
            
            # å¦‚æœè£å‰ªåå°ºå¯¸ä¸è¶³ï¼Œç”¨é›¶å¡«å……
            if actual_w < target_w or actual_h < target_h:
                final_img = np.zeros((target_h, target_w, img.shape[2]), dtype=img.dtype)
                final_mask = np.zeros((target_h, target_w), dtype=mask.dtype)
                final_img[:actual_h, :actual_w] = cropped_img
                final_mask[:actual_h, :actual_w] = cropped_mask
                return final_img, final_mask
            
            return cropped_img, cropped_mask
        
        return scaled_img, scaled_mask
    
    def calculate_placement(self, target_bbox, subject_shape, alignment):
        """è®¡ç®—ä¸»ä½“åœ¨ç›®æ ‡åŒºåŸŸçš„æ”¾ç½®ä½ç½®"""
        target_x, target_y = target_bbox['x'], target_bbox['y']
        target_w, target_h = target_bbox['width'], target_bbox['height']
        subj_h, subj_w = subject_shape[:2]
        
        if alignment == "å±…ä¸­":
            x = target_x + max(0, (target_w - subj_w) // 2)
            y = target_y + max(0, (target_h - subj_h) // 2)
        elif alignment == "å·¦ä¸Š":
            x, y = target_x, target_y
        elif alignment == "å³ä¸Š":
            x = target_x + max(0, target_w - subj_w)
            y = target_y
        elif alignment == "å·¦ä¸‹":
            x = target_x
            y = target_y + max(0, target_h - subj_h)
        else:  # å³ä¸‹
            x = target_x + max(0, target_w - subj_w)
            y = target_y + max(0, target_h - subj_h)
        
        return max(0, x), max(0, y)
    
    def apply_edge_processing(self, mask, blur_radius, feather_edge):
        """åº”ç”¨è¾¹ç¼˜å¤„ç†æ•ˆæœ"""
        processed_mask = mask.copy()
        
        if feather_edge:
            # ç¾½åŒ–è¾¹ç¼˜
            print("ğŸ¨ åº”ç”¨è¾¹ç¼˜ç¾½åŒ–...")
            processed_mask = self.feather_mask_edges(processed_mask)
        
        if blur_radius > 0:
            # è¾¹ç¼˜æ¨¡ç³Š
            print(f"ğŸ¨ åº”ç”¨è¾¹ç¼˜æ¨¡ç³Š (åŠå¾„: {blur_radius})...")
            processed_mask = self.apply_edge_blur(processed_mask, blur_radius)
        
        return processed_mask
    
    def feather_mask_edges(self, mask):
        """ç¾½åŒ–é®ç½©è¾¹ç¼˜"""
        # ä½¿ç”¨å½¢æ€å­¦æ“ä½œåˆ›å»ºç¾½åŒ–æ•ˆæœ
        mask_uint8 = (mask * 255).astype(np.uint8)
        
        # åˆ›å»ºè·ç¦»å˜æ¢
        dist_transform = cv2.distanceTransform(mask_uint8, cv2.DIST_L2, 5)
        
        # å½’ä¸€åŒ–è·ç¦»å˜æ¢
        if dist_transform.max() > 0:
            feathered = dist_transform / dist_transform.max()
            # åº”ç”¨å¹³æ»‘æ›²çº¿
            feathered = np.power(feathered, 0.5)
        else:
            feathered = mask
        
        return feathered.astype(np.float32)
    
    def tensor_to_numpy(self, tensor):
        """å°†ComfyUIçš„å›¾åƒtensorè½¬æ¢ä¸ºnumpyæ•°ç»„"""
        if len(tensor.shape) == 4:  # batch dimension
            tensor = tensor[0]
        
        # ä»CHWæˆ–HWCæ ¼å¼è½¬æ¢ä¸ºHWC
        if tensor.shape[0] == 3 or tensor.shape[0] == 1:  # CHWæ ¼å¼
            tensor = tensor.permute(1, 2, 0)
        
        # è½¬æ¢ä¸ºnumpyå¹¶ç¡®ä¿æ•°æ®ç±»å‹
        img = tensor.cpu().numpy()
        if img.dtype != np.float32:
            img = img.astype(np.float32)
        
        # ç¡®ä¿å€¼åœ¨0-1èŒƒå›´å†…
        img = np.clip(img, 0, 1)
        
        return img
    
    def mask_tensor_to_numpy(self, mask_tensor):
        """å°†é®ç½©tensorè½¬æ¢ä¸ºnumpyæ•°ç»„"""
        if len(mask_tensor.shape) == 3:  # ç§»é™¤batch dimension
            mask_tensor = mask_tensor[0]
        
        mask = mask_tensor.cpu().numpy()
        if mask.dtype != np.float32:
            mask = mask.astype(np.float32)
        
        # ç¡®ä¿å€¼åœ¨0-1èŒƒå›´å†…
        mask = np.clip(mask, 0, 1)
        
        return mask
    
    def numpy_to_tensor(self, img):
        """å°†numpyæ•°ç»„è½¬æ¢ä¸ºComfyUIçš„tensoræ ¼å¼"""
        # ç¡®ä¿æ˜¯HWCæ ¼å¼
        if len(img.shape) == 2:  # ç°åº¦å›¾
            img = np.expand_dims(img, axis=2)
        
        # è½¬æ¢ä¸ºtensor
        tensor = torch.from_numpy(img).float()
        
        # æ·»åŠ batch dimension
        tensor = tensor.unsqueeze(0)
        
        return tensor
    
    def apply_edge_blur(self, mask, blur_radius):
        """å¯¹é®ç½©è¾¹ç¼˜åº”ç”¨æ¨¡ç³Šæ•ˆæœ"""
        # å°†maskè½¬æ¢ä¸º0-255èŒƒå›´ç”¨äºOpenCVå¤„ç†
        mask_uint8 = (mask * 255).astype(np.uint8)
        
        # é«˜æ–¯æ¨¡ç³Š
        kernel_size = max(3, int(blur_radius * 2) | 1)  # ç¡®ä¿æ˜¯å¥‡æ•°
        blurred_mask = cv2.GaussianBlur(mask_uint8, (kernel_size, kernel_size), blur_radius)
        
        # è½¬æ¢å›0-1èŒƒå›´
        blurred_mask = blurred_mask.astype(np.float32) / 255.0
        
        return blurred_mask
    
    def blend_images(self, background, subject, mask, blend_mode):
        """æ ¹æ®é®ç½©å’Œæ··åˆæ¨¡å¼æ··åˆå›¾åƒ"""
        # ç¡®ä¿æ‰€æœ‰å›¾åƒå°ºå¯¸ä¸€è‡´
        bg_h, bg_w = background.shape[:2]
        
        # å¦‚æœsubjectæˆ–maskå°ºå¯¸ä¸åŒ¹é…ï¼Œè°ƒæ•´ä¸ºèƒŒæ™¯å›¾åƒå°ºå¯¸
        if subject.shape[:2] != (bg_h, bg_w):
            print(f"ğŸ”§ è°ƒæ•´ä¸»ä½“å›¾åƒå°ºå¯¸: {subject.shape[:2]} -> ({bg_h}, {bg_w})")
            subject = cv2.resize(subject, (bg_w, bg_h))
        
        if mask.shape[:2] != (bg_h, bg_w):
            print(f"ğŸ”§ è°ƒæ•´é®ç½©å°ºå¯¸: {mask.shape[:2]} -> ({bg_h}, {bg_w})")
            mask = cv2.resize(mask, (bg_w, bg_h))
        
        # ç¡®ä¿é®ç½©æœ‰æ­£ç¡®çš„ç»´åº¦
        if len(mask.shape) == 2:
            mask = np.expand_dims(mask, axis=2)
        if mask.shape[2] == 1:
            mask = np.repeat(mask, 3, axis=2)
        
        print(f"ğŸ¨ åº”ç”¨æ··åˆæ¨¡å¼ï¼š{blend_mode}")
        
        # æ ¹æ®æ··åˆæ¨¡å¼å¤„ç†
        if blend_mode == "æ­£å¸¸":
            blended = subject
        elif blend_mode == "å åŠ ":
            blended = background * subject
        elif blend_mode == "æ»¤è‰²":
            blended = 1 - (1 - background) * (1 - subject)
        elif blend_mode == "è¦†ç›–":
            # è¦†ç›–æ··åˆæ¨¡å¼
            blended = np.where(background < 0.5,
                             2 * background * subject,
                             1 - 2 * (1 - background) * (1 - subject))
        else:
            blended = subject
        
        # ä½¿ç”¨é®ç½©æ··åˆèƒŒæ™¯å’Œå¤„ç†åçš„ä¸»ä½“å›¾åƒ
        result = background * (1 - mask) + blended * mask
        
        # ç¡®ä¿å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
        result = np.clip(result, 0, 1)
        
        print("âœ… å›¾åƒæ‹¼æ¥å®Œæˆï¼")
        
        return result


class MaskBasedImageComposite:
    """
    ComfyUIè‡ªå®šä¹‰èŠ‚ç‚¹ï¼šåŸºäºé®ç½©çš„å›¾åƒæ‹¼æ¥
    æ ¹æ®é®ç½©å°†ä¸»ä½“å›¾åƒæ‹¼æ¥åˆ°èƒŒæ™¯å›¾åƒä¸Šï¼Œæ”¯æŒè¾¹ç¼˜æ¨¡ç³Šæ§åˆ¶
    """
    
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "background_image": ("IMAGE", {"tooltip": "èƒŒæ™¯å›¾åƒ - Background Image"}),  
                "subject_image": ("IMAGE", {"tooltip": "ä¸»ä½“å›¾åƒ - Subject Image"}),     
                "range_mask": ("MASK", {"tooltip": "æ‹¼æ¥èŒƒå›´é®ç½© - Range Mask"}),         
                "subject_mask": ("MASK", {"tooltip": "ä¸»ä½“å½¢çŠ¶é®ç½© - Subject Mask"}),       
                "edge_blur": ("FLOAT", {
                    "default": 0.0,
                    "min": 0.0,
                    "max": 50.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "è¾¹ç¼˜æ¨¡ç³Šç¨‹åº¦ - Edge Blur Amount"
                }),  
                "blend_mode": (["normal", "multiply", "screen", "overlay"], {
                    "default": "normal",
                    "tooltip": "æ··åˆæ¨¡å¼ - Blend Mode"
                }),  
            }
        }
    
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("æ‹¼æ¥å›¾åƒ",)
    FUNCTION = "composite_images"
    CATEGORY = "ğŸ³Pond/mask"
    
    def composite_images(self, background_image, subject_image, range_mask, subject_mask, edge_blur, blend_mode):
        """
        æ‰§è¡Œå›¾åƒæ‹¼æ¥çš„ä¸»è¦å‡½æ•°
        """
        # è½¬æ¢tensoråˆ°numpyæ•°ç»„
        bg_img = self.tensor_to_numpy(background_image)
        subj_img = self.tensor_to_numpy(subject_image)
        range_mask_np = self.mask_tensor_to_numpy(range_mask)
        subject_mask_np = self.mask_tensor_to_numpy(subject_mask)
        
        # è·å–æ‰€æœ‰è¾“å…¥çš„å°ºå¯¸ï¼Œæ‰¾åˆ°æœ€å¤§å°ºå¯¸
        bg_h, bg_w = bg_img.shape[:2]
        subj_h, subj_w = subj_img.shape[:2]
        range_h, range_w = range_mask_np.shape[:2]
        subject_h, subject_w = subject_mask_np.shape[:2]
        
        # è®¡ç®—æœ€å¤§ç”»å¸ƒå°ºå¯¸
        max_h = max(bg_h, subj_h, range_h, subject_h)
        max_w = max(bg_w, subj_w, range_w, subject_w)
        
        # å°†æ‰€æœ‰å›¾åƒå’Œé®ç½©å±…ä¸­å¯¹é½åˆ°æœ€å¤§ç”»å¸ƒ
        bg_img_aligned = self.center_align_image(bg_img, max_h, max_w)
        subj_img_aligned = self.center_align_image(subj_img, max_h, max_w)
        range_mask_aligned = self.center_align_mask(range_mask_np, max_h, max_w)
        subject_mask_aligned = self.center_align_mask(subject_mask_np, max_h, max_w)
        
        # å¤„ç†é®ç½©ç»„åˆ
        # range_maskå®šä¹‰æ‹¼æ¥çš„æ€»ä½“èŒƒå›´
        # subject_maskå®šä¹‰åœ¨è¯¥èŒƒå›´å†…çš„å…·ä½“å½¢çŠ¶
        combined_mask = range_mask_aligned * subject_mask_aligned
        
        # è¾¹ç¼˜æ¨¡ç³Šå¤„ç†
        if edge_blur > 0:
            combined_mask = self.apply_edge_blur(combined_mask, edge_blur)
        
        # æ‰§è¡Œå›¾åƒæ··åˆ
        result_img = self.blend_images(bg_img_aligned, subj_img_aligned, combined_mask, blend_mode)
        
        # è½¬æ¢å›tensoræ ¼å¼
        result_tensor = self.numpy_to_tensor(result_img)
        
        return (result_tensor,)
    
    def center_align_image(self, img, target_h, target_w):
        """å°†å›¾åƒå±…ä¸­å¯¹é½åˆ°ç›®æ ‡å°ºå¯¸"""
        current_h, current_w = img.shape[:2]
        
        # å¦‚æœå·²ç»æ˜¯ç›®æ ‡å°ºå¯¸ï¼Œç›´æ¥è¿”å›
        if current_h == target_h and current_w == target_w:
            return img
        
        # åˆ›å»ºç›®æ ‡å°ºå¯¸çš„ç”»å¸ƒï¼Œå¡«å……é»‘è‰²
        if len(img.shape) == 3:  # å½©è‰²å›¾åƒ
            canvas = np.zeros((target_h, target_w, img.shape[2]), dtype=img.dtype)
        else:  # ç°åº¦å›¾åƒ
            canvas = np.zeros((target_h, target_w), dtype=img.dtype)
        
        # è®¡ç®—å±…ä¸­ä½ç½®
        start_y = (target_h - current_h) // 2
        start_x = (target_w - current_w) // 2
        end_y = start_y + current_h
        end_x = start_x + current_w
        
        # å°†åŸå›¾åƒæ”¾åˆ°ç”»å¸ƒä¸­å¿ƒ
        canvas[start_y:end_y, start_x:end_x] = img
        
        return canvas
    
    def center_align_mask(self, mask, target_h, target_w):
        """å°†é®ç½©å±…ä¸­å¯¹é½åˆ°ç›®æ ‡å°ºå¯¸"""
        current_h, current_w = mask.shape[:2]
        
        # å¦‚æœå·²ç»æ˜¯ç›®æ ‡å°ºå¯¸ï¼Œç›´æ¥è¿”å›
        if current_h == target_h and current_w == target_w:
            return mask
        
        # åˆ›å»ºç›®æ ‡å°ºå¯¸çš„ç”»å¸ƒï¼Œå¡«å……0ï¼ˆé»‘è‰²é®ç½©ï¼‰
        canvas = np.zeros((target_h, target_w), dtype=mask.dtype)
        
        # è®¡ç®—å±…ä¸­ä½ç½®
        start_y = (target_h - current_h) // 2
        start_x = (target_w - current_w) // 2
        end_y = start_y + current_h
        end_x = start_x + current_w
        
        # å°†åŸé®ç½©æ”¾åˆ°ç”»å¸ƒä¸­å¿ƒ
        canvas[start_y:end_y, start_x:end_x] = mask
        
        return canvas

    def tensor_to_numpy(self, tensor):
        """å°†ComfyUIçš„å›¾åƒtensorè½¬æ¢ä¸ºnumpyæ•°ç»„"""
        if len(tensor.shape) == 4:  # batch dimension
            tensor = tensor[0]
        
        # ä»CHWæˆ–HWCæ ¼å¼è½¬æ¢ä¸ºHWC
        if tensor.shape[0] == 3 or tensor.shape[0] == 1:  # CHWæ ¼å¼
            tensor = tensor.permute(1, 2, 0)
        
        # è½¬æ¢ä¸ºnumpyå¹¶ç¡®ä¿æ•°æ®ç±»å‹
        img = tensor.cpu().numpy()
        if img.dtype != np.float32:
            img = img.astype(np.float32)
        
        # ç¡®ä¿å€¼åœ¨0-1èŒƒå›´å†…
        img = np.clip(img, 0, 1)
        
        return img
    
    def mask_tensor_to_numpy(self, mask_tensor):
        """å°†é®ç½©tensorè½¬æ¢ä¸ºnumpyæ•°ç»„"""
        if len(mask_tensor.shape) == 3:  # ç§»é™¤batch dimension
            mask_tensor = mask_tensor[0]
        
        mask = mask_tensor.cpu().numpy()
        if mask.dtype != np.float32:
            mask = mask.astype(np.float32)
        
        # ç¡®ä¿å€¼åœ¨0-1èŒƒå›´å†…
        mask = np.clip(mask, 0, 1)
        
        return mask
    
    def numpy_to_tensor(self, img):
        """å°†numpyæ•°ç»„è½¬æ¢ä¸ºComfyUIçš„tensoræ ¼å¼"""
        # ç¡®ä¿æ˜¯HWCæ ¼å¼
        if len(img.shape) == 2:  # ç°åº¦å›¾
            img = np.expand_dims(img, axis=2)
        
        # è½¬æ¢ä¸ºtensor
        tensor = torch.from_numpy(img).float()
        
        # æ·»åŠ batch dimension
        tensor = tensor.unsqueeze(0)
        
        return tensor
    
    def apply_edge_blur(self, mask, blur_radius):
        """å¯¹é®ç½©è¾¹ç¼˜åº”ç”¨æ¨¡ç³Šæ•ˆæœ"""
        # å°†maskè½¬æ¢ä¸º0-255èŒƒå›´ç”¨äºOpenCVå¤„ç†
        mask_uint8 = (mask * 255).astype(np.uint8)
        
        # é«˜æ–¯æ¨¡ç³Š
        kernel_size = max(3, int(blur_radius * 2) | 1)  # ç¡®ä¿æ˜¯å¥‡æ•°
        blurred_mask = cv2.GaussianBlur(mask_uint8, (kernel_size, kernel_size), blur_radius)
        
        # è½¬æ¢å›0-1èŒƒå›´
        blurred_mask = blurred_mask.astype(np.float32) / 255.0
        
        return blurred_mask
    
    def blend_images(self, background, subject, mask, blend_mode):
        """æ ¹æ®é®ç½©å’Œæ··åˆæ¨¡å¼æ··åˆå›¾åƒ"""
        # ç¡®ä¿é®ç½©æœ‰æ­£ç¡®çš„ç»´åº¦
        if len(mask.shape) == 2:
            mask = np.expand_dims(mask, axis=2)
        if mask.shape[2] == 1:
            mask = np.repeat(mask, 3, axis=2)
        
        # æ ¹æ®æ··åˆæ¨¡å¼å¤„ç†
        if blend_mode == "normal":
            blended = subject
        elif blend_mode == "multiply":
            blended = background * subject
        elif blend_mode == "screen":
            blended = 1 - (1 - background) * (1 - subject)
        elif blend_mode == "overlay":
            # è¦†ç›–æ··åˆæ¨¡å¼
            blended = np.where(background < 0.5,
                             2 * background * subject,
                             1 - 2 * (1 - background) * (1 - subject))
        else:
            blended = subject
        
        # ä½¿ç”¨é®ç½©æ··åˆèƒŒæ™¯å’Œå¤„ç†åçš„ä¸»ä½“å›¾åƒ
        result = background * (1 - mask) + blended * mask
        
        # ç¡®ä¿å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
        result = np.clip(result, 0, 1)
        
        return result



# ComfyUIèŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "AdvancedMaskImageComposite": AdvancedMaskImageComposite,
    "MaskBasedImageComposite": MaskBasedImageComposite
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "AdvancedMaskImageComposite": "ğŸ³é®ç½©å›¾åƒæ‹¼æ¥",
    "MaskBasedImageComposite": "ğŸ­ é®ç½©å›¾åƒæ‹¼æ¥ (Mask Image Composite)"
}