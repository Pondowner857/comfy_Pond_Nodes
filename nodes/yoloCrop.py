import torch
import numpy as np
import os
import sys
from PIL import Image
import folder_paths

# å°è¯•å¯¼å…¥ultralytics
try:
    from ultralytics import YOLO
except ImportError:
    print("è­¦å‘Š: æœªå®‰è£…ultralyticsåº“ï¼Œè¯·è¿è¡Œ: pip install ultralytics")
    YOLO = None

class YoloBboxesCropNode:

    def __init__(self):
        self.model = None
        self.current_model_name = None
        
    @classmethod
    def INPUT_TYPES(cls):
        # è·å–YOLOæ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„
        yolo_models_dir = os.path.join(folder_paths.models_dir, "yolo")
        
        # è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        model_files = []
        if os.path.exists(yolo_models_dir):
            for file in os.listdir(yolo_models_dir):
                if file.endswith(('.pt', '.onnx', '.engine')):
                    model_files.append(file)
        
        if not model_files:
            model_files = ["è¯·å°†YOLOæ¨¡å‹æ”¾å…¥models/yoloæ–‡ä»¶å¤¹"]
        
        return {
            "required": {
                "image": ("IMAGE", {"display": "è¾“å…¥å›¾åƒ"}),
                "model_name": (model_files, {
                    "default": model_files[0] if model_files else "yolov8n.pt",
                    "display": "YOLOæ¨¡å‹"
                }),
                "confidence": ("FLOAT", {
                    "default": 0.25,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "display": "ç½®ä¿¡åº¦é˜ˆå€¼"
                }),
                "class_filter": ("STRING", {
                    "default": "å…¨éƒ¨",
                    "multiline": False,
                    "display": "ç±»åˆ«è¿‡æ»¤",
                    "tooltip": "è¦æ£€æµ‹çš„ç±»åˆ«ï¼Œç”¨é€—å·åˆ†éš”ï¼ˆå¦‚ï¼šäºº,æ±½è½¦ï¼‰æˆ–'å…¨éƒ¨'æ£€æµ‹æ‰€æœ‰"
                }),
                "expand_width": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 500,
                    "step": 10,
                    "display": "å®½åº¦æ‰©å±•"
                }),
                "expand_height": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 500,
                    "step": 10,
                    "display": "é«˜åº¦æ‰©å±•"
                }),
                "sort_by": (["é»˜è®¤", "ä»å·¦åˆ°å³", "ä»å³åˆ°å·¦", "ä»ä¸Šåˆ°ä¸‹", "ä»ä¸‹åˆ°ä¸Š", "ç½®ä¿¡åº¦é™åº", "ç½®ä¿¡åº¦å‡åº", "é¢ç§¯é™åº", "é¢ç§¯å‡åº"], {
                    "default": "ä»å·¦åˆ°å³",
                    "display": "æ’åºæ–¹å¼"
                }),
                "crop_mode": (["å…¨éƒ¨å¯¹è±¡", "å•ä¸ªå¯¹è±¡", "æŒ‰ç±»åˆ«"], {
                    "default": "å…¨éƒ¨å¯¹è±¡",
                    "display": "è£å‰ªæ¨¡å¼"
                }),
                "object_index": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                    "display": "å¯¹è±¡ç´¢å¼•"
                })
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK", "BBOXES", "STRING", "INT")
    RETURN_NAMES = ("è£å‰ªå›¾åƒ", "é®ç½©", "è¾¹ç•Œæ¡†", "æ£€æµ‹ä¿¡æ¯", "æ£€æµ‹æ•°é‡")
    OUTPUT_IS_LIST = (True, False, False, False, False)  # åªæœ‰å›¾åƒè¾“å‡ºä¸ºåˆ—è¡¨
    FUNCTION = "detect_and_crop"
    CATEGORY = "ğŸ³Pond/yolo"
    DESCRIPTION = "ä½¿ç”¨YOLOæ¨¡å‹æ£€æµ‹å›¾åƒä¸­çš„å¯¹è±¡å¹¶è¿›è¡Œæ™ºèƒ½è£å‰ªï¼Œæ”¯æŒå¤šç§æ’åºæ–¹å¼ï¼Œè¾“å‡ºå•å¼ é®ç½©"

    def load_model(self, model_name):
        """åŠ è½½YOLOæ¨¡å‹"""
        if YOLO is None:
            raise ImportError("è¯·å®‰è£…ultralyticsåº“: pip install ultralytics")
        
        # å¦‚æœå·²ç»åŠ è½½äº†ç›¸åŒçš„æ¨¡å‹ï¼Œç›´æ¥è¿”å›
        if self.model is not None and self.current_model_name == model_name:
            return self.model
        
        # æ„å»ºæ¨¡å‹è·¯å¾„
        model_path = os.path.join(folder_paths.models_dir, "yolo", model_name)
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
        
        #print(f"åŠ è½½YOLOæ¨¡å‹: {model_path}")
        
        try:
            self.model = YOLO(model_path)
            self.current_model_name = model_name
            #print(f"æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
            return self.model
        except Exception as e:
            raise RuntimeError(f"åŠ è½½YOLOæ¨¡å‹å¤±è´¥: {str(e)}")

    def tensor_to_pil(self, tensor):
        """å°†tensorè½¬æ¢ä¸ºPILå›¾åƒ"""
        if len(tensor.shape) == 4:
            tensor = tensor[0]
        
        # ä»CHWæˆ–HWCæ ¼å¼è½¬æ¢ä¸ºHWC
        if tensor.shape[0] == 3 or tensor.shape[0] == 1:
            tensor = tensor.permute(1, 2, 0)
        
        # è½¬æ¢ä¸ºnumpy
        img = tensor.cpu().numpy()
        
        # ç¡®ä¿å€¼åœ¨0-255èŒƒå›´å†…
        if img.max() <= 1.0:
            img = img * 255
        
        img = img.astype(np.uint8)
        
        # å¦‚æœæ˜¯å•é€šé“ï¼Œè½¬æ¢ä¸ºRGB
        if img.shape[2] == 1:
            img = np.repeat(img, 3, axis=2)
        
        return Image.fromarray(img)

    def pil_to_tensor(self, pil_img):
        """å°†PILå›¾åƒè½¬æ¢ä¸ºtensor"""
        img = np.array(pil_img).astype(np.float32) / 255.0
        tensor = torch.from_numpy(img).float()
        return tensor

    def filter_detections(self, results, class_filter):
        """æ ¹æ®ç±»åˆ«è¿‡æ»¤æ£€æµ‹ç»“æœ"""
        if not results or len(results) == 0:
            return []
        
        result = results[0]
        if not hasattr(result, 'boxes') or result.boxes is None:
            return []
        
        filtered_boxes = []
        
        # ä¸­æ–‡ç±»åˆ«åç§°æ˜ å°„
        class_name_cn = {
            'person': 'äºº',
            'bicycle': 'è‡ªè¡Œè½¦',
            'car': 'æ±½è½¦',
            'motorcycle': 'æ‘©æ‰˜è½¦',
            'airplane': 'é£æœº',
            'bus': 'å…¬äº¤è½¦',
            'train': 'ç«è½¦',
            'truck': 'å¡è½¦',
            'boat': 'èˆ¹',
            'traffic light': 'äº¤é€šç¯',
            'fire hydrant': 'æ¶ˆé˜²æ “',
            'stop sign': 'åœæ­¢æ ‡å¿—',
            'parking meter': 'åœè½¦è®¡æ—¶å™¨',
            'bench': 'é•¿æ¤…',
            'bird': 'é¸Ÿ',
            'cat': 'çŒ«',
            'dog': 'ç‹—',
            'horse': 'é©¬',
            'sheep': 'ç¾Š',
            'cow': 'ç‰›',
            'elephant': 'å¤§è±¡',
            'bear': 'ç†Š',
            'zebra': 'æ–‘é©¬',
            'giraffe': 'é•¿é¢ˆé¹¿',
            'backpack': 'èƒŒåŒ…',
            'umbrella': 'é›¨ä¼',
            'handbag': 'æ‰‹æåŒ…',
            'tie': 'é¢†å¸¦',
            'suitcase': 'æ‰‹æç®±',
            'frisbee': 'é£ç›˜',
            'skis': 'æ»‘é›ªæ¿',
            'snowboard': 'æ»‘é›ªæ¿',
            'sports ball': 'è¿åŠ¨çƒ',
            'kite': 'é£ç­',
            'baseball bat': 'æ£’çƒæ£’',
            'baseball glove': 'æ£’çƒæ‰‹å¥—',
            'skateboard': 'æ»‘æ¿',
            'surfboard': 'å†²æµªæ¿',
            'tennis racket': 'ç½‘çƒæ‹',
            'bottle': 'ç“¶å­',
            'wine glass': 'é…’æ¯',
            'cup': 'æ¯å­',
            'fork': 'å‰å­',
            'knife': 'åˆ€',
            'spoon': 'å‹ºå­',
            'bowl': 'ç¢—',
            'banana': 'é¦™è•‰',
            'apple': 'è‹¹æœ',
            'sandwich': 'ä¸‰æ˜æ²»',
            'orange': 'æ©™å­',
            'broccoli': 'è¥¿å…°èŠ±',
            'carrot': 'èƒ¡èåœ',
            'hot dog': 'çƒ­ç‹—',
            'pizza': 'æŠ«è¨',
            'donut': 'ç”œç”œåœˆ',
            'cake': 'è›‹ç³•',
            'chair': 'æ¤…å­',
            'couch': 'æ²™å‘',
            'potted plant': 'ç›†æ ½',
            'bed': 'åºŠ',
            'dining table': 'é¤æ¡Œ',
            'toilet': 'é©¬æ¡¶',
            'tv': 'ç”µè§†',
            'laptop': 'ç¬”è®°æœ¬ç”µè„‘',
            'mouse': 'é¼ æ ‡',
            'remote': 'é¥æ§å™¨',
            'keyboard': 'é”®ç›˜',
            'cell phone': 'æ‰‹æœº',
            'microwave': 'å¾®æ³¢ç‚‰',
            'oven': 'çƒ¤ç®±',
            'toaster': 'çƒ¤é¢åŒ…æœº',
            'sink': 'æ°´æ§½',
            'refrigerator': 'å†°ç®±',
            'book': 'ä¹¦',
            'clock': 'æ—¶é’Ÿ',
            'vase': 'èŠ±ç“¶',
            'scissors': 'å‰ªåˆ€',
            'teddy bear': 'æ³°è¿ªç†Š',
            'hair drier': 'å¹é£æœº',
            'toothbrush': 'ç‰™åˆ·'
        }
        
        # å¦‚æœæ£€æµ‹æ‰€æœ‰ç±»åˆ«
        if class_filter.lower() == 'all' or class_filter == 'å…¨éƒ¨':
            for box in result.boxes:
                xyxy = box.xyxy[0].cpu().numpy()
                conf = box.conf[0].cpu().numpy()
                cls = int(box.cls[0].cpu().numpy())
                
                # è·å–ç±»åˆ«åç§°
                class_name = result.names[cls] if hasattr(result, 'names') else str(cls)
                class_name_display = class_name_cn.get(class_name, class_name)
                
                filtered_boxes.append({
                    'bbox': xyxy.tolist(),
                    'confidence': float(conf),
                    'class': class_name,
                    'class_display': class_name_display,
                    'class_id': cls
                })
        else:
            # è§£æè¦æ£€æµ‹çš„ç±»åˆ«ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
            target_classes = [c.strip().lower() for c in class_filter.split(',')]
            
            # åˆ›å»ºåå‘æ˜ å°„ï¼ˆä¸­æ–‡åˆ°è‹±æ–‡ï¼‰
            class_name_en = {v: k for k, v in class_name_cn.items()}
            
            # è½¬æ¢ä¸­æ–‡ç±»åˆ«ååˆ°è‹±æ–‡
            target_classes_en = []
            for tc in target_classes:
                if tc in class_name_en:
                    target_classes_en.append(class_name_en[tc])
                else:
                    target_classes_en.append(tc)
            
            for box in result.boxes:
                cls = int(box.cls[0].cpu().numpy())
                class_name = result.names[cls] if hasattr(result, 'names') else str(cls)
                
                if class_name.lower() in target_classes_en:
                    xyxy = box.xyxy[0].cpu().numpy()
                    conf = box.conf[0].cpu().numpy()
                    class_name_display = class_name_cn.get(class_name, class_name)
                    
                    filtered_boxes.append({
                        'bbox': xyxy.tolist(),
                        'confidence': float(conf),
                        'class': class_name,
                        'class_display': class_name_display,
                        'class_id': cls
                    })
        
        return filtered_boxes

    def sort_detections(self, detections, sort_by):
        """æ ¹æ®æŒ‡å®šæ–¹å¼å¯¹æ£€æµ‹ç»“æœæ’åº"""
        if not detections or sort_by == "é»˜è®¤":
            return detections
        
        if sort_by == "ä»å·¦åˆ°å³":
            # æŒ‰ç…§è¾¹ç•Œæ¡†ä¸­å¿ƒçš„xåæ ‡æ’åº
            return sorted(detections, key=lambda d: (d['bbox'][0] + d['bbox'][2]) / 2)
        elif sort_by == "ä»å³åˆ°å·¦":
            # æŒ‰ç…§è¾¹ç•Œæ¡†ä¸­å¿ƒçš„xåæ ‡é™åºæ’åº
            return sorted(detections, key=lambda d: (d['bbox'][0] + d['bbox'][2]) / 2, reverse=True)
        elif sort_by == "ä»ä¸Šåˆ°ä¸‹":
            # æŒ‰ç…§è¾¹ç•Œæ¡†ä¸­å¿ƒçš„yåæ ‡æ’åº
            return sorted(detections, key=lambda d: (d['bbox'][1] + d['bbox'][3]) / 2)
        elif sort_by == "ä»ä¸‹åˆ°ä¸Š":
            # æŒ‰ç…§è¾¹ç•Œæ¡†ä¸­å¿ƒçš„yåæ ‡é™åºæ’åº
            return sorted(detections, key=lambda d: (d['bbox'][1] + d['bbox'][3]) / 2, reverse=True)
        elif sort_by == "ç½®ä¿¡åº¦é™åº":
            # æŒ‰ç½®ä¿¡åº¦ä»é«˜åˆ°ä½æ’åº
            return sorted(detections, key=lambda d: d['confidence'], reverse=True)
        elif sort_by == "ç½®ä¿¡åº¦å‡åº":
            # æŒ‰ç½®ä¿¡åº¦ä»ä½åˆ°é«˜æ’åº
            return sorted(detections, key=lambda d: d['confidence'])
        elif sort_by == "é¢ç§¯é™åº":
            # æŒ‰è¾¹ç•Œæ¡†é¢ç§¯ä»å¤§åˆ°å°æ’åº
            def get_area(d):
                x1, y1, x2, y2 = d['bbox']
                return (x2 - x1) * (y2 - y1)
            return sorted(detections, key=get_area, reverse=True)
        elif sort_by == "é¢ç§¯å‡åº":
            # æŒ‰è¾¹ç•Œæ¡†é¢ç§¯ä»å°åˆ°å¤§æ’åº
            def get_area(d):
                x1, y1, x2, y2 = d['bbox']
                return (x2 - x1) * (y2 - y1)
            return sorted(detections, key=get_area)
        
        return detections

    def crop_single_object(self, image, detection, expand_width, expand_height, original_height, original_width, scale_x=1.0, scale_y=1.0):
        """è£å‰ªå•ä¸ªæ£€æµ‹å¯¹è±¡"""
        height, width = image.shape[:2]
        
        # è·å–è¾¹ç•Œæ¡†åæ ‡
        x1, y1, x2, y2 = [int(coord) for coord in detection['bbox']]
        
        # åº”ç”¨æ‰©å±•
        x1_expanded = max(0, x1 - expand_width)
        y1_expanded = max(0, y1 - expand_height)
        x2_expanded = min(width, x2 + expand_width)
        y2_expanded = min(height, y2 + expand_height)
        
        # è£å‰ªå›¾åƒ
        cropped = image[y1_expanded:y2_expanded, x1_expanded:x2_expanded]
        
        # åˆ›å»ºåŸå›¾å°ºå¯¸çš„é®ç½©ï¼Œè€ƒè™‘å¯èƒ½çš„ç¼©æ”¾
        mask = np.zeros((original_height, original_width), dtype=np.float32)
        
        # è®¡ç®—åœ¨åŸå§‹å°ºå¯¸ä¸Šçš„åæ ‡
        mask_x1 = int(x1_expanded * scale_x)
        mask_y1 = int(y1_expanded * scale_y)
        mask_x2 = int(x2_expanded * scale_x)
        mask_y2 = int(y2_expanded * scale_y)
        
        # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
        mask_x1 = max(0, min(mask_x1, original_width))
        mask_y1 = max(0, min(mask_y1, original_height))
        mask_x2 = max(mask_x1, min(mask_x2, original_width))
        mask_y2 = max(mask_y1, min(mask_y2, original_height))
        
        if mask_x2 > mask_x1 and mask_y2 > mask_y1:
            mask[mask_y1:mask_y2, mask_x1:mask_x2] = 1.0
        
        return cropped, mask, (x1_expanded, y1_expanded, x2_expanded, y2_expanded)

    def detect_and_crop(self, image, model_name, confidence, class_filter, 
                       expand_width, expand_height, sort_by, crop_mode, object_index):
        """æ‰§è¡Œæ£€æµ‹å’Œè£å‰ª"""
        # ç¡®ä¿å›¾åƒæ˜¯4ç»´çš„
        if len(image.shape) == 3:
            image = image.unsqueeze(0)
        
        # ä¿å­˜åŸå§‹å›¾åƒå°ºå¯¸ï¼ˆä»tensorè·å–ï¼‰
        batch_size, tensor_height, tensor_width, channels = image.shape
        
        # åŠ è½½æ¨¡å‹
        try:
            model = self.load_model(model_name)
        except Exception as e:
            #print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # è¿”å›åŸå›¾å’Œç©ºé®ç½©
            empty_mask = torch.zeros((1, tensor_height, tensor_width), dtype=torch.float32)
            empty_bboxes = []  # æ”¹ä¸ºè¿”å›ç©ºåˆ—è¡¨
            return ([image], empty_mask, empty_bboxes, "æ¨¡å‹åŠ è½½å¤±è´¥", 0)
        
        # è½¬æ¢ä¸ºPILå›¾åƒè¿›è¡Œæ£€æµ‹
        pil_img = self.tensor_to_pil(image)
        
        # æ‰§è¡Œæ£€æµ‹
        #print(f"æ‰§è¡ŒYOLOæ£€æµ‹ï¼Œç½®ä¿¡åº¦é˜ˆå€¼: {confidence}")
        results = model(pil_img, conf=confidence, verbose=False)
        
        # è¿‡æ»¤æ£€æµ‹ç»“æœ
        detections = self.filter_detections(results, class_filter)
        
        # å¯¹æ£€æµ‹ç»“æœè¿›è¡Œæ’åº
        detections = self.sort_detections(detections, sort_by)
        
        if not detections:
            #print("æœªæ£€æµ‹åˆ°ä»»ä½•å¯¹è±¡")
            # è¿”å›åŸå›¾å’ŒåŸå›¾å¤§å°çš„ç©ºé®ç½©
            empty_mask = torch.zeros((1, tensor_height, tensor_width), dtype=torch.float32)
            empty_bboxes = []  # æ”¹ä¸ºè¿”å›ç©ºåˆ—è¡¨
            return ([image], empty_mask, empty_bboxes, "æœªæ£€æµ‹åˆ°å¯¹è±¡", 0)
        
        #print(f"æ£€æµ‹åˆ° {len(detections)} ä¸ªå¯¹è±¡ï¼Œæ’åºæ–¹å¼: {sort_by}")
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„è¿›è¡Œå¤„ç†
        img_np = np.array(pil_img)
        
        # ä½¿ç”¨tensorçš„åŸå§‹å°ºå¯¸ä½œä¸ºé®ç½©å°ºå¯¸
        original_height, original_width = tensor_height, tensor_width
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼ˆå¤„ç†PILå›¾åƒå’ŒåŸå§‹tensorå°ºå¯¸ä¸ä¸€è‡´çš„æƒ…å†µï¼‰
        pil_height, pil_width = img_np.shape[:2]
        scale_x = original_width / pil_width
        scale_y = original_height / pil_height
        
        # æ ¹æ®è£å‰ªæ¨¡å¼é€‰æ‹©è¦å¤„ç†çš„å¯¹è±¡
        if crop_mode == "å•ä¸ªå¯¹è±¡":
            if object_index >= len(detections):
                #print(f"è­¦å‘Š: å¯¹è±¡ç´¢å¼• {object_index} è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªå¯¹è±¡")
                object_index = len(detections) - 1
            selected_detections = [detections[object_index]]
        elif crop_mode == "æŒ‰ç±»åˆ«":
            # æŒ‰ç±»åˆ«åˆ†ç»„ï¼Œæ¯ä¸ªç±»åˆ«é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„
            class_dict = {}
            for det in detections:
                cls = det['class']
                if cls not in class_dict or det['confidence'] > class_dict[cls]['confidence']:
                    class_dict[cls] = det
            selected_detections = list(class_dict.values())
        else:  # å…¨éƒ¨å¯¹è±¡
            selected_detections = detections
        
        # è£å‰ªé€‰ä¸­çš„å¯¹è±¡
        cropped_images = []
        bboxes = []
        detection_info = []
        
        # åˆ›å»ºä¸€ä¸ªä¸åŸå›¾å°ºå¯¸ç›¸åŒçš„åˆå¹¶é®ç½©
        combined_mask = torch.zeros((original_height, original_width), dtype=torch.float32)
        
        for i, det in enumerate(selected_detections):
            cropped, mask, bbox = self.crop_single_object(
                img_np, det, expand_width, expand_height, 
                original_height, original_width, scale_x, scale_y
            )
            
            # è½¬æ¢ä¸ºtensor
            cropped_tensor = self.pil_to_tensor(Image.fromarray(cropped))
            mask_tensor = torch.from_numpy(mask).float()
            
            # ä¸ºè£å‰ªçš„å›¾åƒæ·»åŠ æ‰¹æ¬¡ç»´åº¦
            cropped_images.append(cropped_tensor.unsqueeze(0))
            
            # åˆå¹¶é®ç½©ï¼ˆä½¿ç”¨æœ€å¤§å€¼ä¿ç•™æ‰€æœ‰æ£€æµ‹åŒºåŸŸï¼‰
            combined_mask = torch.maximum(combined_mask, mask_tensor)
            
            bboxes.append(bbox)
            
            # è®°å½•æ£€æµ‹ä¿¡æ¯ï¼ˆæ˜¾ç¤ºæ’åºåçš„ç´¢å¼•å’Œä½ç½®ï¼‰
            center_x = int((det['bbox'][0] + det['bbox'][2]) / 2)
            center_y = int((det['bbox'][1] + det['bbox'][3]) / 2)
            info = f"[{i}] {det['class_display']} (ç½®ä¿¡åº¦: {det['confidence']:.2f}, ä¸­å¿ƒ: x={center_x},y={center_y})"
            detection_info.append(info)
        
        # ä¸ºåˆå¹¶é®ç½©æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        # MASKæ ¼å¼åº”è¯¥æ˜¯ (batch, height, width)ï¼Œä¸éœ€è¦é€šé“ç»´åº¦
        final_mask = combined_mask.unsqueeze(0)
        
        # ç¡®ä¿é®ç½©æ•°æ®ç±»å‹å’ŒèŒƒå›´æ­£ç¡®
        final_mask = final_mask.clamp(0.0, 1.0)
        
        # ä¿®æ”¹è¿™é‡Œï¼šç¡®ä¿è¾¹ç•Œæ¡†æ˜¯æ‰å¹³çš„åˆ—è¡¨æ ¼å¼
        # æ¯ä¸ªè¾¹ç•Œæ¡†åº”è¯¥æ˜¯ [x1, y1, x2, y2] çš„åˆ—è¡¨
        final_bboxes = []
        for bbox in bboxes:
            if isinstance(bbox, (list, tuple)) and len(bbox) == 4:
                # ç¡®ä¿æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œä¸æ˜¯å…ƒç»„
                final_bboxes.append(list(bbox))
            else:
                print(f"è­¦å‘Šï¼šè·³è¿‡æ— æ•ˆçš„è¾¹ç•Œæ¡†æ ¼å¼: {bbox}")
        
        # ç”Ÿæˆæ£€æµ‹ä¿¡æ¯å­—ç¬¦ä¸²
        info_str = f"æ£€æµ‹åˆ° {len(detections)} ä¸ªå¯¹è±¡ï¼Œè£å‰ªäº† {len(selected_detections)} ä¸ª\n"
        info_str += "\n".join(detection_info)
        
        #print(f"è¾“å‡ºè¾¹ç•Œæ¡†æ•°é‡: {len(final_bboxes)}")
        #print(f"è¾¹ç•Œæ¡†åˆ—è¡¨: {final_bboxes}")
        
        # è¿”å›è£å‰ªçš„å›¾åƒåˆ—è¡¨å’Œåˆå¹¶çš„é®ç½©
        return (cropped_images, final_mask, final_bboxes, info_str, len(selected_detections))

# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "YoloBboxesCropNode": YoloBboxesCropNode
}

# èŠ‚ç‚¹æ˜¾ç¤ºåç§°æ˜ å°„
NODE_DISPLAY_NAME_MAPPINGS = {
    "YoloBboxesCropNode": "ğŸ³YOLOæ£€æµ‹è£å‰ª"
}