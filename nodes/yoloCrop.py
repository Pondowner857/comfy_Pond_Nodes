import torch
import numpy as np
import os
import sys
from PIL import Image
import folder_paths

# å°è¯•å¯¼å…¥ultralytics
try:
    from ultralytics import YOLO
except ImportError:
    print("è­¦å‘Š: æœªå®‰è£…ultralyticsåº“ï¼Œè¯·è¿è¡Œ: pip install ultralytics")
    YOLO = None

class YoloBboxesCropNode:
    """
    é›†æˆYOLOæ£€æµ‹å’Œè£å‰ªåŠŸèƒ½çš„èŠ‚ç‚¹
    è‡ªåŠ¨åŠ è½½YOLOæ¨¡å‹ï¼Œæ£€æµ‹å¹¶è£å‰ªå›¾åƒä¸­çš„å¯¹è±¡
    """
    
    def __init__(self):
        self.model = None
        self.current_model_name = None
        
    @classmethod
    def INPUT_TYPES(cls):
        # è·å–YOLOæ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„
        yolo_models_dir = os.path.join(folder_paths.models_dir, "yolo")
        
        # è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        model_files = []
        if os.path.exists(yolo_models_dir):
            for file in os.listdir(yolo_models_dir):
                if file.endswith(('.pt', '.onnx', '.engine')):
                    model_files.append(file)
        
        if not model_files:
            model_files = ["è¯·å°†YOLOæ¨¡å‹æ”¾å…¥models/yoloæ–‡ä»¶å¤¹"]
        
        return {
            "required": {
                "image": ("IMAGE", {"display": "è¾“å…¥å›¾åƒ"}),
                "model_name": (model_files, {
                    "default": model_files[0] if model_files else "yolov8n.pt",
                    "display": "YOLOæ¨¡å‹"
                }),
                "confidence": ("FLOAT", {
                    "default": 0.25,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "display": "ç½®ä¿¡åº¦é˜ˆå€¼"
                }),
                "class_filter": ("STRING", {
                    "default": "å…¨éƒ¨",
                    "multiline": False,
                    "display": "ç±»åˆ«è¿‡æ»¤",
                    "tooltip": "è¦æ£€æµ‹çš„ç±»åˆ«ï¼Œç”¨é€—å·åˆ†éš”ï¼ˆå¦‚ï¼šäºº,æ±½è½¦ï¼‰æˆ–'å…¨éƒ¨'æ£€æµ‹æ‰€æœ‰"
                }),
                "expand_width": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 500,
                    "step": 10,
                    "display": "å®½åº¦æ‰©å±•"
                }),
                "expand_height": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 500,
                    "step": 10,
                    "display": "é«˜åº¦æ‰©å±•"
                }),
                "crop_mode": (["å…¨éƒ¨å¯¹è±¡", "å•ä¸ªå¯¹è±¡", "æŒ‰ç±»åˆ«"], {
                    "default": "å…¨éƒ¨å¯¹è±¡",
                    "display": "è£å‰ªæ¨¡å¼"
                }),
                "object_index": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                    "display": "å¯¹è±¡ç´¢å¼•"
                }),
                "output_mode": (["å•ç‹¬å›¾åƒ", "æ‹¼æ¥å›¾åƒ"], {
                    "default": "å•ç‹¬å›¾åƒ",
                    "display": "è¾“å‡ºæ¨¡å¼"
                })
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK", "BBOXES", "STRING", "INT")
    RETURN_NAMES = ("è£å‰ªå›¾åƒ", "é®ç½©", "è¾¹ç•Œæ¡†", "æ£€æµ‹ä¿¡æ¯", "æ£€æµ‹æ•°é‡")
    FUNCTION = "detect_and_crop"
    CATEGORY = "ğŸ³Pond/yolo"
    DESCRIPTION = "ä½¿ç”¨YOLOæ¨¡å‹æ£€æµ‹å›¾åƒä¸­çš„å¯¹è±¡å¹¶è¿›è¡Œæ™ºèƒ½è£å‰ª"

    def load_model(self, model_name):
        """åŠ è½½YOLOæ¨¡å‹"""
        if YOLO is None:
            raise ImportError("è¯·å®‰è£…ultralyticsåº“: pip install ultralytics")
        
        # å¦‚æœå·²ç»åŠ è½½äº†ç›¸åŒçš„æ¨¡å‹ï¼Œç›´æ¥è¿”å›
        if self.model is not None and self.current_model_name == model_name:
            return self.model
        
        # æ„å»ºæ¨¡å‹è·¯å¾„
        model_path = os.path.join(folder_paths.models_dir, "yolo", model_name)
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
        
        print(f"åŠ è½½YOLOæ¨¡å‹: {model_path}")
        
        try:
            self.model = YOLO(model_path)
            self.current_model_name = model_name
            print(f"æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
            return self.model
        except Exception as e:
            raise RuntimeError(f"åŠ è½½YOLOæ¨¡å‹å¤±è´¥: {str(e)}")

    def tensor_to_pil(self, tensor):
        """å°†tensorè½¬æ¢ä¸ºPILå›¾åƒ"""
        if len(tensor.shape) == 4:
            tensor = tensor[0]
        
        # ä»CHWæˆ–HWCæ ¼å¼è½¬æ¢ä¸ºHWC
        if tensor.shape[0] == 3 or tensor.shape[0] == 1:
            tensor = tensor.permute(1, 2, 0)
        
        # è½¬æ¢ä¸ºnumpy
        img = tensor.cpu().numpy()
        
        # ç¡®ä¿å€¼åœ¨0-255èŒƒå›´å†…
        if img.max() <= 1.0:
            img = img * 255
        
        img = img.astype(np.uint8)
        
        # å¦‚æœæ˜¯å•é€šé“ï¼Œè½¬æ¢ä¸ºRGB
        if img.shape[2] == 1:
            img = np.repeat(img, 3, axis=2)
        
        return Image.fromarray(img)

    def pil_to_tensor(self, pil_img):
        """å°†PILå›¾åƒè½¬æ¢ä¸ºtensor"""
        img = np.array(pil_img).astype(np.float32) / 255.0
        tensor = torch.from_numpy(img).float()
        return tensor

    def filter_detections(self, results, class_filter):
        """æ ¹æ®ç±»åˆ«è¿‡æ»¤æ£€æµ‹ç»“æœ"""
        if not results or len(results) == 0:
            return []
        
        result = results[0]
        if not hasattr(result, 'boxes') or result.boxes is None:
            return []
        
        filtered_boxes = []
        
        # ä¸­æ–‡ç±»åˆ«åç§°æ˜ å°„
        class_name_cn = {
            'person': 'äºº',
            'bicycle': 'è‡ªè¡Œè½¦',
            'car': 'æ±½è½¦',
            'motorcycle': 'æ‘©æ‰˜è½¦',
            'airplane': 'é£æœº',
            'bus': 'å…¬äº¤è½¦',
            'train': 'ç«è½¦',
            'truck': 'å¡è½¦',
            'boat': 'èˆ¹',
            'traffic light': 'äº¤é€šç¯',
            'fire hydrant': 'æ¶ˆé˜²æ “',
            'stop sign': 'åœæ­¢æ ‡å¿—',
            'parking meter': 'åœè½¦è®¡æ—¶å™¨',
            'bench': 'é•¿æ¤…',
            'bird': 'é¸Ÿ',
            'cat': 'çŒ«',
            'dog': 'ç‹—',
            'horse': 'é©¬',
            'sheep': 'ç¾Š',
            'cow': 'ç‰›',
            'elephant': 'å¤§è±¡',
            'bear': 'ç†Š',
            'zebra': 'æ–‘é©¬',
            'giraffe': 'é•¿é¢ˆé¹¿',
            'backpack': 'èƒŒåŒ…',
            'umbrella': 'é›¨ä¼',
            'handbag': 'æ‰‹æåŒ…',
            'tie': 'é¢†å¸¦',
            'suitcase': 'æ‰‹æç®±',
            'frisbee': 'é£ç›˜',
            'skis': 'æ»‘é›ªæ¿',
            'snowboard': 'æ»‘é›ªæ¿',
            'sports ball': 'è¿åŠ¨çƒ',
            'kite': 'é£ç­',
            'baseball bat': 'æ£’çƒæ£’',
            'baseball glove': 'æ£’çƒæ‰‹å¥—',
            'skateboard': 'æ»‘æ¿',
            'surfboard': 'å†²æµªæ¿',
            'tennis racket': 'ç½‘çƒæ‹',
            'bottle': 'ç“¶å­',
            'wine glass': 'é…’æ¯',
            'cup': 'æ¯å­',
            'fork': 'å‰å­',
            'knife': 'åˆ€',
            'spoon': 'å‹ºå­',
            'bowl': 'ç¢—',
            'banana': 'é¦™è•‰',
            'apple': 'è‹¹æœ',
            'sandwich': 'ä¸‰æ˜æ²»',
            'orange': 'æ©™å­',
            'broccoli': 'è¥¿å…°èŠ±',
            'carrot': 'èƒ¡èåœ',
            'hot dog': 'çƒ­ç‹—',
            'pizza': 'æŠ«è¨',
            'donut': 'ç”œç”œåœˆ',
            'cake': 'è›‹ç³•',
            'chair': 'æ¤…å­',
            'couch': 'æ²™å‘',
            'potted plant': 'ç›†æ ½',
            'bed': 'åºŠ',
            'dining table': 'é¤æ¡Œ',
            'toilet': 'é©¬æ¡¶',
            'tv': 'ç”µè§†',
            'laptop': 'ç¬”è®°æœ¬ç”µè„‘',
            'mouse': 'é¼ æ ‡',
            'remote': 'é¥æ§å™¨',
            'keyboard': 'é”®ç›˜',
            'cell phone': 'æ‰‹æœº',
            'microwave': 'å¾®æ³¢ç‚‰',
            'oven': 'çƒ¤ç®±',
            'toaster': 'çƒ¤é¢åŒ…æœº',
            'sink': 'æ°´æ§½',
            'refrigerator': 'å†°ç®±',
            'book': 'ä¹¦',
            'clock': 'æ—¶é’Ÿ',
            'vase': 'èŠ±ç“¶',
            'scissors': 'å‰ªåˆ€',
            'teddy bear': 'æ³°è¿ªç†Š',
            'hair drier': 'å¹é£æœº',
            'toothbrush': 'ç‰™åˆ·'
        }
        
        # å¦‚æœæ£€æµ‹æ‰€æœ‰ç±»åˆ«
        if class_filter.lower() == 'all' or class_filter == 'å…¨éƒ¨':
            for box in result.boxes:
                xyxy = box.xyxy[0].cpu().numpy()
                conf = box.conf[0].cpu().numpy()
                cls = int(box.cls[0].cpu().numpy())
                
                # è·å–ç±»åˆ«åç§°
                class_name = result.names[cls] if hasattr(result, 'names') else str(cls)
                class_name_display = class_name_cn.get(class_name, class_name)
                
                filtered_boxes.append({
                    'bbox': xyxy.tolist(),
                    'confidence': float(conf),
                    'class': class_name,
                    'class_display': class_name_display,
                    'class_id': cls
                })
        else:
            # è§£æè¦æ£€æµ‹çš„ç±»åˆ«ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
            target_classes = [c.strip().lower() for c in class_filter.split(',')]
            
            # åˆ›å»ºåå‘æ˜ å°„ï¼ˆä¸­æ–‡åˆ°è‹±æ–‡ï¼‰
            class_name_en = {v: k for k, v in class_name_cn.items()}
            
            # è½¬æ¢ä¸­æ–‡ç±»åˆ«ååˆ°è‹±æ–‡
            target_classes_en = []
            for tc in target_classes:
                if tc in class_name_en:
                    target_classes_en.append(class_name_en[tc])
                else:
                    target_classes_en.append(tc)
            
            for box in result.boxes:
                cls = int(box.cls[0].cpu().numpy())
                class_name = result.names[cls] if hasattr(result, 'names') else str(cls)
                
                if class_name.lower() in target_classes_en:
                    xyxy = box.xyxy[0].cpu().numpy()
                    conf = box.conf[0].cpu().numpy()
                    class_name_display = class_name_cn.get(class_name, class_name)
                    
                    filtered_boxes.append({
                        'bbox': xyxy.tolist(),
                        'confidence': float(conf),
                        'class': class_name,
                        'class_display': class_name_display,
                        'class_id': cls
                    })
        
        return filtered_boxes

    def crop_single_object(self, image, detection, expand_width, expand_height):
        """è£å‰ªå•ä¸ªæ£€æµ‹å¯¹è±¡"""
        height, width = image.shape[:2]
        
        # è·å–è¾¹ç•Œæ¡†åæ ‡
        x1, y1, x2, y2 = [int(coord) for coord in detection['bbox']]
        
        # åº”ç”¨æ‰©å±•
        x1_expanded = max(0, x1 - expand_width)
        y1_expanded = max(0, y1 - expand_height)
        x2_expanded = min(width, x2 + expand_width)
        y2_expanded = min(height, y2 + expand_height)
        
        # è£å‰ªå›¾åƒ
        cropped = image[y1_expanded:y2_expanded, x1_expanded:x2_expanded]
        
        # åˆ›å»ºé®ç½©
        mask = np.zeros((height, width), dtype=np.float32)
        mask[y1_expanded:y2_expanded, x1_expanded:x2_expanded] = 1.0
        
        return cropped, mask, (x1_expanded, y1_expanded, x2_expanded, y2_expanded)

    def detect_and_crop(self, image, model_name, confidence, class_filter, 
                       expand_width, expand_height, crop_mode, object_index, output_mode):
        """æ‰§è¡Œæ£€æµ‹å’Œè£å‰ª"""
        # ç¡®ä¿å›¾åƒæ˜¯4ç»´çš„
        if len(image.shape) == 3:
            image = image.unsqueeze(0)
        
        # åŠ è½½æ¨¡å‹
        try:
            model = self.load_model(model_name)
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # è¿”å›åŸå›¾å’Œç©ºé®ç½©
            empty_mask = torch.zeros((1, image.shape[1], image.shape[2], 1), dtype=torch.float32)
            empty_bboxes = torch.zeros((0, 4), dtype=torch.float32)
            return (image, empty_mask, empty_bboxes, "æ¨¡å‹åŠ è½½å¤±è´¥", 0)
        
        # è½¬æ¢ä¸ºPILå›¾åƒè¿›è¡Œæ£€æµ‹
        pil_img = self.tensor_to_pil(image)
        
        # æ‰§è¡Œæ£€æµ‹
        print(f"æ‰§è¡ŒYOLOæ£€æµ‹ï¼Œç½®ä¿¡åº¦é˜ˆå€¼: {confidence}")
        results = model(pil_img, conf=confidence, verbose=False)
        
        # è¿‡æ»¤æ£€æµ‹ç»“æœ
        detections = self.filter_detections(results, class_filter)
        
        if not detections:
            print("æœªæ£€æµ‹åˆ°ä»»ä½•å¯¹è±¡")
            empty_mask = torch.zeros((1, image.shape[1], image.shape[2], 1), dtype=torch.float32)
            empty_bboxes = torch.zeros((0, 4), dtype=torch.float32)
            return (image, empty_mask, empty_bboxes, "æœªæ£€æµ‹åˆ°å¯¹è±¡", 0)
        
        print(f"æ£€æµ‹åˆ° {len(detections)} ä¸ªå¯¹è±¡")
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„è¿›è¡Œå¤„ç†
        img_np = np.array(pil_img)
        
        # æ ¹æ®è£å‰ªæ¨¡å¼é€‰æ‹©è¦å¤„ç†çš„å¯¹è±¡
        if crop_mode == "å•ä¸ªå¯¹è±¡":
            if object_index >= len(detections):
                print(f"è­¦å‘Š: å¯¹è±¡ç´¢å¼• {object_index} è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªå¯¹è±¡")
                object_index = len(detections) - 1
            selected_detections = [detections[object_index]]
        elif crop_mode == "æŒ‰ç±»åˆ«":
            # æŒ‰ç±»åˆ«åˆ†ç»„ï¼Œæ¯ä¸ªç±»åˆ«é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„
            class_dict = {}
            for det in detections:
                cls = det['class']
                if cls not in class_dict or det['confidence'] > class_dict[cls]['confidence']:
                    class_dict[cls] = det
            selected_detections = list(class_dict.values())
        else:  # å…¨éƒ¨å¯¹è±¡
            selected_detections = detections
        
        # è£å‰ªé€‰ä¸­çš„å¯¹è±¡
        cropped_images = []
        masks = []
        bboxes = []
        detection_info = []
        
        for i, det in enumerate(selected_detections):
            cropped, mask, bbox = self.crop_single_object(img_np, det, expand_width, expand_height)
            
            # è½¬æ¢ä¸ºtensor
            cropped_tensor = self.pil_to_tensor(Image.fromarray(cropped))
            mask_tensor = torch.from_numpy(mask).float()
            
            cropped_images.append(cropped_tensor.unsqueeze(0))
            masks.append(mask_tensor.unsqueeze(0).unsqueeze(-1))
            bboxes.append(bbox)
            
            # è®°å½•æ£€æµ‹ä¿¡æ¯
            info = f"[{i}] {det['class_display']} (ç½®ä¿¡åº¦: {det['confidence']:.2f})"
            detection_info.append(info)
        
        # åˆå¹¶ç»“æœ
        if output_mode == "æ‹¼æ¥å›¾åƒ" and len(cropped_images) > 1:
            # å°†æ‰€æœ‰è£å‰ªå›¾åƒæ‹¼æ¥æˆæ‰¹æ¬¡
            final_images = torch.cat(cropped_images, dim=0)
        else:
            # å•ç‹¬è¾“å‡ºæ¯ä¸ªå›¾åƒ
            final_images = torch.cat(cropped_images, dim=0)
        
        # åˆå¹¶é®ç½©ï¼ˆä½¿ç”¨æœ€å¤§å€¼ä¿ç•™é‡å åŒºåŸŸï¼‰
        final_mask = masks[0]
        for mask in masks[1:]:
            final_mask = torch.maximum(final_mask, mask)
        
        # è½¬æ¢è¾¹ç•Œæ¡†ä¸ºtensor
        final_bboxes = torch.tensor(bboxes, dtype=torch.float32)
        
        # ç”Ÿæˆæ£€æµ‹ä¿¡æ¯å­—ç¬¦ä¸²
        info_str = f"æ£€æµ‹åˆ° {len(detections)} ä¸ªå¯¹è±¡ï¼Œè£å‰ªäº† {len(selected_detections)} ä¸ª\n"
        info_str += "\n".join(detection_info)
        
        return (final_images, final_mask, final_bboxes, info_str, len(selected_detections))

# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "YoloBboxesCropNode": YoloBboxesCropNode
}

# èŠ‚ç‚¹æ˜¾ç¤ºåç§°æ˜ å°„
NODE_DISPLAY_NAME_MAPPINGS = {
    "YoloBboxesCropNode": "ğŸ³YOLOæ£€æµ‹è£å‰ª"
}