import torch
import numpy as np
import os
import sys
from PIL import Image
import folder_paths

# å°è¯•å¯¼å…¥ultralytics
try:
    from ultralytics import YOLO
except ImportError:
    print("è­¦å‘Š: æœªå®‰è£…ultralyticsåº“ï¼Œè¯·è¿è¡Œ: pip install ultralytics")
    YOLO = None

class YoloBboxesCropNode:

    def __init__(self):
        self.model = None
        self.current_model_name = None
        
    @classmethod
    def INPUT_TYPES(cls):
        # è·å–YOLOæ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„
        yolo_models_dir = os.path.join(folder_paths.models_dir, "yolo")
        
        # è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        model_files = []
        if os.path.exists(yolo_models_dir):
            for file in os.listdir(yolo_models_dir):
                if file.endswith(('.pt', '.onnx', '.engine')):
                    model_files.append(file)
        
        if not model_files:
            model_files = ["è¯·å°†YOLOæ¨¡å‹æ”¾å…¥models/yoloæ–‡ä»¶å¤¹"]
        
        return {
            "required": {
                "image": ("IMAGE", {"display": "è¾“å…¥å›¾åƒ"}),
                "model_name": (model_files, {
                    "default": model_files[0] if model_files else "yolov8n.pt",
                    "display": "YOLOæ¨¡å‹"
                }),
                "confidence": ("FLOAT", {
                    "default": 0.25,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.05,
                    "display": "ç½®ä¿¡åº¦é˜ˆå€¼"
                }),
                "class_filter": ("STRING", {
                    "default": "å…¨éƒ¨",
                    "multiline": False,
                    "display": "ç±»åˆ«è¿‡æ»¤",
                    "tooltip": "è¦æ£€æµ‹çš„ç±»åˆ«ï¼Œç”¨é€—å·åˆ†éš”ï¼ˆå¦‚ï¼šäºº,æ±½è½¦ï¼‰æˆ–'å…¨éƒ¨'æ£€æµ‹æ‰€æœ‰"
                }),
                "square_size": ("FLOAT", {
                    "default": 100.0,
                    "min": 10.0,
                    "max": 200.0,
                    "step": 1.0,
                    "display": "æ–¹æ¡†å¤§å°%",
                    "tooltip": "åŸºäºæ£€æµ‹å¯¹è±¡å¤§å°çš„ç™¾åˆ†æ¯”è°ƒæ•´"
                }),
                "object_margin": ("FLOAT", {
                    "default": 1.5,
                    "min": 1.0,
                    "max": 3.0,
                    "step": 0.1,
                    "display": "å¯¹è±¡è¾¹è·ç³»æ•°",
                    "tooltip": "åœ¨æ£€æµ‹å¯¹è±¡å‘¨å›´æ·»åŠ çš„é¢å¤–è¾¹è·ç³»æ•°"
                }),
                "vertical_offset": ("FLOAT", {
                    "default": 0.0,
                    "min": -50.0,
                    "max": 50.0,
                    "step": 1.0,
                    "display": "å‚ç›´åç§»%"
                }),
                "horizontal_offset": ("FLOAT", {
                    "default": 0.0,
                    "min": -50.0,
                    "max": 50.0,
                    "step": 1.0,
                    "display": "æ°´å¹³åç§»%"
                }),
                "sort_by": (["é»˜è®¤", "ä»å·¦åˆ°å³", "ä»å³åˆ°å·¦", "ä»ä¸Šåˆ°ä¸‹", "ä»ä¸‹åˆ°ä¸Š", "ç½®ä¿¡åº¦é™åº", "ç½®ä¿¡åº¦å‡åº", "é¢ç§¯é™åº", "é¢ç§¯å‡åº"], {
                    "default": "ä»å·¦åˆ°å³",
                    "display": "æ’åºæ–¹å¼"
                }),
                "crop_mode": (["å…¨éƒ¨å¯¹è±¡", "å•ä¸ªå¯¹è±¡", "æŒ‰ç±»åˆ«"], {
                    "default": "å…¨éƒ¨å¯¹è±¡",
                    "display": "è£å‰ªæ¨¡å¼"
                }),
                "object_index": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                    "display": "å¯¹è±¡ç´¢å¼•"
                })
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK", "BBOXES", "STRING", "INT")
    RETURN_NAMES = ("è£å‰ªå›¾åƒ", "é®ç½©", "è¾¹ç•Œæ¡†", "æ£€æµ‹ä¿¡æ¯", "æ£€æµ‹æ•°é‡")
    OUTPUT_IS_LIST = (True, False, False, False, False)  # åªæœ‰å›¾åƒè¾“å‡ºä¸ºåˆ—è¡¨
    FUNCTION = "detect_and_crop"
    CATEGORY = "ğŸ³Pond/yolo"
    DESCRIPTION = "ä½¿ç”¨YOLOæ¨¡å‹æ£€æµ‹å›¾åƒä¸­çš„å¯¹è±¡å¹¶è¿›è¡Œæ™ºèƒ½è£å‰ªï¼Œæ ¹æ®å¯¹è±¡å¤§å°è‡ªåŠ¨è°ƒæ•´è£å‰ªæ¡†ï¼Œæ”¯æŒå¤šç§æ’åºæ–¹å¼"

    def load_model(self, model_name):
        """åŠ è½½YOLOæ¨¡å‹"""
        if YOLO is None:
            raise ImportError("è¯·å®‰è£…ultralyticsåº“: pip install ultralytics")
        
        # å¦‚æœå·²ç»åŠ è½½äº†ç›¸åŒçš„æ¨¡å‹ï¼Œç›´æ¥è¿”å›
        if self.model is not None and self.current_model_name == model_name:
            return self.model
        
        # æ„å»ºæ¨¡å‹è·¯å¾„
        model_path = os.path.join(folder_paths.models_dir, "yolo", model_name)
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
        
        try:
            # å¯¹äºPyTorch 2.6+ï¼Œéœ€è¦æ·»åŠ å®‰å…¨å…¨å±€å˜é‡
            if hasattr(torch.serialization, 'add_safe_globals'):
                safe_list = []
                
                # æ·»åŠ PyTorchåŸºç¡€æ¨¡å—
                try:
                    safe_list.extend([
                        torch.nn.modules.container.Sequential,
                        torch.nn.modules.container.ModuleList,
                        torch.nn.modules.container.ModuleDict,
                    ])
                except:
                    pass
                
                # æ·»åŠ YOLOç›¸å…³æ¨¡å—
                try:
                    from ultralytics.nn.tasks import DetectionModel
                    from ultralytics.nn.modules import Conv, C2f, SPPF, Detect
                    safe_list.extend([DetectionModel, Conv, C2f, SPPF, Detect])
                except ImportError:
                    pass
                
                # æ·»åŠ æ›´å¤šå¯èƒ½éœ€è¦çš„æ¨¡å—
                try:
                    safe_list.extend([
                        torch.nn.Conv2d,
                        torch.nn.BatchNorm2d,
                        torch.nn.SiLU,
                        torch.nn.Upsample,
                        torch.nn.MaxPool2d,
                    ])
                except:
                    pass
                
                if safe_list:
                    torch.serialization.add_safe_globals(safe_list)
            
            # å¼ºåˆ¶è®¾ç½®weights_only=Falseçš„å¦ä¸€ç§æ–¹æ³•
            import os as os_module
            os_module.environ['YOLO_AUTOINSTALL'] = 'False'
            
            # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸´æ—¶ä¿®æ”¹torch.load
            class LoadContext:
                def __enter__(self):
                    self.original_load = torch.load
                    torch.load = lambda *args, **kwargs: self.original_load(
                        *args, **{k: v for k, v in kwargs.items() if k != 'weights_only'}, 
                        weights_only=False
                    )
                    return self
                
                def __exit__(self, *args):
                    torch.load = self.original_load
            
            with LoadContext():
                self.model = YOLO(model_path)
                
            self.current_model_name = model_name
            return self.model
            
        except Exception as e:
            print(f"åŠ è½½å¤±è´¥: {e}")
            # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨monkey patch
            try:
                import ultralytics.engine.model
                original_torch_load = torch.load
                
                def patched_load(*args, **kwargs):
                    kwargs['weights_only'] = False
                    return original_torch_load(*args, **kwargs)
                
                torch.load = patched_load
                self.model = YOLO(model_path)
                torch.load = original_torch_load
                
                self.current_model_name = model_name
                return self.model
            except Exception as e2:
                raise RuntimeError(f"åŠ è½½YOLOæ¨¡å‹å¤±è´¥: {str(e2)}")

    def tensor_to_pil(self, tensor):
        """å°†tensorè½¬æ¢ä¸ºPILå›¾åƒ"""
        if len(tensor.shape) == 4:
            tensor = tensor[0]
        
        # ä»CHWæˆ–HWCæ ¼å¼è½¬æ¢ä¸ºHWC
        if tensor.shape[0] == 3 or tensor.shape[0] == 1:
            tensor = tensor.permute(1, 2, 0)
        
        # è½¬æ¢ä¸ºnumpy
        img = tensor.cpu().numpy()
        
        # ç¡®ä¿å€¼åœ¨0-255èŒƒå›´å†…
        if img.max() <= 1.0:
            img = img * 255
        
        img = img.astype(np.uint8)
        
        # å¦‚æœæ˜¯å•é€šé“ï¼Œè½¬æ¢ä¸ºRGB
        if img.shape[2] == 1:
            img = np.repeat(img, 3, axis=2)
        
        return Image.fromarray(img)

    def pil_to_tensor(self, pil_img):
        """å°†PILå›¾åƒè½¬æ¢ä¸ºtensor"""
        img = np.array(pil_img).astype(np.float32) / 255.0
        tensor = torch.from_numpy(img).float()
        return tensor

    def filter_detections(self, results, class_filter):
        """æ ¹æ®ç±»åˆ«è¿‡æ»¤æ£€æµ‹ç»“æœ"""
        if not results or len(results) == 0:
            return []
        
        result = results[0]
        if not hasattr(result, 'boxes') or result.boxes is None:
            return []
        
        filtered_boxes = []
        
        # ä¸­æ–‡ç±»åˆ«åç§°æ˜ å°„
        class_name_cn = {
            'person': 'äºº',
            'bicycle': 'è‡ªè¡Œè½¦',
            'car': 'æ±½è½¦',
            'motorcycle': 'æ‘©æ‰˜è½¦',
            'airplane': 'é£æœº',
            'bus': 'å…¬äº¤è½¦',
            'train': 'ç«è½¦',
            'truck': 'å¡è½¦',
            'boat': 'èˆ¹',
            'traffic light': 'äº¤é€šç¯',
            'fire hydrant': 'æ¶ˆé˜²æ “',
            'stop sign': 'åœæ­¢æ ‡å¿—',
            'parking meter': 'åœè½¦è®¡æ—¶å™¨',
            'bench': 'é•¿æ¤…',
            'bird': 'é¸Ÿ',
            'cat': 'çŒ«',
            'dog': 'ç‹—',
            'horse': 'é©¬',
            'sheep': 'ç¾Š',
            'cow': 'ç‰›',
            'elephant': 'å¤§è±¡',
            'bear': 'ç†Š',
            'zebra': 'æ–‘é©¬',
            'giraffe': 'é•¿é¢ˆé¹¿',
            'backpack': 'èƒŒåŒ…',
            'umbrella': 'é›¨ä¼',
            'handbag': 'æ‰‹æåŒ…',
            'tie': 'é¢†å¸¦',
            'suitcase': 'æ‰‹æç®±',
            'frisbee': 'é£ç›˜',
            'skis': 'æ»‘é›ªæ¿',
            'snowboard': 'æ»‘é›ªæ¿',
            'sports ball': 'è¿åŠ¨çƒ',
            'kite': 'é£ç­',
            'baseball bat': 'æ£’çƒæ£’',
            'baseball glove': 'æ£’çƒæ‰‹å¥—',
            'skateboard': 'æ»‘æ¿',
            'surfboard': 'å†²æµªæ¿',
            'tennis racket': 'ç½‘çƒæ‹',
            'bottle': 'ç“¶å­',
            'wine glass': 'é…’æ¯',
            'cup': 'æ¯å­',
            'fork': 'å‰å­',
            'knife': 'åˆ€',
            'spoon': 'å‹ºå­',
            'bowl': 'ç¢—',
            'banana': 'é¦™è•‰',
            'apple': 'è‹¹æœ',
            'sandwich': 'ä¸‰æ˜æ²»',
            'orange': 'æ©™å­',
            'broccoli': 'è¥¿å…°èŠ±',
            'carrot': 'èƒ¡èåœ',
            'hot dog': 'çƒ­ç‹—',
            'pizza': 'æŠ«è¨',
            'donut': 'ç”œç”œåœˆ',
            'cake': 'è›‹ç³•',
            'chair': 'æ¤…å­',
            'couch': 'æ²™å‘',
            'potted plant': 'ç›†æ ½',
            'bed': 'åºŠ',
            'dining table': 'é¤æ¡Œ',
            'toilet': 'é©¬æ¡¶',
            'tv': 'ç”µè§†',
            'laptop': 'ç¬”è®°æœ¬ç”µè„‘',
            'mouse': 'é¼ æ ‡',
            'remote': 'é¥æ§å™¨',
            'keyboard': 'é”®ç›˜',
            'cell phone': 'æ‰‹æœº',
            'microwave': 'å¾®æ³¢ç‚‰',
            'oven': 'çƒ¤ç®±',
            'toaster': 'çƒ¤é¢åŒ…æœº',
            'sink': 'æ°´æ§½',
            'refrigerator': 'å†°ç®±',
            'book': 'ä¹¦',
            'clock': 'æ—¶é’Ÿ',
            'vase': 'èŠ±ç“¶',
            'scissors': 'å‰ªåˆ€',
            'teddy bear': 'æ³°è¿ªç†Š',
            'hair drier': 'å¹é£æœº',
            'toothbrush': 'ç‰™åˆ·'
        }
        
        # å¦‚æœæ£€æµ‹æ‰€æœ‰ç±»åˆ«
        if class_filter.lower() == 'all' or class_filter == 'å…¨éƒ¨':
            for box in result.boxes:
                xyxy = box.xyxy[0].cpu().numpy()
                conf = box.conf[0].cpu().numpy()
                cls = int(box.cls[0].cpu().numpy())
                
                # è·å–ç±»åˆ«åç§°
                class_name = result.names[cls] if hasattr(result, 'names') else str(cls)
                class_name_display = class_name_cn.get(class_name, class_name)
                
                filtered_boxes.append({
                    'bbox': xyxy.tolist(),
                    'confidence': float(conf),
                    'class': class_name,
                    'class_display': class_name_display,
                    'class_id': cls
                })
        else:
            # è§£æè¦æ£€æµ‹çš„ç±»åˆ«ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
            target_classes = [c.strip().lower() for c in class_filter.split(',')]
            
            # åˆ›å»ºåå‘æ˜ å°„ï¼ˆä¸­æ–‡åˆ°è‹±æ–‡ï¼‰
            class_name_en = {v: k for k, v in class_name_cn.items()}
            
            # è½¬æ¢ä¸­æ–‡ç±»åˆ«ååˆ°è‹±æ–‡
            target_classes_en = []
            for tc in target_classes:
                if tc in class_name_en:
                    target_classes_en.append(class_name_en[tc])
                else:
                    target_classes_en.append(tc)
            
            for box in result.boxes:
                cls = int(box.cls[0].cpu().numpy())
                class_name = result.names[cls] if hasattr(result, 'names') else str(cls)
                
                if class_name.lower() in target_classes_en:
                    xyxy = box.xyxy[0].cpu().numpy()
                    conf = box.conf[0].cpu().numpy()
                    class_name_display = class_name_cn.get(class_name, class_name)
                    
                    filtered_boxes.append({
                        'bbox': xyxy.tolist(),
                        'confidence': float(conf),
                        'class': class_name,
                        'class_display': class_name_display,
                        'class_id': cls
                    })
        
        return filtered_boxes

    def sort_detections(self, detections, sort_by):
        """æ ¹æ®æŒ‡å®šæ–¹å¼å¯¹æ£€æµ‹ç»“æœæ’åº"""
        if not detections or sort_by == "é»˜è®¤":
            return detections
        
        if sort_by == "ä»å·¦åˆ°å³":
            return sorted(detections, key=lambda d: (d['bbox'][0] + d['bbox'][2]) / 2)
        elif sort_by == "ä»å³åˆ°å·¦":
            return sorted(detections, key=lambda d: (d['bbox'][0] + d['bbox'][2]) / 2, reverse=True)
        elif sort_by == "ä»ä¸Šåˆ°ä¸‹":
            return sorted(detections, key=lambda d: (d['bbox'][1] + d['bbox'][3]) / 2)
        elif sort_by == "ä»ä¸‹åˆ°ä¸Š":
            return sorted(detections, key=lambda d: (d['bbox'][1] + d['bbox'][3]) / 2, reverse=True)
        elif sort_by == "ç½®ä¿¡åº¦é™åº":
            return sorted(detections, key=lambda d: d['confidence'], reverse=True)
        elif sort_by == "ç½®ä¿¡åº¦å‡åº":
            return sorted(detections, key=lambda d: d['confidence'])
        elif sort_by == "é¢ç§¯é™åº":
            def get_area(d):
                x1, y1, x2, y2 = d['bbox']
                return (x2 - x1) * (y2 - y1)
            return sorted(detections, key=get_area, reverse=True)
        elif sort_by == "é¢ç§¯å‡åº":
            def get_area(d):
                x1, y1, x2, y2 = d['bbox']
                return (x2 - x1) * (y2 - y1)
            return sorted(detections, key=get_area)
        
        return detections

    def crop_single_object(self, image, detection, square_size, object_margin, 
                          vertical_offset, horizontal_offset, original_height, 
                          original_width, scale_x=1.0, scale_y=1.0):
        """ä½¿ç”¨æ™ºèƒ½å°ºå¯¸è°ƒæ•´è£å‰ªå•ä¸ªæ£€æµ‹å¯¹è±¡"""
        height, width = image.shape[:2]
        
        # è·å–æ£€æµ‹æ¡†åæ ‡
        xmin, ymin, xmax, ymax = detection['bbox']
        
        # è®¡ç®—æ£€æµ‹åˆ°çš„å¯¹è±¡æ¡†çš„å®½åº¦å’Œé«˜åº¦
        object_width = xmax - xmin
        object_height = ymax - ymin
        object_size = max(object_width, object_height)  # ä½¿ç”¨è¾ƒå¤§çš„è¾¹ä½œä¸ºåŸºå‡†
        
        # æ ¹æ®å¯¹è±¡å°ºå¯¸è®¡ç®—åˆé€‚çš„æ–¹å—å¤§å°
        # æ·»åŠ é¢å¤–çš„è¾¹è·
        actual_square_size = object_size * object_margin
        
        # åº”ç”¨ç”¨æˆ·æŒ‡å®šçš„ç™¾åˆ†æ¯”è°ƒæ•´
        actual_square_size = actual_square_size * (square_size / 100.0)
        
        # è®¡ç®—è¾¹ç•Œæ¡†çš„ä¸­å¿ƒ
        center_x = (xmin + xmax) / 2
        center_y = (ymin + ymax) / 2
        
        # ä½¿ç”¨å®é™…çš„æ–¹å—å¤§å°
        half_size = actual_square_size / 2
        
        # è®¡ç®—åç§»åçš„åæ ‡
        vertical_offset_px = (actual_square_size) * vertical_offset / 100
        horizontal_offset_px = (actual_square_size) * horizontal_offset / 100
        
        # è®¡ç®—æ–°çš„è¾¹ç•Œæ¡†åæ ‡ï¼Œä¿æŒæ­£æ–¹å½¢å¤§å°
        x1_new = max(0, int(center_x - half_size + horizontal_offset_px))
        x2_new = min(width, int(center_x + half_size + horizontal_offset_px))
        y1_new = max(0, int(center_y - half_size + vertical_offset_px))
        y2_new = min(height, int(center_y + half_size + vertical_offset_px))
        
        # ç¡®ä¿è£å‰ªåŒºåŸŸæ˜¯æ­£æ–¹å½¢
        crop_width = x2_new - x1_new
        crop_height = y2_new - y1_new
        
        if crop_width != crop_height:
            # å¦‚æœä¸æ˜¯æ­£æ–¹å½¢ï¼Œè°ƒæ•´ä¸ºæ­£æ–¹å½¢
            target_size = min(crop_width, crop_height)
            center_x_new = (x1_new + x2_new) / 2
            center_y_new = (y1_new + y2_new) / 2
            half_target = target_size / 2
            
            x1_new = max(0, int(center_x_new - half_target))
            x2_new = min(width, int(center_x_new + half_target))
            y1_new = max(0, int(center_y_new - half_target))
            y2_new = min(height, int(center_y_new + half_target))
        
        # è£å‰ªå›¾åƒ
        cropped = image[y1_new:y2_new, x1_new:x2_new]
        
        # åˆ›å»ºåŸå›¾å°ºå¯¸çš„é®ç½©
        mask = np.zeros((original_height, original_width), dtype=np.float32)
        
        # è®¡ç®—åœ¨åŸå§‹å°ºå¯¸ä¸Šçš„åæ ‡
        mask_x1 = int(x1_new * scale_x)
        mask_y1 = int(y1_new * scale_y)
        mask_x2 = int(x2_new * scale_x)
        mask_y2 = int(y2_new * scale_y)
        
        # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
        mask_x1 = max(0, min(mask_x1, original_width))
        mask_y1 = max(0, min(mask_y1, original_height))
        mask_x2 = max(mask_x1, min(mask_x2, original_width))
        mask_y2 = max(mask_y1, min(mask_y2, original_height))
        
        if mask_x2 > mask_x1 and mask_y2 > mask_y1:
            mask[mask_y1:mask_y2, mask_x1:mask_x2] = 1.0
        
        return cropped, mask, (x1_new, y1_new, x2_new, y2_new)

    def detect_and_crop(self, image, model_name, confidence, class_filter, 
                       square_size, object_margin, vertical_offset, horizontal_offset,
                       sort_by, crop_mode, object_index):
        """æ‰§è¡Œæ£€æµ‹å’Œè£å‰ª"""
        # ç¡®ä¿å›¾åƒæ˜¯4ç»´çš„
        if len(image.shape) == 3:
            image = image.unsqueeze(0)
        
        # ä¿å­˜åŸå§‹å›¾åƒå°ºå¯¸
        batch_size, tensor_height, tensor_width, channels = image.shape
        
        # åŠ è½½æ¨¡å‹
        try:
            model = self.load_model(model_name)
        except Exception as e:
            empty_mask = torch.zeros((1, tensor_height, tensor_width), dtype=torch.float32)
            empty_bboxes = []
            return ([image], empty_mask, empty_bboxes, "æ¨¡å‹åŠ è½½å¤±è´¥", 0)
        
        # è½¬æ¢ä¸ºPILå›¾åƒè¿›è¡Œæ£€æµ‹
        pil_img = self.tensor_to_pil(image)
        
        # æ‰§è¡Œæ£€æµ‹
        results = model(pil_img, conf=confidence, verbose=False)
        
        # è¿‡æ»¤æ£€æµ‹ç»“æœ
        detections = self.filter_detections(results, class_filter)
        
        # å¯¹æ£€æµ‹ç»“æœè¿›è¡Œæ’åº
        detections = self.sort_detections(detections, sort_by)
        
        if not detections:
            empty_mask = torch.zeros((1, tensor_height, tensor_width), dtype=torch.float32)
            empty_bboxes = []
            return ([image], empty_mask, empty_bboxes, "æœªæ£€æµ‹åˆ°å¯¹è±¡", 0)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„è¿›è¡Œå¤„ç†
        img_np = np.array(pil_img)
        
        # ä½¿ç”¨tensorçš„åŸå§‹å°ºå¯¸ä½œä¸ºé®ç½©å°ºå¯¸
        original_height, original_width = tensor_height, tensor_width
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        pil_height, pil_width = img_np.shape[:2]
        scale_x = original_width / pil_width
        scale_y = original_height / pil_height
        
        # æ ¹æ®è£å‰ªæ¨¡å¼é€‰æ‹©è¦å¤„ç†çš„å¯¹è±¡
        if crop_mode == "å•ä¸ªå¯¹è±¡":
            if object_index >= len(detections):
                object_index = len(detections) - 1
            selected_detections = [detections[object_index]]
        elif crop_mode == "æŒ‰ç±»åˆ«":
            # æŒ‰ç±»åˆ«åˆ†ç»„ï¼Œæ¯ä¸ªç±»åˆ«é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„
            class_dict = {}
            for det in detections:
                cls = det['class']
                if cls not in class_dict or det['confidence'] > class_dict[cls]['confidence']:
                    class_dict[cls] = det
            selected_detections = list(class_dict.values())
        else:  # å…¨éƒ¨å¯¹è±¡
            selected_detections = detections
        
        # è£å‰ªé€‰ä¸­çš„å¯¹è±¡
        cropped_images = []
        bboxes = []
        detection_info = []
        
        # åˆ›å»ºä¸€ä¸ªä¸åŸå›¾å°ºå¯¸ç›¸åŒçš„åˆå¹¶é®ç½©
        combined_mask = torch.zeros((original_height, original_width), dtype=torch.float32)
        
        for i, det in enumerate(selected_detections):
            cropped, mask, bbox = self.crop_single_object(
                img_np, det, square_size, object_margin, 
                vertical_offset, horizontal_offset,
                original_height, original_width, scale_x, scale_y
            )
            
            # è½¬æ¢ä¸ºtensor
            cropped_tensor = self.pil_to_tensor(Image.fromarray(cropped))
            mask_tensor = torch.from_numpy(mask).float()
            
            # ä¸ºè£å‰ªçš„å›¾åƒæ·»åŠ æ‰¹æ¬¡ç»´åº¦
            cropped_images.append(cropped_tensor.unsqueeze(0))
            
            # åˆå¹¶é®ç½©
            combined_mask = torch.maximum(combined_mask, mask_tensor)
            
            bboxes.append(bbox)
            
            # è®°å½•æ£€æµ‹ä¿¡æ¯
            center_x = int((det['bbox'][0] + det['bbox'][2]) / 2)
            center_y = int((det['bbox'][1] + det['bbox'][3]) / 2)
            object_size = max(det['bbox'][2] - det['bbox'][0], det['bbox'][3] - det['bbox'][1])
            info = f"[{i}] {det['class_display']} (ç½®ä¿¡åº¦: {det['confidence']:.2f}, ä¸­å¿ƒ: x={center_x},y={center_y}, å°ºå¯¸: {object_size:.0f})"
            detection_info.append(info)
        
        # ä¸ºåˆå¹¶é®ç½©æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        final_mask = combined_mask.unsqueeze(0)
        final_mask = final_mask.clamp(0.0, 1.0)
        
        # ç¡®ä¿è¾¹ç•Œæ¡†æ ¼å¼æ­£ç¡®
        final_bboxes = []
        for bbox in bboxes:
            if isinstance(bbox, (list, tuple)) and len(bbox) == 4:
                final_bboxes.append(list(bbox))
        
        # ç”Ÿæˆæ£€æµ‹ä¿¡æ¯å­—ç¬¦ä¸²
        info_str = f"æ£€æµ‹åˆ° {len(detections)} ä¸ªå¯¹è±¡ï¼Œè£å‰ªäº† {len(selected_detections)} ä¸ª\n"
        info_str += f"è£å‰ªè®¾ç½®: å¤§å°={square_size}%, è¾¹è·={object_margin}x, å‚ç›´åç§»={vertical_offset}%, æ°´å¹³åç§»={horizontal_offset}%\n"
        info_str += "\n".join(detection_info)
        
        # è¿”å›è£å‰ªçš„å›¾åƒåˆ—è¡¨å’Œåˆå¹¶çš„é®ç½©
        return (cropped_images, final_mask, final_bboxes, info_str, len(selected_detections))

# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "YoloBboxesCropNode": YoloBboxesCropNode
}

# èŠ‚ç‚¹æ˜¾ç¤ºåç§°æ˜ å°„
NODE_DISPLAY_NAME_MAPPINGS = {
    "YoloBboxesCropNode": "ğŸ³YOLOæ™ºèƒ½è£å‰ª"
}